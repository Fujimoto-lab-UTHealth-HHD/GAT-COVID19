{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VR8J2d4YM9GQ"
      },
      "outputs": [],
      "source": [
        "# This file is part of MARTRIX.\n",
        "#\n",
        "# MARTRIX is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "#\n",
        "# MARTRIX is distributed in the hope that it will be useful,\n",
        "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
        "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n",
        "# GNU General Public License for more details.\n",
        "#\n",
        "# You should have received a copy of the GNU General Public License\n",
        "# along with MARTRIX. If not, see <https://www.gnu.org/licenses/>.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tqdm\n",
        "import csv\n",
        "\n",
        "import torch\n",
        "from torch.nn import Linear\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv, GATConv\n",
        "from torch_geometric.explain import Explainer, GNNExplainer\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from typing import Dict, List, Optional, Union\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqwTQjksM9GR",
        "outputId": "63908c6e-8c61-4f1c-b730-f2b1327f7cd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "22\n"
          ]
        }
      ],
      "source": [
        "feature_names = [\n",
        "                \"b\",\"c\",\"d\", \"Gender\", \"BLACK_AFRICAN_AMERICAN\",\n",
        "                                  \"OTHER\", \"ASIAN\", \"pp_degree\",\"pp_cls_coef\",\n",
        "                                  \"pp_exposure_numerator\",\"hh_degree\"\n",
        "                                  ,\"education_center\" , \"assisted_living\" ,\n",
        "                                  \"healthcare\" ,   \"charities_homeless\" ,  \"detention_center\" ,\n",
        "                                  \"commercial_offices\" ,  \"government\",\"hh_exposure_numerator\" ,\"hhnum_edu\" ,\"ppdeg_homeless\",\"ppdeg_edu\"]\n",
        "\n",
        "print(len(feature_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S13w1zhBXJmk"
      },
      "outputs": [],
      "source": [
        "folder = \"../data/\"\n",
        "att_name = \"att_pp_main_interaction.csv\"\n",
        "edge_list_name = [\"edge_pp_main.csv\", \"edge_hh_main.csv\",\"edge_pv_main_uniq.csv\", \"edge_pv_main_1.csv\", \"edge_pv_main_2.csv\", \"edge_pv_main_3.csv\"\n",
        ", \"edge_pv_main_4.csv\", \"edge_pv_main_5.csv\", \"edge_pv_main_6.csv\", \"edge_pv_main_7.csv\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fhqu9F5RYMt_"
      },
      "outputs": [],
      "source": [
        "att = pd.read_csv(folder + att_name)\n",
        "att.columns=['NodeId', 'EventID', 'Age', 'Gender', 'pp_degree', 'pp_page_rank',\n",
        "       'pp_cls_coef', 'pp_net_exp', 'pp_eigen', 'labels',\n",
        "       'BLACK_AFRICAN_AMERICAN', 'OTHER', 'WHITE', 'ASIAN',\n",
        "       'pv_binalized_degree', 'a', 'b', 'c', 'd', 'education_center',\n",
        "       'assisted_living', 'healthcare', 'charities_homeless',\n",
        "       'detention_center', 'commercial_offices', 'government', 'sum',\n",
        "       'pp_exposure_numerator', 'hh_exposure_numerator', 'hh_degree',\n",
        "       'hh_net_exp',\"hhnum_edu\" ,\"ppdeg_homeless\",\"ppdeg_edu\"]\n",
        "att['features'] = att.apply(lambda row: np.array([ row['Gender'], row['pp_degree'],  row['pp_cls_coef']\n",
        "                                                  , row['BLACK_AFRICAN_AMERICAN'], row['OTHER']\n",
        "                                                  , row['ASIAN'], row['b'], row['c'], row['d']\n",
        "                                                  , row['education_center'], row['assisted_living'], row['healthcare']\n",
        "                                                  , row['charities_homeless'], row['detention_center'], row['commercial_offices'], row['government']\n",
        "                                                   , row['hhnum_edu'], row['ppdeg_homeless'], row['ppdeg_edu']\n",
        "                                                  , row['pp_exposure_numerator'], row['hh_exposure_numerator'], row['hh_degree']]), axis=1)\n",
        "\n",
        "edge_list = []\n",
        "for name in edge_list_name:\n",
        "      edge_list.append(pd.read_csv(folder + name))\n",
        "for edge in edge_list:\n",
        "  row_idx = []\n",
        "  col_idx = []\n",
        "  for i in range(edge.shape[0]):\n",
        "        id_from = edge.iloc[i,0]\n",
        "        id_to = edge.iloc[i,1]\n",
        "        row_id = att.loc[:,\"NodeId\"][att[\"EventID\"] == id_from]\n",
        "        row_idx.append(row_id)\n",
        "        col_id = att.loc[:,\"NodeId\"][att[\"EventID\"] == id_to]\n",
        "        col_idx.append(col_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juqnsNaYM9GS",
        "outputId": "8af75c5a-f3b1-4b55-efad-d7e84275bdeb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "numpy.int64"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "positive_ids = att['NodeId'].loc[att['labels'] == 1]\n",
        "\n",
        "positive_ids = positive_ids.values\n",
        "type(positive_ids[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UQSJ4F7M9GS",
        "outputId": "f2e7959a-f07f-42b0-b93c-7b3e73f3096e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "22"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(att['features'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLpJ0KJzf-Mi"
      },
      "outputs": [],
      "source": [
        "source_node = np.concatenate((col_idx, row_idx))\n",
        "target_node = np.concatenate((row_idx, col_idx))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZW91KBfYg_Gl",
        "outputId": "7c49c3a4-0dbe-4b58-fbc2-2dfdf1dcc16e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train count: 1584\n",
            "test count: 680\n"
          ]
        }
      ],
      "source": [
        "train_share = 0.70\n",
        "n_papers=2264\n",
        "cut_off = int(n_papers * train_share)\n",
        "print(\"train count:\", cut_off)\n",
        "train_mask = n_papers * [False]\n",
        "train_mask[:cut_off] = cut_off * [True]\n",
        "test_mask = [not e for e in train_mask]\n",
        "\n",
        "print(\"test count:\", test_mask.count(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ny-tt77ZM9GT",
        "outputId": "fdade308-df33-4ee7-f550-4bdd0ca5ee00"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-01cda7ad07ef>:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  x = torch.tensor(\n"
          ]
        }
      ],
      "source": [
        "x = torch.tensor(\n",
        "    att['features'].values.tolist(), dtype=torch.float\n",
        "    )\n",
        "\n",
        "y = torch.tensor(\n",
        "    att['labels'].values.tolist(), dtype=torch.long\n",
        ")\n",
        "\n",
        "edge_index = torch.tensor(np.array([source_node, target_node]), dtype=torch.int64)\n",
        "\n",
        "graph_object = Data(x=x, edge_index=edge_index, y=y)\n",
        "graph_object.train_mask = torch.tensor(train_mask)\n",
        "graph_object.test_mask = torch.tensor(test_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhGrnCdbM9GT",
        "outputId": "7ac83f3d-f416-405c-ba67-42fe84f19ffd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data(x=[2264, 22], edge_index=[2, 296, 1], y=[2264], train_mask=[2264], test_mask=[2264])\n",
            "==============================================================\n",
            "Number of nodes: 2264\n",
            "Number of edges: 0\n",
            "Average node degree: 0.00\n",
            "Number of training nodes: 1584\n",
            "Number of test nodes: 680\n",
            "Training node label rate: 0.70\n",
            "Test node label rate: 0.30\n",
            "Contains isolated nodes: True\n",
            "Contains self-loops: False\n",
            "Is undirected: False\n"
          ]
        }
      ],
      "source": [
        "print(graph_object)\n",
        "print(\"==============================================================\")\n",
        "\n",
        "print(f\"Number of nodes: {graph_object.num_nodes}\")\n",
        "print(f\"Number of edges: {int(graph_object.num_edges/2)}\")\n",
        "print(f\"Average node degree: {(graph_object.num_edges) / graph_object.num_nodes:.2f}\")\n",
        "print(f\"Number of training nodes: {graph_object.train_mask.sum()}\")\n",
        "print(f\"Number of test nodes: {graph_object.test_mask.sum()}\")\n",
        "print(f\"Training node label rate: {int(graph_object.train_mask.sum()) / graph_object.num_nodes:.2f}\")\n",
        "print(f\"Test node label rate: {int(graph_object.test_mask.sum()) / graph_object.num_nodes:.2f}\")\n",
        "print(f\"Contains isolated nodes: {graph_object.has_isolated_nodes()}\")\n",
        "print(f\"Contains self-loops: {graph_object.has_self_loops()}\")\n",
        "print(f\"Is undirected: {graph_object.is_undirected()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lm4uT4bBM9GT"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1)\n",
        "\n",
        "X_train = X_train.numpy()\n",
        "X_test = X_test.numpy()\n",
        "y_train = y_train.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4P6aji4M9GT"
      },
      "source": [
        "### GAT Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f74cMT-YM9GU"
      },
      "outputs": [],
      "source": [
        "# Define accuracy\n",
        "def accuracy(pred_y, y):\n",
        "    return (pred_y == y).sum() / len(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "re2ZSz9EM9GU",
        "outputId": "5d7053fe-3404-47c9-e1e0-cda8a83b55f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GCN(\n",
            "  (gcn): GCNConv(22, 16)\n",
            "  (out): Linear(in_features=16, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "#GCN\n",
        "\n",
        "device = 'cpu'\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.gcn = GCNConv(graph_object.num_node_features, 16)\n",
        "        self.out = Linear(16, len(graph_object.y.unique()))\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        h = self.gcn(x, edge_index).relu()\n",
        "        z = self.out(h)\n",
        "        return h, z\n",
        "\n",
        "model = GCN()\n",
        "print(model)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqug7i4bM9GU",
        "outputId": "69e65128-e178-48d3-e741-c7b12e811a48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   0 | Loss: 0.70 | Acc: 64.80%\n",
            "Epoch  10 | Loss: 0.68 | Acc: 64.80%\n",
            "Epoch  20 | Loss: 0.65 | Acc: 64.80%\n",
            "Epoch  30 | Loss: 0.64 | Acc: 64.80%\n",
            "Epoch  40 | Loss: 0.62 | Acc: 64.71%\n",
            "Epoch  50 | Loss: 0.61 | Acc: 65.64%\n",
            "Epoch  60 | Loss: 0.60 | Acc: 66.25%\n",
            "Epoch  70 | Loss: 0.59 | Acc: 66.74%\n",
            "Epoch  80 | Loss: 0.58 | Acc: 67.93%\n",
            "Epoch  90 | Loss: 0.57 | Acc: 69.30%\n",
            "Epoch 100 | Loss: 0.56 | Acc: 69.74%\n",
            "Epoch 110 | Loss: 0.55 | Acc: 70.14%\n",
            "Epoch 120 | Loss: 0.55 | Acc: 70.76%\n",
            "Epoch 130 | Loss: 0.54 | Acc: 71.60%\n",
            "Epoch 140 | Loss: 0.53 | Acc: 72.22%\n",
            "Epoch 150 | Loss: 0.53 | Acc: 72.39%\n",
            "Epoch 160 | Loss: 0.52 | Acc: 72.92%\n",
            "Epoch 170 | Loss: 0.52 | Acc: 73.01%\n",
            "Epoch 180 | Loss: 0.52 | Acc: 73.81%\n",
            "Epoch 190 | Loss: 0.51 | Acc: 74.29%\n",
            "Epoch 200 | Loss: 0.51 | Acc: 74.20%\n",
            "Epoch 210 | Loss: 0.51 | Acc: 74.29%\n",
            "Epoch 220 | Loss: 0.51 | Acc: 74.38%\n",
            "Epoch 230 | Loss: 0.51 | Acc: 74.38%\n",
            "Epoch 240 | Loss: 0.51 | Acc: 74.60%\n",
            "Epoch 250 | Loss: 0.50 | Acc: 74.69%\n",
            "Epoch 260 | Loss: 0.50 | Acc: 74.78%\n",
            "Epoch 270 | Loss: 0.50 | Acc: 74.91%\n",
            "Epoch 280 | Loss: 0.50 | Acc: 75.00%\n",
            "Epoch 290 | Loss: 0.50 | Acc: 75.04%\n",
            "Epoch 300 | Loss: 0.50 | Acc: 75.09%\n",
            "Epoch 310 | Loss: 0.50 | Acc: 75.13%\n",
            "Epoch 320 | Loss: 0.49 | Acc: 75.13%\n",
            "Epoch 330 | Loss: 0.49 | Acc: 75.13%\n",
            "Epoch 340 | Loss: 0.49 | Acc: 75.22%\n",
            "Epoch 350 | Loss: 0.49 | Acc: 75.27%\n",
            "Epoch 360 | Loss: 0.49 | Acc: 75.09%\n",
            "Epoch 370 | Loss: 0.49 | Acc: 75.13%\n",
            "Epoch 380 | Loss: 0.49 | Acc: 75.09%\n",
            "Epoch 390 | Loss: 0.49 | Acc: 75.18%\n",
            "Epoch 400 | Loss: 0.48 | Acc: 75.18%\n",
            "Epoch 410 | Loss: 0.48 | Acc: 75.22%\n",
            "Epoch 420 | Loss: 0.48 | Acc: 75.40%\n",
            "Epoch 430 | Loss: 0.48 | Acc: 75.53%\n",
            "Epoch 440 | Loss: 0.48 | Acc: 75.57%\n",
            "Epoch 450 | Loss: 0.48 | Acc: 75.71%\n",
            "Epoch 460 | Loss: 0.48 | Acc: 75.88%\n",
            "Epoch 470 | Loss: 0.48 | Acc: 75.88%\n",
            "Epoch 480 | Loss: 0.48 | Acc: 76.28%\n",
            "Epoch 490 | Loss: 0.47 | Acc: 76.15%\n",
            "Epoch 500 | Loss: 0.47 | Acc: 76.41%\n",
            "Epoch 510 | Loss: 0.47 | Acc: 76.10%\n",
            "Epoch 520 | Loss: 0.47 | Acc: 76.63%\n",
            "Epoch 530 | Loss: 0.47 | Acc: 76.81%\n",
            "Epoch 540 | Loss: 0.47 | Acc: 76.77%\n",
            "Epoch 550 | Loss: 0.47 | Acc: 76.94%\n",
            "Epoch 560 | Loss: 0.47 | Acc: 76.94%\n",
            "Epoch 570 | Loss: 0.47 | Acc: 76.81%\n",
            "Epoch 580 | Loss: 0.46 | Acc: 76.68%\n",
            "Epoch 590 | Loss: 0.46 | Acc: 76.77%\n",
            "Epoch 600 | Loss: 0.46 | Acc: 76.86%\n",
            "Epoch 610 | Loss: 0.46 | Acc: 76.68%\n",
            "Epoch 620 | Loss: 0.46 | Acc: 76.77%\n",
            "Epoch 630 | Loss: 0.46 | Acc: 76.86%\n",
            "Epoch 640 | Loss: 0.46 | Acc: 76.81%\n",
            "Epoch 650 | Loss: 0.46 | Acc: 76.86%\n",
            "Epoch 660 | Loss: 0.46 | Acc: 76.90%\n",
            "Epoch 670 | Loss: 0.45 | Acc: 76.90%\n",
            "Epoch 680 | Loss: 0.45 | Acc: 76.94%\n",
            "Epoch 690 | Loss: 0.45 | Acc: 77.08%\n",
            "Epoch 700 | Loss: 0.45 | Acc: 77.03%\n",
            "Epoch 710 | Loss: 0.45 | Acc: 76.94%\n",
            "Epoch 720 | Loss: 0.45 | Acc: 76.99%\n",
            "Epoch 730 | Loss: 0.45 | Acc: 77.03%\n",
            "Epoch 740 | Loss: 0.45 | Acc: 77.08%\n",
            "Epoch 750 | Loss: 0.45 | Acc: 77.03%\n",
            "Epoch 760 | Loss: 0.45 | Acc: 77.08%\n",
            "Epoch 770 | Loss: 0.45 | Acc: 77.03%\n",
            "Epoch 780 | Loss: 0.45 | Acc: 76.99%\n",
            "Epoch 790 | Loss: 0.44 | Acc: 76.99%\n",
            "Epoch 800 | Loss: 0.44 | Acc: 76.94%\n",
            "Epoch 810 | Loss: 0.44 | Acc: 76.99%\n",
            "Epoch 820 | Loss: 0.44 | Acc: 76.86%\n",
            "Epoch 830 | Loss: 0.44 | Acc: 76.94%\n",
            "Epoch 840 | Loss: 0.44 | Acc: 77.08%\n",
            "Epoch 850 | Loss: 0.44 | Acc: 77.12%\n",
            "Epoch 860 | Loss: 0.44 | Acc: 77.12%\n",
            "Epoch 870 | Loss: 0.44 | Acc: 77.08%\n",
            "Epoch 880 | Loss: 0.44 | Acc: 77.16%\n",
            "Epoch 890 | Loss: 0.44 | Acc: 77.30%\n",
            "Epoch 900 | Loss: 0.44 | Acc: 77.25%\n",
            "Epoch 910 | Loss: 0.44 | Acc: 77.30%\n",
            "Epoch 920 | Loss: 0.44 | Acc: 77.25%\n",
            "Epoch 930 | Loss: 0.44 | Acc: 77.25%\n",
            "Epoch 940 | Loss: 0.44 | Acc: 77.25%\n",
            "Epoch 950 | Loss: 0.43 | Acc: 77.25%\n",
            "Epoch 960 | Loss: 0.43 | Acc: 77.25%\n",
            "Epoch 970 | Loss: 0.43 | Acc: 77.30%\n",
            "Epoch 980 | Loss: 0.43 | Acc: 77.34%\n",
            "Epoch 990 | Loss: 0.43 | Acc: 77.39%\n",
            "Epoch 1000 | Loss: 0.43 | Acc: 77.25%\n",
            "Epoch 1010 | Loss: 0.43 | Acc: 77.30%\n",
            "Epoch 1020 | Loss: 0.43 | Acc: 77.30%\n",
            "Epoch 1030 | Loss: 0.43 | Acc: 77.30%\n",
            "Epoch 1040 | Loss: 0.43 | Acc: 77.21%\n",
            "Epoch 1050 | Loss: 0.43 | Acc: 77.21%\n",
            "Epoch 1060 | Loss: 0.43 | Acc: 77.25%\n",
            "Epoch 1070 | Loss: 0.43 | Acc: 77.34%\n",
            "Epoch 1080 | Loss: 0.43 | Acc: 77.30%\n",
            "Epoch 1090 | Loss: 0.43 | Acc: 77.39%\n",
            "Epoch 1100 | Loss: 0.43 | Acc: 77.43%\n",
            "Epoch 1110 | Loss: 0.43 | Acc: 77.43%\n",
            "Epoch 1120 | Loss: 0.43 | Acc: 77.43%\n",
            "Epoch 1130 | Loss: 0.43 | Acc: 77.43%\n",
            "Epoch 1140 | Loss: 0.43 | Acc: 77.43%\n",
            "Epoch 1150 | Loss: 0.43 | Acc: 77.52%\n",
            "Epoch 1160 | Loss: 0.42 | Acc: 77.56%\n",
            "Epoch 1170 | Loss: 0.42 | Acc: 77.61%\n",
            "Epoch 1180 | Loss: 0.42 | Acc: 77.69%\n",
            "Epoch 1190 | Loss: 0.42 | Acc: 77.74%\n",
            "Epoch 1200 | Loss: 0.42 | Acc: 77.69%\n",
            "Epoch 1210 | Loss: 0.42 | Acc: 77.69%\n",
            "Epoch 1220 | Loss: 0.42 | Acc: 77.83%\n",
            "Epoch 1230 | Loss: 0.42 | Acc: 77.78%\n",
            "Epoch 1240 | Loss: 0.42 | Acc: 77.87%\n",
            "Epoch 1250 | Loss: 0.42 | Acc: 77.83%\n",
            "Epoch 1260 | Loss: 0.42 | Acc: 77.87%\n",
            "Epoch 1270 | Loss: 0.42 | Acc: 77.83%\n",
            "Epoch 1280 | Loss: 0.42 | Acc: 77.83%\n",
            "Epoch 1290 | Loss: 0.42 | Acc: 77.83%\n",
            "Epoch 1300 | Loss: 0.42 | Acc: 77.83%\n",
            "Epoch 1310 | Loss: 0.42 | Acc: 77.83%\n",
            "Epoch 1320 | Loss: 0.42 | Acc: 77.92%\n",
            "Epoch 1330 | Loss: 0.42 | Acc: 77.96%\n",
            "Epoch 1340 | Loss: 0.42 | Acc: 77.96%\n",
            "Epoch 1350 | Loss: 0.42 | Acc: 77.96%\n",
            "Epoch 1360 | Loss: 0.42 | Acc: 77.96%\n",
            "Epoch 1370 | Loss: 0.42 | Acc: 77.96%\n",
            "Epoch 1380 | Loss: 0.42 | Acc: 78.00%\n",
            "Epoch 1390 | Loss: 0.42 | Acc: 77.96%\n",
            "Epoch 1400 | Loss: 0.41 | Acc: 78.05%\n",
            "Epoch 1410 | Loss: 0.41 | Acc: 78.05%\n",
            "Epoch 1420 | Loss: 0.41 | Acc: 78.00%\n",
            "Epoch 1430 | Loss: 0.41 | Acc: 78.00%\n",
            "Epoch 1440 | Loss: 0.41 | Acc: 78.09%\n",
            "Epoch 1450 | Loss: 0.41 | Acc: 78.09%\n",
            "Epoch 1460 | Loss: 0.41 | Acc: 78.09%\n",
            "Epoch 1470 | Loss: 0.41 | Acc: 78.09%\n",
            "Epoch 1480 | Loss: 0.41 | Acc: 78.09%\n",
            "Epoch 1490 | Loss: 0.41 | Acc: 78.09%\n",
            "Epoch 1500 | Loss: 0.41 | Acc: 78.14%\n",
            "Epoch 1510 | Loss: 0.41 | Acc: 78.05%\n",
            "Epoch 1520 | Loss: 0.41 | Acc: 78.14%\n",
            "Epoch 1530 | Loss: 0.41 | Acc: 78.14%\n",
            "Epoch 1540 | Loss: 0.41 | Acc: 78.22%\n",
            "Epoch 1550 | Loss: 0.41 | Acc: 78.31%\n",
            "Epoch 1560 | Loss: 0.41 | Acc: 78.27%\n",
            "Epoch 1570 | Loss: 0.41 | Acc: 78.27%\n",
            "Epoch 1580 | Loss: 0.41 | Acc: 78.27%\n",
            "Epoch 1590 | Loss: 0.41 | Acc: 78.27%\n",
            "Epoch 1600 | Loss: 0.41 | Acc: 78.36%\n",
            "Epoch 1610 | Loss: 0.41 | Acc: 78.27%\n",
            "Epoch 1620 | Loss: 0.41 | Acc: 78.31%\n",
            "Epoch 1630 | Loss: 0.41 | Acc: 78.36%\n",
            "Epoch 1640 | Loss: 0.41 | Acc: 78.40%\n",
            "Epoch 1650 | Loss: 0.41 | Acc: 78.45%\n",
            "Epoch 1660 | Loss: 0.41 | Acc: 78.31%\n",
            "Epoch 1670 | Loss: 0.41 | Acc: 78.36%\n",
            "Epoch 1680 | Loss: 0.41 | Acc: 78.40%\n",
            "Epoch 1690 | Loss: 0.41 | Acc: 78.27%\n",
            "Epoch 1700 | Loss: 0.41 | Acc: 78.31%\n",
            "Epoch 1710 | Loss: 0.41 | Acc: 78.27%\n",
            "Epoch 1720 | Loss: 0.40 | Acc: 78.14%\n",
            "Epoch 1730 | Loss: 0.40 | Acc: 78.09%\n",
            "Epoch 1740 | Loss: 0.40 | Acc: 78.18%\n",
            "Epoch 1750 | Loss: 0.40 | Acc: 78.27%\n",
            "Epoch 1760 | Loss: 0.40 | Acc: 78.40%\n",
            "Epoch 1770 | Loss: 0.40 | Acc: 78.36%\n",
            "Epoch 1780 | Loss: 0.40 | Acc: 78.40%\n",
            "Epoch 1790 | Loss: 0.40 | Acc: 78.45%\n",
            "Epoch 1800 | Loss: 0.40 | Acc: 78.49%\n",
            "Epoch 1810 | Loss: 0.40 | Acc: 78.58%\n",
            "Epoch 1820 | Loss: 0.40 | Acc: 78.67%\n",
            "Epoch 1830 | Loss: 0.40 | Acc: 78.67%\n",
            "Epoch 1840 | Loss: 0.40 | Acc: 78.67%\n",
            "Epoch 1850 | Loss: 0.40 | Acc: 78.62%\n",
            "Epoch 1860 | Loss: 0.40 | Acc: 78.62%\n",
            "Epoch 1870 | Loss: 0.40 | Acc: 78.62%\n",
            "Epoch 1880 | Loss: 0.40 | Acc: 78.62%\n",
            "Epoch 1890 | Loss: 0.40 | Acc: 78.67%\n",
            "Epoch 1900 | Loss: 0.40 | Acc: 78.67%\n",
            "Epoch 1910 | Loss: 0.40 | Acc: 78.67%\n",
            "Epoch 1920 | Loss: 0.40 | Acc: 78.62%\n",
            "Epoch 1930 | Loss: 0.40 | Acc: 78.62%\n",
            "Epoch 1940 | Loss: 0.40 | Acc: 78.62%\n",
            "Epoch 1950 | Loss: 0.40 | Acc: 78.62%\n",
            "Epoch 1960 | Loss: 0.40 | Acc: 78.62%\n",
            "Epoch 1970 | Loss: 0.40 | Acc: 78.62%\n",
            "Epoch 1980 | Loss: 0.40 | Acc: 78.62%\n",
            "Epoch 1990 | Loss: 0.40 | Acc: 78.62%\n",
            "Epoch 2000 | Loss: 0.40 | Acc: 78.58%\n",
            "Epoch 2010 | Loss: 0.39 | Acc: 78.67%\n",
            "Epoch 2020 | Loss: 0.39 | Acc: 78.71%\n",
            "Epoch 2030 | Loss: 0.39 | Acc: 78.67%\n",
            "Epoch 2040 | Loss: 0.39 | Acc: 78.67%\n",
            "Epoch 2050 | Loss: 0.39 | Acc: 78.67%\n",
            "Epoch 2060 | Loss: 0.39 | Acc: 78.67%\n",
            "Epoch 2070 | Loss: 0.39 | Acc: 78.67%\n",
            "Epoch 2080 | Loss: 0.39 | Acc: 78.67%\n",
            "Epoch 2090 | Loss: 0.39 | Acc: 78.71%\n",
            "Epoch 2100 | Loss: 0.39 | Acc: 78.75%\n",
            "Epoch 2110 | Loss: 0.39 | Acc: 78.80%\n",
            "Epoch 2120 | Loss: 0.39 | Acc: 78.89%\n",
            "Epoch 2130 | Loss: 0.39 | Acc: 78.80%\n",
            "Epoch 2140 | Loss: 0.39 | Acc: 78.89%\n",
            "Epoch 2150 | Loss: 0.39 | Acc: 78.89%\n",
            "Epoch 2160 | Loss: 0.39 | Acc: 78.89%\n",
            "Epoch 2170 | Loss: 0.39 | Acc: 78.84%\n",
            "Epoch 2180 | Loss: 0.39 | Acc: 78.84%\n",
            "Epoch 2190 | Loss: 0.39 | Acc: 78.80%\n",
            "Epoch 2200 | Loss: 0.39 | Acc: 78.80%\n",
            "Epoch 2210 | Loss: 0.39 | Acc: 78.89%\n",
            "Epoch 2220 | Loss: 0.39 | Acc: 78.89%\n",
            "Epoch 2230 | Loss: 0.39 | Acc: 78.89%\n",
            "Epoch 2240 | Loss: 0.39 | Acc: 78.89%\n",
            "Epoch 2250 | Loss: 0.39 | Acc: 78.89%\n",
            "Epoch 2260 | Loss: 0.39 | Acc: 78.84%\n",
            "Epoch 2270 | Loss: 0.39 | Acc: 78.84%\n",
            "Epoch 2280 | Loss: 0.39 | Acc: 78.89%\n",
            "Epoch 2290 | Loss: 0.39 | Acc: 78.93%\n",
            "Epoch 2300 | Loss: 0.39 | Acc: 78.93%\n",
            "Epoch 2310 | Loss: 0.39 | Acc: 78.89%\n",
            "Epoch 2320 | Loss: 0.39 | Acc: 78.93%\n",
            "Epoch 2330 | Loss: 0.39 | Acc: 78.98%\n",
            "Epoch 2340 | Loss: 0.39 | Acc: 78.84%\n",
            "Epoch 2350 | Loss: 0.38 | Acc: 78.80%\n",
            "Epoch 2360 | Loss: 0.38 | Acc: 78.84%\n",
            "Epoch 2370 | Loss: 0.38 | Acc: 78.80%\n",
            "Epoch 2380 | Loss: 0.38 | Acc: 78.80%\n",
            "Epoch 2390 | Loss: 0.38 | Acc: 78.75%\n"
          ]
        }
      ],
      "source": [
        "## For GCN Training\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Data for animations\n",
        "embeddings = []\n",
        "losses = []\n",
        "accuracies = []\n",
        "outputs = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(2400):\n",
        "    # Clear gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    h,z = model(graph_object.x, graph_object.edge_index)\n",
        "\n",
        "    # Calculate loss function\n",
        "    loss = criterion(z[graph_object.train_mask], graph_object.y[graph_object.train_mask])\n",
        "\n",
        "    # Calculate accuracy\n",
        "    acc = accuracy(z.argmax(dim=1), graph_object.y)\n",
        "\n",
        "    # Compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Tune parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # Store data for animations\n",
        "    embeddings.append(h)\n",
        "    losses.append(loss)\n",
        "    accuracies.append(acc)\n",
        "    outputs.append(z.argmax(dim=1))\n",
        "\n",
        "    # Print metrics every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch:>3} | Loss: {loss:.2f} | Acc: {acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iP9bHl5ZM9GU",
        "outputId": "7749cb03-69b9-4d2e-b376-28578d37561a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GAT(\n",
            "  (conv1): GATConv(22, 64, heads=1)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "## GAT\n",
        "\n",
        "device = 'cpu'\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, heads):\n",
        "        super().__init__()\n",
        "        self.conv1 = GATConv(in_channels, hidden_channels, heads, dropout=0.1)\n",
        "        # On the Pubmed dataset, use `heads` output heads in `conv2`.\n",
        "        # self.conv2 = GATConv(hidden_channels * heads, out_channels, heads=1,\n",
        "        #                      concat=False, dropout=0.6)\n",
        "        #self.out = Linear(hidden_channels, len(graph_object.y.unique()))\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = F.dropout(x, p=0.1, training=self.training)\n",
        "        x = F.relu(self.conv1(x, edge_index))\n",
        "\n",
        "        return x\n",
        "\n",
        "model = GAT(graph_object.num_features, 64, len(graph_object.y.unique()),1).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-5)\n",
        "\n",
        "print(model)\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDAq7rR0M9GU",
        "outputId": "62866e40-87d6-44e7-d3a1-42d2517840c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 10, Loss: 4.0673, Training Acc: 0.1285\n",
            "Epoch: 20, Loss: 3.6870, Training Acc: 0.4448\n",
            "Epoch: 30, Loss: 3.3100, Training Acc: 0.5093\n",
            "Epoch: 40, Loss: 2.9796, Training Acc: 0.4973\n",
            "Epoch: 50, Loss: 2.7081, Training Acc: 0.4947\n",
            "Epoch: 60, Loss: 2.4818, Training Acc: 0.4912\n",
            "Epoch: 70, Loss: 2.2962, Training Acc: 0.5256\n",
            "Epoch: 80, Loss: 2.1726, Training Acc: 0.5406\n",
            "Epoch: 90, Loss: 2.0576, Training Acc: 0.5804\n",
            "Epoch: 100, Loss: 1.9220, Training Acc: 0.5963\n",
            "Epoch: 110, Loss: 1.8639, Training Acc: 0.6378\n",
            "Epoch: 120, Loss: 1.7315, Training Acc: 0.6590\n",
            "Epoch: 130, Loss: 1.6854, Training Acc: 0.6656\n",
            "Epoch: 140, Loss: 1.5802, Training Acc: 0.6837\n",
            "Epoch: 150, Loss: 1.5180, Training Acc: 0.6815\n",
            "Epoch: 160, Loss: 1.4670, Training Acc: 0.6992\n",
            "Epoch: 170, Loss: 1.3971, Training Acc: 0.7023\n",
            "Epoch: 180, Loss: 1.3964, Training Acc: 0.7058\n",
            "Epoch: 190, Loss: 1.3465, Training Acc: 0.7080\n",
            "Epoch: 200, Loss: 1.2998, Training Acc: 0.7054\n",
            "Epoch: 210, Loss: 1.2828, Training Acc: 0.7111\n",
            "Epoch: 220, Loss: 1.2767, Training Acc: 0.7080\n",
            "Epoch: 230, Loss: 1.2054, Training Acc: 0.7160\n",
            "Epoch: 240, Loss: 1.1905, Training Acc: 0.7200\n",
            "Epoch: 250, Loss: 1.1635, Training Acc: 0.7164\n",
            "Epoch: 260, Loss: 1.1642, Training Acc: 0.7208\n",
            "Epoch: 270, Loss: 1.1414, Training Acc: 0.7195\n",
            "Epoch: 280, Loss: 1.1357, Training Acc: 0.7178\n",
            "Epoch: 290, Loss: 1.0829, Training Acc: 0.7173\n",
            "Epoch: 300, Loss: 1.0891, Training Acc: 0.7125\n",
            "Epoch: 310, Loss: 1.0779, Training Acc: 0.7076\n",
            "Epoch: 320, Loss: 1.0267, Training Acc: 0.7164\n",
            "Epoch: 330, Loss: 1.0279, Training Acc: 0.7138\n",
            "Epoch: 340, Loss: 1.0341, Training Acc: 0.7178\n",
            "Epoch: 350, Loss: 1.0091, Training Acc: 0.7164\n",
            "Epoch: 360, Loss: 0.9892, Training Acc: 0.7213\n",
            "Epoch: 370, Loss: 0.9599, Training Acc: 0.7155\n",
            "Epoch: 380, Loss: 0.9724, Training Acc: 0.7155\n",
            "Epoch: 390, Loss: 0.9496, Training Acc: 0.7235\n",
            "Epoch: 400, Loss: 0.9779, Training Acc: 0.7182\n",
            "Epoch: 410, Loss: 0.9311, Training Acc: 0.7142\n",
            "Epoch: 420, Loss: 0.9456, Training Acc: 0.7041\n",
            "Epoch: 430, Loss: 0.9245, Training Acc: 0.7102\n",
            "Epoch: 440, Loss: 0.9242, Training Acc: 0.7244\n",
            "Epoch: 450, Loss: 0.9283, Training Acc: 0.7111\n",
            "Epoch: 460, Loss: 0.8677, Training Acc: 0.7213\n",
            "Epoch: 470, Loss: 0.9259, Training Acc: 0.7151\n",
            "Epoch: 480, Loss: 0.8923, Training Acc: 0.7169\n",
            "Epoch: 490, Loss: 0.8838, Training Acc: 0.7142\n",
            "Epoch: 500, Loss: 0.8487, Training Acc: 0.7120\n",
            "Epoch: 510, Loss: 0.8744, Training Acc: 0.7120\n",
            "Epoch: 520, Loss: 0.8702, Training Acc: 0.7217\n",
            "Epoch: 530, Loss: 0.8562, Training Acc: 0.7173\n",
            "Epoch: 540, Loss: 0.8691, Training Acc: 0.7164\n",
            "Epoch: 550, Loss: 0.8318, Training Acc: 0.7102\n",
            "Epoch: 560, Loss: 0.8285, Training Acc: 0.7164\n",
            "Epoch: 570, Loss: 0.8633, Training Acc: 0.7164\n",
            "Epoch: 580, Loss: 0.8427, Training Acc: 0.7089\n",
            "Epoch: 590, Loss: 0.8374, Training Acc: 0.7147\n",
            "Epoch: 600, Loss: 0.8400, Training Acc: 0.7142\n",
            "Epoch: 610, Loss: 0.8168, Training Acc: 0.7102\n",
            "Epoch: 620, Loss: 0.8051, Training Acc: 0.7182\n",
            "Epoch: 630, Loss: 0.8148, Training Acc: 0.7138\n",
            "Epoch: 640, Loss: 0.8347, Training Acc: 0.7182\n",
            "Epoch: 650, Loss: 0.8029, Training Acc: 0.7204\n",
            "Epoch: 660, Loss: 0.8307, Training Acc: 0.7155\n",
            "Epoch: 670, Loss: 0.7763, Training Acc: 0.7147\n",
            "Epoch: 680, Loss: 0.7747, Training Acc: 0.7257\n",
            "Epoch: 690, Loss: 0.7820, Training Acc: 0.7129\n",
            "Epoch: 700, Loss: 0.7743, Training Acc: 0.7191\n",
            "Epoch: 710, Loss: 0.7762, Training Acc: 0.7204\n",
            "Epoch: 720, Loss: 0.8012, Training Acc: 0.7120\n",
            "Epoch: 730, Loss: 0.7841, Training Acc: 0.7164\n",
            "Epoch: 740, Loss: 0.7678, Training Acc: 0.7186\n",
            "Epoch: 750, Loss: 0.7742, Training Acc: 0.7142\n",
            "Epoch: 760, Loss: 0.7806, Training Acc: 0.7142\n",
            "Epoch: 770, Loss: 0.7458, Training Acc: 0.7178\n",
            "Epoch: 780, Loss: 0.7576, Training Acc: 0.7116\n",
            "Epoch: 790, Loss: 0.7847, Training Acc: 0.7125\n",
            "Epoch: 800, Loss: 0.7389, Training Acc: 0.7155\n",
            "Epoch: 810, Loss: 0.7707, Training Acc: 0.7129\n",
            "Epoch: 820, Loss: 0.7480, Training Acc: 0.7155\n",
            "Epoch: 830, Loss: 0.7491, Training Acc: 0.7164\n",
            "Epoch: 840, Loss: 0.7395, Training Acc: 0.7111\n",
            "Epoch: 850, Loss: 0.7514, Training Acc: 0.7191\n",
            "Epoch: 860, Loss: 0.7583, Training Acc: 0.7155\n",
            "Epoch: 870, Loss: 0.7415, Training Acc: 0.7147\n",
            "Epoch: 880, Loss: 0.7317, Training Acc: 0.7208\n",
            "Epoch: 890, Loss: 0.7210, Training Acc: 0.7125\n",
            "Epoch: 900, Loss: 0.7399, Training Acc: 0.7164\n",
            "Epoch: 910, Loss: 0.7173, Training Acc: 0.7094\n",
            "Epoch: 920, Loss: 0.7069, Training Acc: 0.7257\n",
            "Epoch: 930, Loss: 0.7285, Training Acc: 0.7186\n",
            "Epoch: 940, Loss: 0.7319, Training Acc: 0.7178\n",
            "Epoch: 950, Loss: 0.7101, Training Acc: 0.7120\n",
            "Epoch: 960, Loss: 0.7235, Training Acc: 0.7129\n",
            "Epoch: 970, Loss: 0.6940, Training Acc: 0.7191\n",
            "Epoch: 980, Loss: 0.7072, Training Acc: 0.7138\n",
            "Epoch: 990, Loss: 0.7427, Training Acc: 0.7155\n",
            "Epoch: 1000, Loss: 0.7021, Training Acc: 0.7231\n",
            "Epoch: 1010, Loss: 0.7268, Training Acc: 0.7138\n",
            "Epoch: 1020, Loss: 0.6923, Training Acc: 0.7217\n",
            "Epoch: 1030, Loss: 0.6975, Training Acc: 0.7186\n",
            "Epoch: 1040, Loss: 0.7104, Training Acc: 0.7191\n",
            "Epoch: 1050, Loss: 0.7093, Training Acc: 0.7133\n",
            "Epoch: 1060, Loss: 0.6998, Training Acc: 0.7142\n",
            "Epoch: 1070, Loss: 0.6905, Training Acc: 0.7151\n",
            "Epoch: 1080, Loss: 0.6866, Training Acc: 0.7182\n",
            "Epoch: 1090, Loss: 0.6823, Training Acc: 0.7142\n",
            "Epoch: 1100, Loss: 0.6701, Training Acc: 0.7257\n",
            "Epoch: 1110, Loss: 0.6952, Training Acc: 0.7213\n",
            "Epoch: 1120, Loss: 0.6667, Training Acc: 0.7138\n",
            "Epoch: 1130, Loss: 0.6638, Training Acc: 0.7155\n",
            "Epoch: 1140, Loss: 0.6774, Training Acc: 0.7208\n",
            "Epoch: 1150, Loss: 0.6800, Training Acc: 0.7142\n",
            "Epoch: 1160, Loss: 0.6739, Training Acc: 0.7178\n",
            "Epoch: 1170, Loss: 0.6649, Training Acc: 0.7160\n",
            "Epoch: 1180, Loss: 0.6831, Training Acc: 0.7125\n",
            "Epoch: 1190, Loss: 0.6490, Training Acc: 0.7147\n",
            "Epoch: 1200, Loss: 0.6654, Training Acc: 0.7186\n",
            "Epoch: 1210, Loss: 0.6723, Training Acc: 0.7147\n",
            "Epoch: 1220, Loss: 0.6549, Training Acc: 0.7133\n",
            "Epoch: 1230, Loss: 0.6555, Training Acc: 0.7120\n",
            "Epoch: 1240, Loss: 0.6661, Training Acc: 0.7235\n",
            "Epoch: 1250, Loss: 0.6565, Training Acc: 0.7129\n",
            "Epoch: 1260, Loss: 0.6674, Training Acc: 0.7222\n",
            "Epoch: 1270, Loss: 0.6571, Training Acc: 0.7111\n",
            "Epoch: 1280, Loss: 0.6761, Training Acc: 0.7191\n",
            "Epoch: 1290, Loss: 0.6545, Training Acc: 0.7138\n",
            "Epoch: 1300, Loss: 0.6589, Training Acc: 0.7125\n",
            "Epoch: 1310, Loss: 0.6465, Training Acc: 0.7208\n",
            "Epoch: 1320, Loss: 0.6527, Training Acc: 0.7178\n",
            "Epoch: 1330, Loss: 0.6468, Training Acc: 0.7142\n",
            "Epoch: 1340, Loss: 0.6340, Training Acc: 0.7204\n",
            "Epoch: 1350, Loss: 0.6537, Training Acc: 0.7072\n",
            "Epoch: 1360, Loss: 0.6315, Training Acc: 0.7178\n",
            "Epoch: 1370, Loss: 0.6406, Training Acc: 0.7147\n",
            "Epoch: 1380, Loss: 0.6381, Training Acc: 0.7257\n",
            "Epoch: 1390, Loss: 0.6540, Training Acc: 0.7208\n",
            "Epoch: 1400, Loss: 0.6397, Training Acc: 0.7191\n",
            "Epoch: 1410, Loss: 0.6441, Training Acc: 0.7142\n",
            "Epoch: 1420, Loss: 0.6417, Training Acc: 0.7169\n",
            "Epoch: 1430, Loss: 0.6378, Training Acc: 0.7231\n",
            "Epoch: 1440, Loss: 0.6284, Training Acc: 0.7169\n",
            "Epoch: 1450, Loss: 0.6271, Training Acc: 0.7195\n",
            "Epoch: 1460, Loss: 0.6418, Training Acc: 0.7204\n",
            "Epoch: 1470, Loss: 0.6259, Training Acc: 0.7125\n",
            "Epoch: 1480, Loss: 0.6369, Training Acc: 0.7089\n",
            "Epoch: 1490, Loss: 0.6288, Training Acc: 0.7155\n",
            "Epoch: 1500, Loss: 0.6328, Training Acc: 0.7120\n",
            "Epoch: 1510, Loss: 0.6288, Training Acc: 0.7222\n",
            "Epoch: 1520, Loss: 0.6163, Training Acc: 0.7222\n",
            "Epoch: 1530, Loss: 0.6267, Training Acc: 0.7182\n",
            "Epoch: 1540, Loss: 0.6157, Training Acc: 0.7200\n",
            "Epoch: 1550, Loss: 0.6235, Training Acc: 0.7213\n",
            "Epoch: 1560, Loss: 0.6345, Training Acc: 0.7155\n",
            "Epoch: 1570, Loss: 0.6187, Training Acc: 0.7178\n",
            "Epoch: 1580, Loss: 0.6246, Training Acc: 0.7182\n",
            "Epoch: 1590, Loss: 0.6134, Training Acc: 0.7160\n",
            "Epoch: 1600, Loss: 0.6065, Training Acc: 0.7178\n",
            "Epoch: 1610, Loss: 0.6154, Training Acc: 0.7160\n",
            "Epoch: 1620, Loss: 0.6288, Training Acc: 0.7102\n",
            "Epoch: 1630, Loss: 0.6142, Training Acc: 0.7169\n",
            "Epoch: 1640, Loss: 0.6148, Training Acc: 0.7204\n",
            "Epoch: 1650, Loss: 0.6090, Training Acc: 0.7186\n",
            "Epoch: 1660, Loss: 0.6042, Training Acc: 0.7186\n",
            "Epoch: 1670, Loss: 0.6210, Training Acc: 0.7129\n",
            "Epoch: 1680, Loss: 0.6140, Training Acc: 0.7169\n",
            "Epoch: 1690, Loss: 0.6081, Training Acc: 0.7142\n",
            "Epoch: 1700, Loss: 0.6117, Training Acc: 0.7208\n",
            "Epoch: 1710, Loss: 0.6115, Training Acc: 0.7186\n",
            "Epoch: 1720, Loss: 0.6129, Training Acc: 0.7200\n",
            "Epoch: 1730, Loss: 0.6019, Training Acc: 0.7253\n",
            "Epoch: 1740, Loss: 0.6009, Training Acc: 0.7257\n",
            "Epoch: 1750, Loss: 0.6036, Training Acc: 0.7133\n",
            "Epoch: 1760, Loss: 0.5974, Training Acc: 0.7155\n",
            "Epoch: 1770, Loss: 0.6121, Training Acc: 0.7107\n",
            "Epoch: 1780, Loss: 0.6000, Training Acc: 0.7200\n",
            "Epoch: 1790, Loss: 0.6057, Training Acc: 0.7213\n",
            "Epoch: 1800, Loss: 0.6089, Training Acc: 0.7178\n",
            "Epoch: 1810, Loss: 0.5983, Training Acc: 0.7208\n",
            "Epoch: 1820, Loss: 0.5973, Training Acc: 0.7253\n",
            "Epoch: 1830, Loss: 0.5985, Training Acc: 0.7169\n",
            "Epoch: 1840, Loss: 0.6009, Training Acc: 0.7208\n",
            "Epoch: 1850, Loss: 0.5896, Training Acc: 0.7239\n",
            "Epoch: 1860, Loss: 0.6003, Training Acc: 0.7178\n",
            "Epoch: 1870, Loss: 0.5937, Training Acc: 0.7195\n",
            "Epoch: 1880, Loss: 0.6088, Training Acc: 0.7217\n",
            "Epoch: 1890, Loss: 0.5965, Training Acc: 0.7200\n",
            "Epoch: 1900, Loss: 0.5999, Training Acc: 0.7129\n",
            "Epoch: 1910, Loss: 0.6029, Training Acc: 0.7217\n",
            "Epoch: 1920, Loss: 0.6010, Training Acc: 0.7213\n",
            "Epoch: 1930, Loss: 0.5862, Training Acc: 0.7178\n",
            "Epoch: 1940, Loss: 0.5940, Training Acc: 0.7160\n",
            "Epoch: 1950, Loss: 0.6003, Training Acc: 0.7147\n",
            "Epoch: 1960, Loss: 0.5981, Training Acc: 0.7178\n",
            "Epoch: 1970, Loss: 0.6030, Training Acc: 0.7142\n",
            "Epoch: 1980, Loss: 0.6041, Training Acc: 0.7155\n",
            "Epoch: 1990, Loss: 0.5958, Training Acc: 0.7169\n",
            "Epoch: 2000, Loss: 0.5988, Training Acc: 0.7231\n",
            "Epoch: 2010, Loss: 0.5939, Training Acc: 0.7173\n",
            "Epoch: 2020, Loss: 0.5941, Training Acc: 0.7186\n",
            "Epoch: 2030, Loss: 0.5972, Training Acc: 0.7160\n",
            "Epoch: 2040, Loss: 0.5865, Training Acc: 0.7133\n",
            "Epoch: 2050, Loss: 0.5903, Training Acc: 0.7151\n",
            "Epoch: 2060, Loss: 0.5794, Training Acc: 0.7244\n",
            "Epoch: 2070, Loss: 0.5931, Training Acc: 0.7116\n",
            "Epoch: 2080, Loss: 0.5966, Training Acc: 0.7173\n",
            "Epoch: 2090, Loss: 0.5872, Training Acc: 0.7182\n",
            "Epoch: 2100, Loss: 0.5785, Training Acc: 0.7261\n",
            "Epoch: 2110, Loss: 0.5977, Training Acc: 0.7147\n",
            "Epoch: 2120, Loss: 0.5882, Training Acc: 0.7155\n",
            "Epoch: 2130, Loss: 0.5743, Training Acc: 0.7279\n",
            "Epoch: 2140, Loss: 0.5765, Training Acc: 0.7257\n",
            "Epoch: 2150, Loss: 0.5804, Training Acc: 0.7182\n",
            "Epoch: 2160, Loss: 0.5783, Training Acc: 0.7253\n",
            "Epoch: 2170, Loss: 0.5900, Training Acc: 0.7080\n",
            "Epoch: 2180, Loss: 0.5755, Training Acc: 0.7226\n",
            "Epoch: 2190, Loss: 0.5860, Training Acc: 0.7138\n",
            "Epoch: 2200, Loss: 0.5836, Training Acc: 0.7217\n",
            "Epoch: 2210, Loss: 0.5820, Training Acc: 0.7169\n",
            "Epoch: 2220, Loss: 0.5781, Training Acc: 0.7182\n",
            "Epoch: 2230, Loss: 0.5667, Training Acc: 0.7279\n",
            "Epoch: 2240, Loss: 0.5800, Training Acc: 0.7248\n",
            "Epoch: 2250, Loss: 0.5789, Training Acc: 0.7217\n",
            "Epoch: 2260, Loss: 0.5815, Training Acc: 0.7164\n",
            "Epoch: 2270, Loss: 0.5752, Training Acc: 0.7102\n",
            "Epoch: 2280, Loss: 0.5758, Training Acc: 0.7129\n",
            "Epoch: 2290, Loss: 0.5789, Training Acc: 0.7120\n",
            "Epoch: 2300, Loss: 0.5814, Training Acc: 0.7067\n",
            "Epoch: 2310, Loss: 0.5696, Training Acc: 0.7195\n",
            "Epoch: 2320, Loss: 0.5815, Training Acc: 0.7178\n",
            "Epoch: 2330, Loss: 0.5847, Training Acc: 0.7076\n",
            "Epoch: 2340, Loss: 0.5783, Training Acc: 0.7186\n",
            "Epoch: 2350, Loss: 0.5834, Training Acc: 0.7102\n",
            "Epoch: 2360, Loss: 0.5771, Training Acc: 0.7129\n",
            "Epoch: 2370, Loss: 0.5719, Training Acc: 0.7222\n",
            "Epoch: 2380, Loss: 0.5692, Training Acc: 0.7155\n",
            "Epoch: 2390, Loss: 0.5671, Training Acc: 0.7169\n",
            "Epoch: 2400, Loss: 0.5688, Training Acc: 0.7257\n"
          ]
        }
      ],
      "source": [
        "## GAT test\n",
        "\n",
        "#criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Data for animations\n",
        "embeddings = []\n",
        "losses = []\n",
        "accuracies = []\n",
        "outputs = []\n",
        "\n",
        "\n",
        "for epoch in range(2400):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    out = model(graph_object.x, graph_object.edge_index)\n",
        "\n",
        "    loss = F.cross_entropy(out[graph_object.train_mask], graph_object.y[graph_object.train_mask])\n",
        "\n",
        "    # Calculate accuracy\n",
        "    acc = accuracy(out.argmax(dim=1), graph_object.y)\n",
        "\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    losses.append(loss)\n",
        "    accuracies.append(acc)\n",
        "\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print('Epoch: {}, Loss: {:.4f}, Training Acc: {:.4f}'.format(epoch+1, loss.item(), acc))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1iUxtmZE5op",
        "outputId": "9f8ea965-76e2-4f07-d279-a818dae183a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.6779411764705883\n",
            "F1: 0.574757281553398\n",
            "Recall: 0.4713375796178344\n",
            "AUPRC:  0.5911721832948409\n",
            "AUC:  0.6632644182242177\n",
            "PRAUC:  0.7258868173184284\n",
            "false_alarm_rate:  0.29558823529411765\n"
          ]
        }
      ],
      "source": [
        "# GNN sklearn Metrics\n",
        "from sklearn.metrics import f1_score, accuracy_score, recall_score, average_precision_score, roc_auc_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn import metrics\n",
        "print('Accuracy: ', accuracy_score(graph_object.y[graph_object.test_mask], z.argmax(dim=1)[graph_object.test_mask]))\n",
        "\n",
        "print('F1:', f1_score(graph_object.y[graph_object.test_mask], z.argmax(dim=1)[graph_object.test_mask]))\n",
        "\n",
        "print('Recall:', recall_score(graph_object.y[graph_object.test_mask], z.argmax(dim=1)[graph_object.test_mask]))\n",
        "\n",
        "print('AUPRC: ', average_precision_score(graph_object.y[graph_object.test_mask], z.argmax(dim=1)[graph_object.test_mask]))\n",
        "\n",
        "print('AUC: ', roc_auc_score(graph_object.y[graph_object.test_mask], z.argmax(dim=1)[graph_object.test_mask]))\n",
        "precisionn, recalll, _ = precision_recall_curve(graph_object.y[graph_object.test_mask], z.argmax(dim=1)[graph_object.test_mask])\n",
        "pr_auc = metrics.auc(recalll, precisionn)\n",
        "print('PRAUC: ', pr_auc)\n",
        "\n",
        "false_alarm_rate = np.mean(z.argmax(dim=1)[graph_object.test_mask].numpy() == 1)\n",
        "print('false_alarm_rate: ', false_alarm_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87a0V1-0E6xX",
        "outputId": "a341286f-3d4b-436d-cdd5-f1c6e631176b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.6558823529411765\n",
            "F1: 0.4890829694323144\n",
            "Recall: 0.35668789808917195\n",
            "AUPRC:  0.5744827442654344\n",
            "AUC:  0.6346281020500504\n",
            "PRAUC:  0.7157622496981808\n",
            "false_alarm_rate:  0.21176470588235294\n"
          ]
        }
      ],
      "source": [
        "# GAT sklearn Metrics\n",
        "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score, roc_auc_score\n",
        "\n",
        "print('Accuracy: ', accuracy_score(graph_object.y[graph_object.test_mask], out.argmax(dim=1)[graph_object.test_mask]))\n",
        "\n",
        "print('F1:', f1_score(graph_object.y[graph_object.test_mask], out.argmax(dim=1)[graph_object.test_mask]))\n",
        "\n",
        "print('Recall:', recall_score(graph_object.y[graph_object.test_mask], out.argmax(dim=1)[graph_object.test_mask]))\n",
        "\n",
        "print('AUPRC: ', average_precision_score(graph_object.y[graph_object.test_mask], out.argmax(dim=1)[graph_object.test_mask]))\n",
        "\n",
        "print('AUC: ', roc_auc_score(graph_object.y[graph_object.test_mask], out.argmax(dim=1)[graph_object.test_mask]))\n",
        "precisionn, recalll, _ = precision_recall_curve(graph_object.y[graph_object.test_mask], out.argmax(dim=1)[graph_object.test_mask])\n",
        "pr_auc = metrics.auc(recalll, precisionn)\n",
        "print('PRAUC: ', pr_auc)\n",
        "\n",
        "false_alarm_rate = np.mean(out.argmax(dim=1)[graph_object.test_mask].numpy() == 1)\n",
        "print('false_alarm_rate: ', false_alarm_rate)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
