{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7KFN8G9hsbPI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "import math\n",
        "\n",
        "class GraphAttentionLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
        "        super(GraphAttentionLayer, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.alpha = alpha\n",
        "        self.concat = concat\n",
        "\n",
        "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
        "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
        "        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))\n",
        "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
        "\n",
        "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
        "\n",
        "    def forward(self, h, adj):\n",
        "        Wh = torch.mm(h, self.W) # h.shape: (N, in_features), Wh.shape: (N, out_features)\n",
        "        e = self._prepare_attentional_mechanism_input(Wh)\n",
        "\n",
        "        zero_vec = -9e15*torch.ones_like(e)\n",
        "        attention = torch.where(adj > 0, e, zero_vec)\n",
        "        attention = F.softmax(attention, dim=1)\n",
        "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
        "        h_prime = torch.matmul(attention, Wh)\n",
        "\n",
        "        if self.concat:\n",
        "            return F.elu(h_prime)\n",
        "        else:\n",
        "            return h_prime\n",
        "\n",
        "    def _prepare_attentional_mechanism_input(self, Wh):\n",
        "        # Wh.shape (N, out_feature)\n",
        "        # self.a.shape (2 * out_feature, 1)\n",
        "        # Wh1&2.shape (N, 1)\n",
        "        # e.shape (N, N)\n",
        "        Wh1 = torch.matmul(Wh, self.a[:self.out_features, :])\n",
        "        Wh2 = torch.matmul(Wh, self.a[self.out_features:, :])\n",
        "        # broadcast add\n",
        "        e = Wh1 + Wh2.T\n",
        "        return self.leakyrelu(e)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
        "\n",
        "\n",
        "class GraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        support = torch.mm(input, self.weight)\n",
        "        output = torch.spmm(adj, support)\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'\n",
        "############################################################################\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import os\n",
        "\n",
        "def normalize_features(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx\n",
        "\n",
        "def normalize_adj(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
        "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
        "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    probs = torch.exp(output)\n",
        "    preds = torch.argmax(probs, dim = 1)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)\n",
        "\n",
        "def load_multi_data(folder, att_file, edge_list_name):\n",
        "    att = pd.read_csv(folder + att_file)\n",
        "    edge_list = []\n",
        "    for name in edge_list_name:\n",
        "        edge_list.append(pd.read_csv(folder + name))\n",
        "\n",
        "    #get y and x\n",
        "    labels = np.array(att[\"SARSCoV2\"])\n",
        "    # b,c,d are age groups\n",
        "    features = sp.csr_matrix(att[[\n",
        "       'race', 'birth_sex', 'hiv_aids_age_yrs',\n",
        "       'trans_categ', 'clusterSize',\n",
        "       'cd4_group', 'sqnetwork_exp', 'eigen',\n",
        "       'clustering_coef', 'degree', 'log10vl',\n",
        "       #'priority_clusters',# need numrical\n",
        "       'dyad',\n",
        "       'ncovid',\n",
        "       'pcd0', 'pcd1', 'pcd2',  'page0', 'page1',\n",
        "      'ptrans1', 'ptrans2', 'ptrans3', 'ptrans5']])\n",
        "    #features = normalize_features(features)\n",
        "\n",
        "    #get adj mat\n",
        "    adj_list = []\n",
        "    for edge in edge_list:\n",
        "        #get row col idx for adj matrix\n",
        "        row_idx = []\n",
        "        col_idx = []\n",
        "        for i in range(edge.shape[0]):\n",
        "            id_from = edge.iloc[i,0]\n",
        "            id_to = edge.iloc[i,1]\n",
        "            if sum(att[\"uid\"] == id_from)>0:\n",
        "              row_id = att.index[att[\"uid\"] == id_from]\n",
        "              row_idx.append(row_id[0])\n",
        "\n",
        "            if sum(att[\"uid\"] == id_to)>0:\n",
        "              col_id = att.index[att[\"uid\"] == id_to]\n",
        "              col_idx.append(col_id[0])\n",
        "\n",
        "\n",
        "        adj = sp.coo_matrix((np.ones(edge.shape[0]), (row_idx, col_idx)), shape=(att.shape[0], att.shape[0]), dtype=np.float32)\n",
        "\n",
        "\n",
        "\n",
        "        #make adj symmetric\n",
        "        adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "        #normaliza adj\n",
        "        adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "        adj = torch.FloatTensor(np.array(adj.todense()))\n",
        "        adj_list.append(adj)\n",
        "\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    labels = torch.LongTensor(labels)\n",
        "\n",
        "    dim = len(labels)\n",
        "    idx_train = range(1915)\n",
        "    idx_test = range(1915, dim)\n",
        "\n",
        "    return adj_list, features, labels, idx_train, idx_test\n",
        "\n",
        "def load_data(folder, att_file, edge_name):\n",
        "    att = pd.read_csv(folder + att_file)\n",
        "    edge_list = pd.read_csv(folder + edge_name)\n",
        "\n",
        "    #get y and x\n",
        "    labels = np.array(att[\"SARSCoV2\"])\n",
        "    # b,c,d are age groups\n",
        "\n",
        "    features = sp.csr_matrix(att[[\n",
        "       'race', 'birth_sex', 'hiv_aids_age_yrs',\n",
        "       'trans_categ', 'clusterSize',\n",
        "       'cd4_group', 'sqnetwork_exp', 'eigen',\n",
        "       'clustering_coef', 'degree', 'log10vl',\n",
        "       #'priority_clusters',# need numrical\n",
        "       'dyad',\n",
        "       'ncovid',\n",
        "       'pcd0', 'pcd1', 'pcd2',  'page0', 'page1',\n",
        "      'ptrans1', 'ptrans2', 'ptrans3', 'ptrans5']])\n",
        "    #features = normalize_features(features)\n",
        "\n",
        "    #get adj mat\n",
        "    row_idx = []\n",
        "    col_idx = []\n",
        "    for i in range(edge_list.shape[0]):\n",
        "        id_from = edge_list.iloc[i,0]\n",
        "        id_to = edge_list.iloc[i,1]\n",
        "        if sum(att[\"uid\"] == id_from)>0:\n",
        "          row_id = att.index[att[\"uid\"] == id_from]\n",
        "          row_idx.append(row_id[0])\n",
        "\n",
        "        if sum(att[\"uid\"] == id_to)>0:\n",
        "          col_id = att.index[att[\"uid\"] == id_to]\n",
        "          col_idx.append(col_id[0])\n",
        "\n",
        "\n",
        "    adj = sp.coo_matrix((np.ones(edge_list.shape[0]), (row_idx, col_idx)), shape=(att.shape[0], att.shape[0]), dtype=np.float32)\n",
        "\n",
        "        #make adj symmetric\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "        #normaliza adj\n",
        "    adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
        "\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    labels = torch.LongTensor(labels)\n",
        "\n",
        "    dim = len(labels)\n",
        "    idx_train = range(1915)\n",
        "    idx_test = range(1915, dim)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_test\n",
        "    ##########################################################################\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "class GAT(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
        "        \"\"\"Dense version of GAT.\"\"\"\n",
        "        super(GAT, self).__init__()\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
        "        for i, attention in enumerate(self.attentions):\n",
        "            self.add_module('attention_{}'.format(i), attention)\n",
        "\n",
        "        self.out_att = nn.Linear(nhid * nheads, nclass)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = F.elu(self.out_att(x))\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "class FusionGAT(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, adj_list, nheads):\n",
        "        super(FusionGAT, self).__init__()\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.nheads = nheads\n",
        "        self.adj_list = adj_list\n",
        "\n",
        "        # Define list of GAT layers for each adjacency matrix\n",
        "        self.attentions = nn.ModuleList()\n",
        "        for i in range(len(adj_list)):\n",
        "            att_list = nn.ModuleList([GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)])\n",
        "            self.attentions.append(att_list)\n",
        "            for k, attention in enumerate(self.attentions):\n",
        "                self.add_module('adj{}, attention_{}'.format(i, k), attention)\n",
        "\n",
        "        # Define linear layer for integration with L1 regularization\n",
        "        self.integration_att = nn.Linear(nhid * nheads, nclass)\n",
        "        self.fusion = nn.Linear(nclass * len(adj_list), nclass)\n",
        "        self.l1_reg = nn.L1Loss(reduction='mean')\n",
        "\n",
        "    def forward(self, x, adj_list):\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        # Compute output for each adjacency matrix using GAT layers\n",
        "        output_list = []\n",
        "        for i, adj in enumerate(adj_list):\n",
        "            x_i = torch.cat([att(x, adj) for att in self.attentions[i]], dim=1)\n",
        "            x_i = F.dropout(x_i, self.dropout, training=self.training)\n",
        "            x_i = F.elu(self.integration_att(x_i))\n",
        "            output_list.append(x_i)\n",
        "        output = torch.cat(output_list, dim=1)\n",
        "\n",
        "        # Apply linear layer for integration with L1 regularization\n",
        "        output = F.dropout(output, self.dropout, training=self.training)\n",
        "        output = self.fusion(output.view(output.size(0), -1))\n",
        "        l1_loss = self.l1_reg(self.fusion.weight, torch.zeros_like(self.fusion.weight))\n",
        "        return F.log_softmax(output, dim=1), l1_loss\n",
        "\n",
        "\n",
        "\n",
        "class FusionGAT2(nn.Module):\n",
        "    def __init__(self, nfeat, nhid1, nhid2, nclass, dropout, alpha, adj_list, nheads):\n",
        "        super(FusionGAT2, self).__init__()\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.nheads = nheads\n",
        "        self.adj_list = adj_list\n",
        "\n",
        "        # Define list of GAT layers for each adjacency matrix in attention 1\n",
        "        self.attentions1 = nn.ModuleList()\n",
        "        for i in range(len(adj_list)):\n",
        "            att_list = nn.ModuleList([GraphAttentionLayer(nfeat, nhid1, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)])\n",
        "            self.attentions1.append(att_list)\n",
        "            for k, attention in enumerate(self.attentions1):\n",
        "                self.add_module('adj{}, attention_layer1_{}'.format(i, k), attention)\n",
        "\n",
        "        # Define linear layer for integration of multihead attention1\n",
        "        self.integration_att1 = nn.Linear(nhid1 * nheads, nhid1)\n",
        "\n",
        "        self.attentions2 = nn.ModuleList()\n",
        "        for i in range(len(adj_list)):\n",
        "            att_list2 = nn.ModuleList([GraphAttentionLayer(nhid1, nhid2, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)])\n",
        "            self.attentions2.append(att_list2)\n",
        "            for k, attention in enumerate(self.attentions2):\n",
        "                self.add_module('adj{}, attention_layer2_{}'.format(i, k), attention)\n",
        "\n",
        "        # Define linear layer for integration of multihead attention2\n",
        "        self.integration_att2 = nn.Linear(nhid2 * nheads, nclass)\n",
        "\n",
        "        #fusion layer with l1 penalty\n",
        "        self.fusion_att = nn.Linear(nclass * len(adj_list), nclass)\n",
        "        self.l1_reg = nn.L1Loss(reduction='mean')\n",
        "\n",
        "    def forward(self, x, adj_list):\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        # Compute output for each adjacency matrix using GAT layers\n",
        "        output_list = []\n",
        "        for i, adj in enumerate(adj_list):\n",
        "            #attention layer 1\n",
        "            x_i = torch.cat([att(x, adj) for att in self.attentions1[i]], dim=1)\n",
        "            x_i = F.dropout(x_i, self.dropout, training=self.training)\n",
        "            x_i = F.elu(self.integration_att1(x_i))\n",
        "\n",
        "            #attention layer 2\n",
        "            x_i = torch.cat([att(x_i, adj) for att in self.attentions2[i]], dim=1)\n",
        "            x_i = F.dropout(x_i, self.dropout, training=self.training)\n",
        "            x_i = F.elu(self.integration_att2(x_i))\n",
        "            output_list.append(x_i)\n",
        "\n",
        "        output = torch.cat(output_list, dim=1)\n",
        "        output = F.dropout(output, self.dropout, training=self.training)\n",
        "        output = self.fusion_att(output.view(output.size(0), -1))\n",
        "        l1_loss = self.l1_reg(self.fusion_att.weight, torch.zeros_like(self.fusion_att.weight))\n",
        "        return F.log_softmax(output, dim=1), l1_loss\n",
        "\n",
        "\n",
        "\n",
        "class FusionGAT3(nn.Module):\n",
        "    def __init__(self, nfeat, nhid1, nhid2, fusion1_dim, nclass, dropout, alpha, adj_list, nheads):\n",
        "        super(FusionGAT3, self).__init__()\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.nheads = nheads\n",
        "        self.adj_list = adj_list\n",
        "\n",
        "        # Define list of GAT layers for each adjacency matrix\n",
        "        #att 1\n",
        "        self.attentions1 = nn.ModuleList()\n",
        "        for i in range(len(adj_list)):\n",
        "            att_list = nn.ModuleList([GraphAttentionLayer(nfeat, nhid1, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)])\n",
        "            self.attentions1.append(att_list)\n",
        "            for k, attention in enumerate(self.attentions1):\n",
        "                self.add_module('adj{}, attention_layer1_{}'.format(i, k), attention)\n",
        "\n",
        "        # fusion1\n",
        "        self.integration_att1 = nn.Linear(nhid1 * nheads, fusion1_dim)\n",
        "        self.fusion_att1 = nn.Linear(fusion1_dim * len(adj_list), fusion1_dim)\n",
        "        self.l1_reg1 = nn.L1Loss(reduction='mean')\n",
        "\n",
        "        #att 2\n",
        "        self.attentions2 = nn.ModuleList()\n",
        "        for i in range(len(adj_list)):\n",
        "            att_list = nn.ModuleList([GraphAttentionLayer(fusion1_dim, nhid2, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)])\n",
        "            self.attentions2.append(att_list)\n",
        "            for k, attention in enumerate(self.attentions2):\n",
        "                self.add_module('adj{}, attention_layer2_{}'.format(i, k), attention)\n",
        "\n",
        "        #fusion2\n",
        "        self.integration_att2 = nn.Linear(nhid2 * nheads, nclass)\n",
        "        self.fusion_att2 = nn.Linear(nclass * len(adj_list), nclass)\n",
        "        self.l1_reg2 = nn.L1Loss(reduction='mean')\n",
        "\n",
        "\n",
        "    def forward(self, x, adj_list):\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "\n",
        "        # Compute output for each adjacency matrix using GAT layers\n",
        "        output_list = []\n",
        "        for i, adj in enumerate(adj_list):\n",
        "            x_i = x\n",
        "            x_i = torch.cat([att(x, adj) for att in self.attentions1[i]], dim=1)\n",
        "            x_i = F.dropout(x_i, self.dropout, training=self.training)\n",
        "            x_i = F.elu(self.integration_att1(x_i))\n",
        "            output_list.append(x_i)\n",
        "        output = torch.cat(output_list, dim=1)\n",
        "\n",
        "        # Apply linear layer for integration with L1 regularization\n",
        "        output = F.dropout(output, self.dropout, training=self.training)\n",
        "        output = self.fusion_att1(output.view(output.size(0), -1))\n",
        "        l1_loss1 = self.l1_reg1(self.fusion_att1.weight, torch.zeros_like(self.fusion_att1.weight))\n",
        "\n",
        "\n",
        "        output_list2 = []\n",
        "        for i, adj in enumerate(adj_list):\n",
        "            x_i = torch.cat([att(output, adj) for att in self.attentions2[i]], dim=1)\n",
        "            x_i = F.dropout(x_i, self.dropout, training=self.training)\n",
        "            x_i = F.elu(self.integration_att2(x_i))\n",
        "            output_list2.append(x_i)\n",
        "        output2 = torch.cat(output_list2, dim=1)\n",
        "\n",
        "        # Apply linear layer for integration with L1 regularization\n",
        "        output2 = F.dropout(output2, self.dropout, training=self.training)\n",
        "        output2 = self.fusion_att2(output2.view(output.size(0), -1))\n",
        "        l1_loss2 = self.l1_reg2(self.fusion_att2.weight, torch.zeros_like(self.fusion_att2.weight))\n",
        "\n",
        "        return F.log_softmax(output2, dim=1), l1_loss1 + l1_loss2\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = F.relu(self.gc1(x, adj))\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.gc2(x, adj)\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "\n",
        "import time\n",
        "import json\n",
        "\n",
        "#from load import accuracy, load_multi_data\n",
        "#from models import FusionGAT3\n",
        "\n",
        "\n",
        "seed = 72\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output, l1_loss = model(features, adj_list_tt)\n",
        "    loss_train = F.nll_loss(output[idx_train], labels[idx_train]) + l1_loss*lambda_l1\n",
        "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test]) + l1_loss*lambda_l1\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    y_prob = torch.exp(output[:, 1]).cpu().detach().numpy()\n",
        "\n",
        "    auc_score = roc_auc_score(labels[idx_test].cpu().detach().numpy(), y_prob[idx_test.cpu().detach().numpy()])\n",
        "    print('Epoch: {:03d}'.format(epoch+1),\n",
        "          'loss_train: {:.3f}'.format(loss_train.data.item()),\n",
        "          'acc_train: {:.3f}'.format(acc_train.data.item()),\n",
        "          'loss_test: {:.3f}'.format(loss_test.data.item()),\n",
        "          'acc_test: {:.3f}'.format(acc_test.data.item()),\n",
        "          \"AUC = {:.2f}\".format(auc_score),\n",
        "          'time: {:.3f}s'.format(time.time() - t))\n",
        "\n",
        "    return loss_test.data.item(), acc_test, auc_score\n",
        "\n",
        "def compute_test(model):\n",
        "    model.eval()\n",
        "    output, l1_loss = model(features, adj_list_tt)\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test]) + l1_loss*lambda_l1\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    y_prob = torch.exp(output[:, 1]).cpu().detach().numpy()\n",
        "\n",
        "    auc_score = roc_auc_score(labels[idx_test].cpu().detach().numpy(), y_prob[idx_test.cpu().detach().numpy()])\n",
        "    # compute false alarm rate and f1 score\n",
        "    y_pred = np.array([0 if p < 0.5 else 1 for p in y_prob])\n",
        "    false_alarm_rate = np.mean(y_pred[idx_test.cpu().detach().numpy()] == 1)\n",
        "    f1score = f1_score(labels[idx_test].cpu().detach().numpy(), y_pred[idx_test.cpu().detach().numpy()])\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.data.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.data.item()),\n",
        "          \"AUC = {:.2f}\".format(auc_score),\n",
        "          \"False Alarm Rate = {:.4f}\".format(false_alarm_rate),\n",
        "          \"F1 Score = {:.4f}\".format(f1score))\n",
        "    return loss_test.data.item(), acc_test.data.item(), auc_score, false_alarm_rate, f1score, y_pred\n",
        "\n",
        "\n",
        "#parameter setting\n",
        "#\n",
        "\n",
        "cuda = True\n",
        "epochs = 500\n",
        "#lrs = [0.0005, 0.001]\n",
        "#weight_decays = [5e-5, 5e-4]\n",
        "#dropouts = [0.3, 0.5]\n",
        "#hidden_dims1 = [16, 64]\n",
        "#hidden_dims2 = [16, 64]\n",
        "#fusion1_dims = [2,4]\n",
        "#nb_heads = [8, 12]\n",
        "#alpha = 0.2 #Alpha for the leaky_relu\n",
        "#lambda_l1s = [0.0001, 0.001, 0.01]\n",
        "#patience = 50\n",
        "\n",
        "\n",
        "lrs = [0.0005]\n",
        "weight_decays = [0.00005]\n",
        "dropouts = [0.3]\n",
        "hidden_dims1 = [64]\n",
        "hidden_dims2 = [64]\n",
        "fusion1_dims = [4]\n",
        "nb_heads = [8]\n",
        "alpha = 0.2 #Alpha for the leaky_relu\n",
        "lambda_l1s = [0.0001, 0.001, 0.01]\n",
        "patience = 50\n",
        "\n",
        "\n",
        "# Load data\n",
        "sv_path = \"\"\n",
        "folder = \"\"\n",
        "att_name = \"att_RR3.csv\"\n",
        "edge_list_name = \"edge_sqRR3.csv\"\n",
        "adj_list, features, labels, idx_train, idx_test = load_multi_data(folder, att_name, edge_list_name)\n",
        "\n",
        "adj_list_tensor = torch.stack(adj_list)\n",
        "features, adj_list_tt, labels = Variable(features), adj_list_tensor, Variable(labels)\n",
        "\n",
        "count = 0\n",
        "for lr in lrs:\n",
        "    for dropout in dropouts:\n",
        "        for hid1 in hidden_dims1:\n",
        "            for hid2 in hidden_dims2:\n",
        "                for fusion1_dim in fusion1_dims:\n",
        "                    for nb_head in nb_heads:\n",
        "                        for weight_decay in weight_decays:\n",
        "                            for lambda_l1 in lambda_l1s:\n",
        "\n",
        "                                # Model and optimizer\n",
        "                                model = FusionGAT3(nfeat=features.shape[1],\n",
        "                                            nhid1=hid1,\n",
        "                                            nhid2=hid2,\n",
        "                                            fusion1_dim = fusion1_dim,\n",
        "                                            nclass=int(labels.max()) + 1,\n",
        "                                            dropout=dropout,\n",
        "                                            alpha=alpha,\n",
        "                                            adj_list= adj_list_tt,\n",
        "                                            nheads = nb_head)\n",
        "\n",
        "                                optimizer = optim.Adam(model.parameters(),\n",
        "                                                    lr=lr,\n",
        "                                                    weight_decay=weight_decay)\n",
        "\n",
        "                                if cuda:\n",
        "                                    model.cuda()\n",
        "                                    features = features.cuda()\n",
        "                                    adj_list_tt = adj_list_tt.cuda()\n",
        "                                    labels = labels.cuda()\n",
        "                                    idx_train = torch.tensor(idx_train).cuda()\n",
        "                                    idx_test = torch.tensor(idx_test).cuda()\n",
        "\n",
        "\n",
        "                                # Train model\n",
        "                                t_total = time.time()\n",
        "                                bad_counter = 0\n",
        "                                best_auc = -1\n",
        "                                best_epoch = 0\n",
        "                                for epoch in range(epochs):\n",
        "                                    loss, acc, auc = train(epoch)\n",
        "\n",
        "                                    torch.save(model.state_dict(), sv_path + '{}.pkl'.format(epoch))\n",
        "                                    if auc > best_auc:\n",
        "                                        best_auc = auc\n",
        "                                        best_epoch = epoch\n",
        "                                        bad_counter = 0\n",
        "                                    else:\n",
        "                                        bad_counter += 1\n",
        "\n",
        "                                    if bad_counter == patience:\n",
        "                                        break\n",
        "\n",
        "                                files = glob.glob(sv_path +'*.pkl')\n",
        "                                for file in files:\n",
        "\n",
        "                                    filename = file.split('/')[-1]\n",
        "\n",
        "                                    epoch_nb = int(filename.split('.')[0])\n",
        "                                    if epoch_nb != best_epoch:\n",
        "                                        os.remove(file)\n",
        "\n",
        "                                print(\"Optimization Finished!\")\n",
        "                                print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "                                # Restore best model\n",
        "                                print('Loading {}th epoch'.format(best_epoch))\n",
        "                                model.load_state_dict(torch.load(sv_path +'{}.pkl'.format(best_epoch)))\n",
        "\n",
        "                                # Testing\n",
        "                                test_loss, test_acc, auc, far, f1, y_pred = compute_test(model)\n",
        "                                hyper_para = {}\n",
        "                                hyper_para[\"lr\"] = lr\n",
        "                                hyper_para[\"weight_decay\"] = weight_decay\n",
        "                                hyper_para[\"dropout\"] = dropout\n",
        "                                hyper_para[\"hidden_dim1\"] = hid1\n",
        "                                hyper_para[\"hidden_dim2\"] = hid2\n",
        "                                hyper_para[\"lambda\"] = lambda_l1\n",
        "                                hyper_para[\"fusion1_dim\"] = fusion1_dim\n",
        "                                hyper_para[\"nb_heads\"] = nb_head\n",
        "                                hyper_para[\"alpha\"] = alpha\n",
        "                                hyper_para[\"loss\"] = test_loss\n",
        "                                hyper_para[\"accuracy\"] = test_acc\n",
        "                                hyper_para[\"auc\"] = auc\n",
        "                                hyper_para[\"false_alarm_rate\"] = far\n",
        "                                hyper_para[\"f1_score\"] = f1\n",
        "                                with open(sv_path +\"hyperpara.json\", \"a+\") as fp:\n",
        "                                    fp.write('\\n')\n",
        "                                    json.dump(hyper_para, fp)"
      ],
      "metadata": {
        "id": "XjFQSN02HAaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "\n",
        "import time\n",
        "import json\n",
        "\n",
        "#from load import accuracy, load_multi_data\n",
        "#from models import FusionGAT3\n",
        "\n",
        "\n",
        "seed = 72\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output, l1_loss = model(features, adj_list_tt)\n",
        "    loss_train = F.nll_loss(output[idx_train], labels[idx_train]) + l1_loss*lambda_l1\n",
        "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test]) + l1_loss*lambda_l1\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    y_prob = torch.exp(output[:, 1]).cpu().detach().numpy()\n",
        "\n",
        "    auc_score = roc_auc_score(labels[idx_test].cpu().detach().numpy(), y_prob[idx_test.cpu().detach().numpy()])\n",
        "    print('Epoch: {:03d}'.format(epoch+1),\n",
        "          'loss_train: {:.3f}'.format(loss_train.data.item()),\n",
        "          'acc_train: {:.3f}'.format(acc_train.data.item()),\n",
        "          'loss_test: {:.3f}'.format(loss_test.data.item()),\n",
        "          'acc_test: {:.3f}'.format(acc_test.data.item()),\n",
        "          \"AUC = {:.2f}\".format(auc_score),\n",
        "          'time: {:.3f}s'.format(time.time() - t))\n",
        "\n",
        "    return loss_test.data.item(), acc_test, auc_score\n",
        "\n",
        "def compute_test(model):\n",
        "    model.eval()\n",
        "    output, l1_loss = model(features, adj_list_tt)\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test]) + l1_loss*lambda_l1\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    y_prob = torch.exp(output[:, 1]).cpu().detach().numpy()\n",
        "\n",
        "    auc_score = roc_auc_score(labels[idx_test].cpu().detach().numpy(), y_prob[idx_test.cpu().detach().numpy()])\n",
        "    # compute false alarm rate and f1 score\n",
        "    y_pred = np.array([0 if p < 0.5 else 1 for p in y_prob])\n",
        "    false_alarm_rate = np.mean(y_pred[idx_test.cpu().detach().numpy()] == 1)\n",
        "    f1score = f1_score(labels[idx_test].cpu().detach().numpy(), y_pred[idx_test.cpu().detach().numpy()])\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.data.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.data.item()),\n",
        "          \"AUC = {:.2f}\".format(auc_score),\n",
        "          \"False Alarm Rate = {:.4f}\".format(false_alarm_rate),\n",
        "          \"F1 Score = {:.4f}\".format(f1score))\n",
        "    return loss_test.data.item(), acc_test.data.item(), auc_score, false_alarm_rate, f1score, y_pred\n",
        "\n",
        "\n",
        "#parameter setting\n",
        "#\n",
        "\n",
        "cuda = True\n",
        "epochs = 500\n",
        "lrs = [0.0005, 0.001]\n",
        "weight_decays = [5e-5, 5e-4]\n",
        "dropouts = [0.3,0.5]\n",
        "hidden_dims1 = [64]\n",
        "hidden_dims2 = [64]\n",
        "fusion1_dims = [4]\n",
        "nb_heads = [8]\n",
        "alpha = 0.2 #Alpha for the leaky_relu\n",
        "lambda_l1s = [0.0001, 0.001, 0.01]\n",
        "patience = 50\n",
        "\n",
        "\n",
        "#lrs = [0.0005]\n",
        "#weight_decays = [0.00005]\n",
        "#dropouts = [0.3]\n",
        "#hidden_dims1 = [64]\n",
        "#hidden_dims2 = [64]\n",
        "#fusion1_dims = [4]\n",
        "#nb_heads = [8]\n",
        "#alpha = 0.2 #Alpha for the leaky_relu\n",
        "#lambda_l1s = [0.0001, 0.001, 0.01]\n",
        "#patience = 50\n",
        "\n",
        "\n",
        "# Load data\n",
        "sv_path = \"\"\n",
        "folder = \"\"\n",
        "att_name = \"att_RR3.csv\"\n",
        "edge_list_name = \"edge_sqRR3.csv\"\n",
        "\n",
        "adj_list, features, labels, idx_train, idx_test = load_multi_data(folder, att_name, edge_list_name)\n",
        "\n",
        "adj_list_tensor = torch.stack(adj_list)\n",
        "features, adj_list_tt, labels = Variable(features), adj_list_tensor, Variable(labels)\n",
        "\n",
        "count = 0\n",
        "for lr in lrs:\n",
        "    for dropout in dropouts:\n",
        "        for hid1 in hidden_dims1:\n",
        "            for hid2 in hidden_dims2:\n",
        "                    for nb_head in nb_heads:\n",
        "                        for weight_decay in weight_decays:\n",
        "                            for lambda_l1 in lambda_l1s:\n",
        "\n",
        "                                # Model and optimizer\n",
        "                                model = FusionGAT2(nfeat=features.shape[1],\n",
        "                                            nhid1=hid1,\n",
        "                                            nhid2=hid2,\n",
        "                                            nclass=int(labels.max()) + 1,\n",
        "                                            dropout=dropout,\n",
        "                                            alpha=alpha,\n",
        "                                            adj_list= adj_list_tt,\n",
        "                                            nheads = nb_head)\n",
        "\n",
        "                                optimizer = optim.Adam(model.parameters(),\n",
        "                                                    lr=lr,\n",
        "                                                    weight_decay=weight_decay)\n",
        "\n",
        "                                if cuda:\n",
        "                                    model.cuda()\n",
        "                                    features = features.cuda()\n",
        "                                    adj_list_tt = adj_list_tt.cuda()\n",
        "                                    labels = labels.cuda()\n",
        "                                    idx_train = torch.tensor(idx_train).cuda()\n",
        "                                    idx_test = torch.tensor(idx_test).cuda()\n",
        "\n",
        "\n",
        "                                # Train model\n",
        "                                t_total = time.time()\n",
        "                                bad_counter = 0\n",
        "                                best_auc = -1\n",
        "                                best_epoch = 0\n",
        "                                for epoch in range(epochs):\n",
        "                                    loss, acc, auc = train(epoch)\n",
        "\n",
        "                                    torch.save(model.state_dict(), sv_path + '{}.pkl'.format(epoch))\n",
        "                                    if auc > best_auc:\n",
        "                                        best_auc = auc\n",
        "                                        best_epoch = epoch\n",
        "                                        bad_counter = 0\n",
        "                                    else:\n",
        "                                        bad_counter += 1\n",
        "\n",
        "                                    if bad_counter == patience:\n",
        "                                        break\n",
        "\n",
        "                                files = glob.glob(sv_path +'*.pkl')\n",
        "                                for file in files:\n",
        "                                    filename = file.split('/')[-1]\n",
        "                                    epoch_nb = int(filename.split('.')[0])\n",
        "                                    if epoch_nb != best_epoch:\n",
        "                                        os.remove(file)\n",
        "\n",
        "                                print(\"Optimization Finished!\")\n",
        "                                print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "                                # Restore best model\n",
        "                                print('Loading {}th epoch'.format(best_epoch))\n",
        "                                model.load_state_dict(torch.load(sv_path +'{}.pkl'.format(best_epoch)))\n",
        "\n",
        "                                # Testing\n",
        "                                test_loss, test_acc, auc, far, f1, y_pred = compute_test(model)\n",
        "                                hyper_para = {}\n",
        "                                hyper_para[\"lr\"] = lr\n",
        "                                hyper_para[\"weight_decay\"] = weight_decay\n",
        "                                hyper_para[\"dropout\"] = dropout\n",
        "                                hyper_para[\"hidden_dim1\"] = hid1\n",
        "                                hyper_para[\"hidden_dim2\"] = hid2\n",
        "                                hyper_para[\"lambda\"] = lambda_l1\n",
        "\n",
        "                                hyper_para[\"nb_heads\"] = nb_head\n",
        "                                hyper_para[\"alpha\"] = alpha\n",
        "                                hyper_para[\"loss\"] = test_loss\n",
        "                                hyper_para[\"accuracy\"] = test_acc\n",
        "                                hyper_para[\"auc\"] = auc\n",
        "                                hyper_para[\"false_alarm_rate\"] = far\n",
        "                                hyper_para[\"f1_score\"] = f1\n",
        "                                with open(sv_path +\"hyperpara.json\", \"a+\") as fp:\n",
        "                                    fp.write('\\n')\n",
        "                                    json.dump(hyper_para, fp)"
      ],
      "metadata": {
        "id": "fTnhy6gSG9zC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "\n",
        "import time\n",
        "import json\n",
        "\n",
        "#from load import accuracy, load_multi_data\n",
        "#from models import FusionGAT3\n",
        "\n",
        "\n",
        "seed = 72\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output, l1_loss = model(features, adj_list_tt)\n",
        "    loss_train = F.nll_loss(output[idx_train], labels[idx_train]) + l1_loss*lambda_l1\n",
        "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test]) + l1_loss*lambda_l1\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    y_prob = torch.exp(output[:, 1]).cpu().detach().numpy()\n",
        "\n",
        "    auc_score = roc_auc_score(labels[idx_test].cpu().detach().numpy(), y_prob[idx_test.cpu().detach().numpy()])\n",
        "    print('Epoch: {:03d}'.format(epoch+1),\n",
        "          'loss_train: {:.3f}'.format(loss_train.data.item()),\n",
        "          'acc_train: {:.3f}'.format(acc_train.data.item()),\n",
        "          'loss_test: {:.3f}'.format(loss_test.data.item()),\n",
        "          'acc_test: {:.3f}'.format(acc_test.data.item()),\n",
        "          \"AUC = {:.2f}\".format(auc_score),\n",
        "          'time: {:.3f}s'.format(time.time() - t))\n",
        "\n",
        "    return loss_test.data.item(), acc_test, auc_score\n",
        "\n",
        "def compute_test(model):\n",
        "    model.eval()\n",
        "    output, l1_loss = model(features, adj_list_tt)\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test]) + l1_loss*lambda_l1\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    y_prob = torch.exp(output[:, 1]).cpu().detach().numpy()\n",
        "\n",
        "    auc_score = roc_auc_score(labels[idx_test].cpu().detach().numpy(), y_prob[idx_test.cpu().detach().numpy()])\n",
        "    # compute false alarm rate and f1 score\n",
        "    y_pred = np.array([0 if p < 0.5 else 1 for p in y_prob])\n",
        "    false_alarm_rate = np.mean(y_pred[idx_test.cpu().detach().numpy()] == 1)\n",
        "    f1score = f1_score(labels[idx_test].cpu().detach().numpy(), y_pred[idx_test.cpu().detach().numpy()])\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.data.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.data.item()),\n",
        "          \"AUC = {:.2f}\".format(auc_score),\n",
        "          \"False Alarm Rate = {:.4f}\".format(false_alarm_rate),\n",
        "          \"F1 Score = {:.4f}\".format(f1score))\n",
        "    return loss_test.data.item(), acc_test.data.item(), auc_score, false_alarm_rate, f1score, y_pred\n",
        "\n",
        "\n",
        "#parameter setting\n",
        "#\n",
        "\n",
        "cuda = True\n",
        "epochs = 500\n",
        "\n",
        "lrs = [0.0005, 0.001]\n",
        "weight_decays = [5e-5, 5e-4]\n",
        "dropouts = [0.3,0.5]\n",
        "hidden_dims1 = [64]\n",
        "hidden_dims2 = [64]\n",
        "fusion1_dims = [4]\n",
        "nb_heads = [8]\n",
        "alpha = 0.2 #Alpha for the leaky_relu\n",
        "lambda_l1s = [0.0001, 0.001, 0.01]\n",
        "patience = 50\n",
        "\n",
        "\n",
        "#lrs = [0.0005]\n",
        "#weight_decays = [0.00005]\n",
        "#dropouts = [0.3]\n",
        "#hidden_dims1 = [64]\n",
        "#hidden_dims2 = [64]\n",
        "#fusion1_dims = [4]\n",
        "#nb_heads = [8]\n",
        "#alpha = 0.2 #Alpha for the leaky_relu\n",
        "#lambda_l1s = [0.0001, 0.001, 0.01]\n",
        "#patience = 50\n",
        "\n",
        "\n",
        "# Load data\n",
        "sv_path = \"\"\n",
        "folder = \"\"\n",
        "att_name = \"att_RR3.csv\"\n",
        "edge_list_name = \"edge_sqRR3.csv\"\n",
        "\n",
        "adj_list, features, labels, idx_train, idx_test = load_multi_data(folder, att_name, edge_list_name)\n",
        "\n",
        "adj_list_tensor = torch.stack(adj_list)\n",
        "features, adj_list_tt, labels = Variable(features), adj_list_tensor, Variable(labels)\n",
        "\n",
        "\n",
        "count = 0\n",
        "for lr in lrs:\n",
        "    for dropout in dropouts:\n",
        "        for hid1 in hidden_dims1:\n",
        "                    for nb_head in nb_heads:\n",
        "                        for weight_decay in weight_decays:\n",
        "                            for lambda_l1 in lambda_l1s:\n",
        "\n",
        "                                # Model and optimizer\n",
        "                                model = FusionGAT(nfeat=features.shape[1],\n",
        "                                            nhid=hid1,\n",
        "\n",
        "                                            nclass=int(labels.max()) + 1,\n",
        "                                            dropout=dropout,\n",
        "                                            alpha=alpha,\n",
        "                                            adj_list= adj_list_tt,\n",
        "                                            nheads = nb_head)\n",
        "\n",
        "                                optimizer = optim.Adam(model.parameters(),\n",
        "                                                    lr=lr,\n",
        "                                                    weight_decay=weight_decay)\n",
        "\n",
        "                                if cuda:\n",
        "                                    model.cuda()\n",
        "                                    features = features.cuda()\n",
        "                                    adj_list_tt = adj_list_tt.cuda()\n",
        "                                    labels = labels.cuda()\n",
        "                                    idx_train = torch.tensor(idx_train).cuda()\n",
        "                                    idx_test = torch.tensor(idx_test).cuda()\n",
        "\n",
        "\n",
        "                                # Train model\n",
        "                                t_total = time.time()\n",
        "                                bad_counter = 0\n",
        "                                best_auc = -1\n",
        "                                best_epoch = 0\n",
        "                                for epoch in range(epochs):\n",
        "                                    loss, acc, auc = train(epoch)\n",
        "\n",
        "                                    torch.save(model.state_dict(), sv_path + '{}.pkl'.format(epoch))\n",
        "                                    if auc > best_auc:\n",
        "                                        best_auc = auc\n",
        "                                        best_epoch = epoch\n",
        "                                        bad_counter = 0\n",
        "                                    else:\n",
        "                                        bad_counter += 1\n",
        "\n",
        "                                    if bad_counter == patience:\n",
        "                                        break\n",
        "\n",
        "                                files = glob.glob(sv_path +'*.pkl')\n",
        "                                for file in files:\n",
        "                                    filename = file.split('/')[-1]\n",
        "                                    epoch_nb = int(filename.split('.')[0])\n",
        "                                    if epoch_nb != best_epoch:\n",
        "                                        os.remove(file)\n",
        "\n",
        "                                print(\"Optimization Finished!\")\n",
        "                                print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "                                # Restore best model\n",
        "                                print('Loading {}th epoch'.format(best_epoch))\n",
        "                                model.load_state_dict(torch.load(sv_path +'{}.pkl'.format(best_epoch)))\n",
        "\n",
        "                                # Testing\n",
        "                                test_loss, test_acc, auc, far, f1, y_pred = compute_test(model)\n",
        "                                hyper_para = {}\n",
        "                                hyper_para[\"lr\"] = lr\n",
        "                                hyper_para[\"weight_decay\"] = weight_decay\n",
        "                                hyper_para[\"dropout\"] = dropout\n",
        "                                hyper_para[\"hidden_dim1\"] = hid1\n",
        "\n",
        "                                hyper_para[\"lambda\"] = lambda_l1\n",
        "\n",
        "                                hyper_para[\"nb_heads\"] = nb_head\n",
        "                                hyper_para[\"alpha\"] = alpha\n",
        "                                hyper_para[\"loss\"] = test_loss\n",
        "                                hyper_para[\"accuracy\"] = test_acc\n",
        "                                hyper_para[\"auc\"] = auc\n",
        "                                hyper_para[\"false_alarm_rate\"] = far\n",
        "                                hyper_para[\"f1_score\"] = f1\n",
        "                                with open(sv_path +\"hyperpara.json\", \"a+\") as fp:\n",
        "                                    fp.write('\\n')\n",
        "                                    json.dump(hyper_para, fp)"
      ],
      "metadata": {
        "id": "vfM2RIzyG5JK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}