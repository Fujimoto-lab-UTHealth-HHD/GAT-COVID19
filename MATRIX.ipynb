{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "br7MnyBAfVce"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "import math\n",
        "\n",
        "class GraphAttentionLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
        "        super(GraphAttentionLayer, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.alpha = alpha\n",
        "        self.concat = concat\n",
        "\n",
        "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
        "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
        "        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))\n",
        "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
        "\n",
        "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
        "\n",
        "    def forward(self, h, adj):\n",
        "        Wh = torch.mm(h, self.W) # h.shape: (N, in_features), Wh.shape: (N, out_features)\n",
        "        e = self._prepare_attentional_mechanism_input(Wh)\n",
        "\n",
        "        zero_vec = -9e15*torch.ones_like(e)\n",
        "        attention = torch.where(adj > 0, e, zero_vec)\n",
        "        attention = F.softmax(attention, dim=1)\n",
        "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
        "        h_prime = torch.matmul(attention, Wh)\n",
        "\n",
        "        if self.concat:\n",
        "            return F.elu(h_prime)\n",
        "        else:\n",
        "            return h_prime\n",
        "\n",
        "    def _prepare_attentional_mechanism_input(self, Wh):\n",
        "        # Wh.shape (N, out_feature)\n",
        "        # self.a.shape (2 * out_feature, 1)\n",
        "        # Wh1&2.shape (N, 1)\n",
        "        # e.shape (N, N)\n",
        "        Wh1 = torch.matmul(Wh, self.a[:self.out_features, :])\n",
        "        Wh2 = torch.matmul(Wh, self.a[self.out_features:, :])\n",
        "        # broadcast add\n",
        "        e = Wh1 + Wh2.T\n",
        "        return self.leakyrelu(e)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
        "\n",
        "\n",
        "class GraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        if self.bias is not None:\n",
        "            self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, input, adj):\n",
        "        support = torch.mm(input, self.weight)\n",
        "        output = torch.spmm(adj, support)\n",
        "        if self.bias is not None:\n",
        "            return output + self.bias\n",
        "        else:\n",
        "            return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.sparse as sp\n",
        "import torch\n",
        "import os\n",
        "\n",
        "def normalize_features(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    mx = r_mat_inv.dot(mx)\n",
        "    return mx\n",
        "\n",
        "def normalize_adj(mx):\n",
        "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
        "    rowsum = np.array(mx.sum(1))\n",
        "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
        "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
        "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
        "\n",
        "def accuracy(output, labels):\n",
        "    probs = torch.exp(output)\n",
        "    preds = torch.argmax(probs, dim = 1)\n",
        "    correct = preds.eq(labels).double()\n",
        "    correct = correct.sum()\n",
        "    return correct / len(labels)\n",
        "\n",
        "def load_multi_data(folder, att_file, edge_list_name):\n",
        "    att = pd.read_csv(folder + att_file)\n",
        "    edge_list = []\n",
        "    for name in edge_list_name:\n",
        "        edge_list.append(pd.read_csv(folder + name))\n",
        "\n",
        "    #get y and x\n",
        "    labels = np.array(att[\"ever_pos\"])\n",
        "    # b,c,d are age groups\n",
        "    features = sp.csr_matrix(att[[\"b\",\"c\",\"d\", \"Gender\", \"BLACK_AFRICAN_AMERICAN\",\n",
        "                                  \"OTHER\", \"ASIAN\", \"pp_degree\",\"pp_cls_coef\",\n",
        "                                  \"pp_exposure_numerator\",\"hh_degree\"\n",
        "                                  ,\"education_center\" , \"assisted_living\" ,\n",
        "                                  \"healthcare\" ,   \"charities_homeless\" ,  \"detention_center\" ,\n",
        "                                  \"commercial_offices\" ,  \"government\",\"hh_exposure_numerator\" ]])    #features = normalize_features(features)\n",
        "\n",
        "    #get adj mat\n",
        "    adj_list = []\n",
        "    for edge in edge_list:\n",
        "        #get row col idx for adj matrix\n",
        "        row_idx = []\n",
        "        col_idx = []\n",
        "        for i in range(edge.shape[0]):\n",
        "            id_from = edge.iloc[i,0]\n",
        "            id_to = edge.iloc[i,1]\n",
        "            row_id = att.index[att[\"EventID\"] == id_from]\n",
        "            row_idx.append(row_id[0])\n",
        "            col_id = att.index[att[\"EventID\"] == id_to]\n",
        "            col_idx.append(col_id[0])\n",
        "\n",
        "        if edge.shape[1] == 2:\n",
        "            adj = sp.coo_matrix((np.ones(edge.shape[0]), (row_idx, col_idx)), shape=(att.shape[0], att.shape[0]), dtype=np.float32)\n",
        "\n",
        "        elif edge.shape[1] == 3:\n",
        "            adj = sp.coo_matrix((np.array(edge.iloc[:,2]), (row_idx, col_idx)), shape=(att.shape[0], att.shape[0]), dtype=np.float32)\n",
        "\n",
        "        #make adj symmetric\n",
        "        adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "        #normaliza adj\n",
        "        adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "        adj = torch.FloatTensor(np.array(adj.todense()))\n",
        "        adj_list.append(adj)\n",
        "\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    labels = torch.LongTensor(labels)\n",
        "\n",
        "    dim = len(labels)\n",
        "    idx_train = range(1585)\n",
        "    idx_test = range(1585, dim)\n",
        "\n",
        "    return adj_list, features, labels, idx_train, idx_test\n",
        "\n",
        "def load_data(folder, att_file, edge_name):\n",
        "    att = pd.read_csv(folder + att_file)\n",
        "    edge_list = pd.read_csv(folder + edge_name)\n",
        "\n",
        "    #get y and x\n",
        "    labels = np.array(att[\"y\"])\n",
        "    # b,c,d are age groups\n",
        "    features = sp.csr_matrix(att[[\"b\",\"c\",\"d\", \"Gender\", \"BLACK_AFRICAN_AMERICAN\",\n",
        "                                  \"OTHER\", \"ASIAN\", \"pp_degree\",\"pp_cls_coef\",\n",
        "                                  \"pp_exposure_numerator\",\"hh_degree\"\n",
        "                                  ,\"education_center\" , \"assisted_living\" ,\n",
        "                                  \"healthcare\" ,   \"charities_homeless\" ,  \"detention_center\" ,\n",
        "                                  \"commercial_offices\" ,  \"government\",\"hh_exposure_numerator\"  ]])\n",
        "    #features = normalize_features(features)\n",
        "\n",
        "    #get adj mat\n",
        "    row_idx = []\n",
        "    col_idx = []\n",
        "    for i in range(edge_list.shape[0]):\n",
        "        id_from = edge_list.iloc[i,0]\n",
        "        id_to = edge_list.iloc[i,1]\n",
        "        row_id = att.index[att[\"EventID\"] == id_from]\n",
        "        row_idx.append(row_id[0])\n",
        "        col_id = att.index[att[\"EventID\"] == id_to]\n",
        "        col_idx.append(col_id[0])\n",
        "\n",
        "    if edge_list.shape[1] == 2:\n",
        "        adj = sp.coo_matrix((np.ones(edge_list.shape[0]), (row_idx, col_idx)), shape=(att.shape[0], att.shape[0]), dtype=np.float32)\n",
        "\n",
        "\n",
        "        #make adj symmetric\n",
        "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "        #normaliza adj\n",
        "    adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
        "\n",
        "    features = torch.FloatTensor(np.array(features.todense()))\n",
        "    labels = torch.LongTensor(labels)\n",
        "\n",
        "    dim = len(labels)\n",
        "    idx_train = range(1585)\n",
        "    idx_test = range(1585, dim)\n",
        "\n",
        "    return adj, features, labels, idx_train, idx_test"
      ],
      "metadata": {
        "id": "-qpSIrx7f_73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "class GAT(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
        "        \"\"\"Dense version of GAT.\"\"\"\n",
        "        super(GAT, self).__init__()\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
        "        for i, attention in enumerate(self.attentions):\n",
        "            self.add_module('attention_{}'.format(i), attention)\n",
        "\n",
        "        self.out_att = nn.Linear(nhid * nheads, nclass)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = F.elu(self.out_att(x))\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "class FusionGAT(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, adj_list, nheads):\n",
        "        super(FusionGAT, self).__init__()\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.nheads = nheads\n",
        "        self.adj_list = adj_list\n",
        "\n",
        "        # Define list of GAT layers for each adjacency matrix\n",
        "        self.attentions = nn.ModuleList()\n",
        "        for i in range(len(adj_list)):\n",
        "            att_list = nn.ModuleList([GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)])\n",
        "            self.attentions.append(att_list)\n",
        "            for k, attention in enumerate(self.attentions):\n",
        "                self.add_module('adj{}, attention_{}'.format(i, k), attention)\n",
        "\n",
        "        # Define linear layer for integration with L1 regularization\n",
        "        self.integration_att = nn.Linear(nhid * nheads, nclass)\n",
        "        self.fusion = nn.Linear(nclass * len(adj_list), nclass)\n",
        "        self.l1_reg = nn.L1Loss(reduction='mean')\n",
        "\n",
        "    def forward(self, x, adj_list):\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        # Compute output for each adjacency matrix using GAT layers\n",
        "        output_list = []\n",
        "        for i, adj in enumerate(adj_list):\n",
        "            x_i = torch.cat([att(x, adj) for att in self.attentions[i]], dim=1)\n",
        "            x_i = F.dropout(x_i, self.dropout, training=self.training)\n",
        "            x_i = F.elu(self.integration_att(x_i))\n",
        "            output_list.append(x_i)\n",
        "        output = torch.cat(output_list, dim=1)\n",
        "\n",
        "        # Apply linear layer for integration with L1 regularization\n",
        "        output = F.dropout(output, self.dropout, training=self.training)\n",
        "        output = self.fusion(output.view(output.size(0), -1))\n",
        "        l1_loss = self.l1_reg(self.fusion.weight, torch.zeros_like(self.fusion.weight))\n",
        "        return F.log_softmax(output, dim=1), l1_loss\n",
        "\n",
        "\n",
        "\n",
        "class FusionGAT2(nn.Module):\n",
        "    def __init__(self, nfeat, nhid1, nhid2, nclass, dropout, alpha, adj_list, nheads):\n",
        "        super(FusionGAT2, self).__init__()\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.nheads = nheads\n",
        "        self.adj_list = adj_list\n",
        "\n",
        "        # Define list of GAT layers for each adjacency matrix in attention 1\n",
        "        self.attentions1 = nn.ModuleList()\n",
        "        for i in range(len(adj_list)):\n",
        "            att_list = nn.ModuleList([GraphAttentionLayer(nfeat, nhid1, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)])\n",
        "            self.attentions1.append(att_list)\n",
        "            for k, attention in enumerate(self.attentions1):\n",
        "                self.add_module('adj{}, attention_layer1_{}'.format(i, k), attention)\n",
        "\n",
        "        # Define linear layer for integration of multihead attention1\n",
        "        self.integration_att1 = nn.Linear(nhid1 * nheads, nhid1)\n",
        "\n",
        "        self.attentions2 = nn.ModuleList()\n",
        "        for i in range(len(adj_list)):\n",
        "            att_list2 = nn.ModuleList([GraphAttentionLayer(nhid1, nhid2, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)])\n",
        "            self.attentions2.append(att_list2)\n",
        "            for k, attention in enumerate(self.attentions2):\n",
        "                self.add_module('adj{}, attention_layer2_{}'.format(i, k), attention)\n",
        "\n",
        "        # Define linear layer for integration of multihead attention2\n",
        "        self.integration_att2 = nn.Linear(nhid2 * nheads, nclass)\n",
        "\n",
        "        #fusion layer with l1 penalty\n",
        "        self.fusion_att = nn.Linear(nclass * len(adj_list), nclass)\n",
        "        self.l1_reg = nn.L1Loss(reduction='mean')\n",
        "\n",
        "    def forward(self, x, adj_list):\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        # Compute output for each adjacency matrix using GAT layers\n",
        "        output_list = []\n",
        "        for i, adj in enumerate(adj_list):\n",
        "            #attention layer 1\n",
        "            x_i = torch.cat([att(x, adj) for att in self.attentions1[i]], dim=1)\n",
        "            x_i = F.dropout(x_i, self.dropout, training=self.training)\n",
        "            x_i = F.elu(self.integration_att1(x_i))\n",
        "\n",
        "            #attention layer 2\n",
        "            x_i = torch.cat([att(x_i, adj) for att in self.attentions2[i]], dim=1)\n",
        "            x_i = F.dropout(x_i, self.dropout, training=self.training)\n",
        "            x_i = F.elu(self.integration_att2(x_i))\n",
        "            output_list.append(x_i)\n",
        "\n",
        "        output = torch.cat(output_list, dim=1)\n",
        "        output = F.dropout(output, self.dropout, training=self.training)\n",
        "        output = self.fusion_att(output.view(output.size(0), -1))\n",
        "        l1_loss = self.l1_reg(self.fusion_att.weight, torch.zeros_like(self.fusion_att.weight))\n",
        "        return F.log_softmax(output, dim=1), l1_loss\n",
        "\n",
        "\n",
        "\n",
        "class FusionGAT3(nn.Module):\n",
        "    def __init__(self, nfeat, nhid1, nhid2, fusion1_dim, nclass, dropout, alpha, adj_list, nheads):\n",
        "        super(FusionGAT3, self).__init__()\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.nheads = nheads\n",
        "        self.adj_list = adj_list\n",
        "\n",
        "        # Define list of GAT layers for each adjacency matrix\n",
        "        #att 1\n",
        "        self.attentions1 = nn.ModuleList()\n",
        "        for i in range(len(adj_list)):\n",
        "            att_list = nn.ModuleList([GraphAttentionLayer(nfeat, nhid1, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)])\n",
        "            self.attentions1.append(att_list)\n",
        "            for k, attention in enumerate(self.attentions1):\n",
        "                self.add_module('adj{}, attention_layer1_{}'.format(i, k), attention)\n",
        "\n",
        "        # fusion1\n",
        "        self.integration_att1 = nn.Linear(nhid1 * nheads, fusion1_dim)\n",
        "        self.fusion_att1 = nn.Linear(fusion1_dim * len(adj_list), fusion1_dim)\n",
        "        self.l1_reg1 = nn.L1Loss(reduction='mean')\n",
        "\n",
        "        #att 2\n",
        "        self.attentions2 = nn.ModuleList()\n",
        "        for i in range(len(adj_list)):\n",
        "            att_list = nn.ModuleList([GraphAttentionLayer(fusion1_dim, nhid2, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)])\n",
        "            self.attentions2.append(att_list)\n",
        "            for k, attention in enumerate(self.attentions2):\n",
        "                self.add_module('adj{}, attention_layer2_{}'.format(i, k), attention)\n",
        "\n",
        "        #fusion2\n",
        "        self.integration_att2 = nn.Linear(nhid2 * nheads, nclass)\n",
        "        self.fusion_att2 = nn.Linear(nclass * len(adj_list), nclass)\n",
        "        self.l1_reg2 = nn.L1Loss(reduction='mean')\n",
        "\n",
        "\n",
        "    def forward(self, x, adj_list):\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "\n",
        "        # Compute output for each adjacency matrix using GAT layers\n",
        "        output_list = []\n",
        "        for i, adj in enumerate(adj_list):\n",
        "            x_i = x\n",
        "            x_i = torch.cat([att(x, adj) for att in self.attentions1[i]], dim=1)\n",
        "            x_i = F.dropout(x_i, self.dropout, training=self.training)\n",
        "            x_i = F.elu(self.integration_att1(x_i))\n",
        "            output_list.append(x_i)\n",
        "        output = torch.cat(output_list, dim=1)\n",
        "\n",
        "        # Apply linear layer for integration with L1 regularization\n",
        "        output = F.dropout(output, self.dropout, training=self.training)\n",
        "        output = self.fusion_att1(output.view(output.size(0), -1))\n",
        "        l1_loss1 = self.l1_reg1(self.fusion_att1.weight, torch.zeros_like(self.fusion_att1.weight))\n",
        "\n",
        "\n",
        "        output_list2 = []\n",
        "        for i, adj in enumerate(adj_list):\n",
        "            x_i = torch.cat([att(output, adj) for att in self.attentions2[i]], dim=1)\n",
        "            x_i = F.dropout(x_i, self.dropout, training=self.training)\n",
        "            x_i = F.elu(self.integration_att2(x_i))\n",
        "            output_list2.append(x_i)\n",
        "        output2 = torch.cat(output_list2, dim=1)\n",
        "\n",
        "        # Apply linear layer for integration with L1 regularization\n",
        "        output2 = F.dropout(output2, self.dropout, training=self.training)\n",
        "        output2 = self.fusion_att2(output2.view(output.size(0), -1))\n",
        "        l1_loss2 = self.l1_reg2(self.fusion_att2.weight, torch.zeros_like(self.fusion_att2.weight))\n",
        "\n",
        "        return F.log_softmax(output2, dim=1), l1_loss1 + l1_loss2\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        x = F.relu(self.gc1(x, adj))\n",
        "        x = F.dropout(x, self.dropout, training=self.training)\n",
        "        x = self.gc2(x, adj)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "q5auJI3dgKKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from sklearn.metrics import roc_auc_score, f1_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import time\n",
        "import json\n",
        "from sklearn import metrics\n",
        "#from load import accuracy, load_multi_data\n",
        "#from models import FusionGAT3\n",
        "\n",
        "\n",
        "seed = 72\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "def train(epoch):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output, l1_loss = model(features, adj_list_tt)\n",
        "    loss_train = F.nll_loss(output[idx_train], labels[idx_train]) + l1_loss*lambda_l1\n",
        "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test]) + l1_loss*lambda_l1\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    y_prob = torch.exp(output[:, 1]).cpu().detach().numpy()\n",
        "\n",
        "    auc_score = roc_auc_score(labels[idx_test].cpu().detach().numpy(), y_prob[idx_test.cpu().detach().numpy()])\n",
        "    print('Epoch: {:03d}'.format(epoch+1),\n",
        "          'loss_train: {:.3f}'.format(loss_train.data.item()),\n",
        "          'acc_train: {:.3f}'.format(acc_train.data.item()),\n",
        "          'loss_test: {:.3f}'.format(loss_test.data.item()),\n",
        "          'acc_test: {:.3f}'.format(acc_test.data.item()),\n",
        "          \"AUC = {:.2f}\".format(auc_score),\n",
        "          'time: {:.3f}s'.format(time.time() - t))\n",
        "\n",
        "    return loss_test.data.item(), acc_test, auc_score\n",
        "\n",
        "def compute_test(model):\n",
        "    model.eval()\n",
        "    output, l1_loss = model(features, adj_list_tt)\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test]) + l1_loss*lambda_l1\n",
        "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "    y_prob = torch.exp(output[:, 1]).cpu().detach().numpy()\n",
        "\n",
        "    auc_score = roc_auc_score(labels[idx_test].cpu().detach().numpy(), y_prob[idx_test.cpu().detach().numpy()])\n",
        "    # compute false alarm rate and f1 score\n",
        "    y_pred = np.array([0 if p < 0.5 else 1 for p in y_prob])\n",
        "    false_alarm_rate = np.mean(y_pred[idx_test.cpu().detach().numpy()] == 1)\n",
        "    f1score = f1_score(labels[idx_test].cpu().detach().numpy(), y_pred[idx_test.cpu().detach().numpy()])\n",
        "    precisionn, recalll, _ = precision_recall_curve(labels[idx_test].cpu().detach().numpy(), y_prob[idx_test.cpu().detach().numpy()])\n",
        "\n",
        "    pr_auc = metrics.auc(recalll, precisionn)\n",
        "    print(\"Test set results:\",\n",
        "          \"loss= {:.4f}\".format(loss_test.data.item()),\n",
        "          \"accuracy= {:.4f}\".format(acc_test.data.item()),\n",
        "          \"AUC = {:.2f}\".format(auc_score),\n",
        "          \"False Alarm Rate = {:.4f}\".format(false_alarm_rate),\n",
        "          \"F1 Score = {:.4f}\".format(f1score))\n",
        "    return loss_test.data.item(), acc_test.data.item(), auc_score, false_alarm_rate, f1score, y_pred, pr_auc\n",
        "\n",
        "\n",
        "#parameter setting\n",
        "#\n",
        "\n",
        "cuda = True\n",
        "epochs = 500\n",
        "\n",
        "\n",
        "lrs = [0.0005]\n",
        "weight_decays = [0.00005]\n",
        "dropouts = [0.3]\n",
        "hidden_dims1 = [16]\n",
        "hidden_dims2 = [16]\n",
        "fusion1_dims = [4]\n",
        "nb_heads = [8]\n",
        "alpha = 0.2 #Alpha for the leaky_relu\n",
        "lambda_l1s = [0.0001, 0.001, 0.01]\n",
        "patience = 50\n",
        "\n",
        "\n",
        "# Load data\n",
        "sv_path = \"\"\n",
        "folder = \"../data/\"\n",
        "att_name = \"att_pp_main_hh_exp.csv\"\n",
        "edge_list_name = [\"edge_pp_main.csv\", \"edge_hh_main.csv\",\"edge_pv_main_uniq.csv\", \"edge_pv_main_1.csv\", \"edge_pv_main_2.csv\", \"edge_pv_main_3.csv\"\n",
        ", \"edge_pv_main_4.csv\", \"edge_pv_main_5.csv\", \"edge_pv_main_6.csv\", \"edge_pv_main_7.csv\"]\n",
        "\n",
        "adj_list, features, labels, idx_train, idx_test = load_multi_data(folder, att_name, edge_list_name)\n",
        "\n",
        "adj_list_tensor = torch.stack(adj_list)\n",
        "features, adj_list_tt, labels = Variable(features), adj_list_tensor, Variable(labels)\n",
        "\n",
        "count = 0\n",
        "for lr in lrs:\n",
        "    for dropout in dropouts:\n",
        "        for hid1 in hidden_dims1:\n",
        "            for hid2 in hidden_dims2:\n",
        "                for fusion1_dim in fusion1_dims:\n",
        "                    for nb_head in nb_heads:\n",
        "                        for weight_decay in weight_decays:\n",
        "                            for lambda_l1 in lambda_l1s:\n",
        "\n",
        "                                # Model and optimizer\n",
        "                                model = FusionGAT3(nfeat=features.shape[1],\n",
        "                                            nhid1=hid1,\n",
        "                                            nhid2=hid2,\n",
        "                                            fusion1_dim = fusion1_dim,\n",
        "                                            nclass=int(labels.max()) + 1,\n",
        "                                            dropout=dropout,\n",
        "                                            alpha=alpha,\n",
        "                                            adj_list= adj_list_tt,\n",
        "                                            nheads = nb_head)\n",
        "\n",
        "                                optimizer = optim.Adam(model.parameters(),\n",
        "                                                    lr=lr,\n",
        "                                                    weight_decay=weight_decay)\n",
        "\n",
        "                                if cuda:\n",
        "                                    model.cuda()\n",
        "                                    features = features.cuda()\n",
        "                                    adj_list_tt = adj_list_tt.cuda()\n",
        "                                    labels = labels.cuda()\n",
        "                                    idx_train = torch.tensor(idx_train).cuda()\n",
        "                                    idx_test = torch.tensor(idx_test).cuda()\n",
        "\n",
        "\n",
        "                                # Train model\n",
        "                                t_total = time.time()\n",
        "                                bad_counter = 0\n",
        "                                best_auc = -1\n",
        "                                best_epoch = 0\n",
        "                                for epoch in range(epochs):\n",
        "                                    loss, acc, auc = train(epoch)\n",
        "\n",
        "                                    torch.save(model.state_dict(), sv_path + '{}.pkl'.format(epoch))\n",
        "                                    if auc > best_auc:\n",
        "                                        best_auc = auc\n",
        "                                        best_epoch = epoch\n",
        "                                        bad_counter = 0\n",
        "                                    else:\n",
        "                                        bad_counter += 1\n",
        "\n",
        "                                    if bad_counter == patience:\n",
        "                                        break\n",
        "\n",
        "                                files = glob.glob(sv_path +'*.pkl')\n",
        "                                for file in files:\n",
        "                                    filename = file.split('/')[-1]\n",
        "                                    epoch_nb = int(filename.split('.')[0])\n",
        "                                    if epoch_nb != best_epoch:\n",
        "                                        os.remove(file)\n",
        "\n",
        "                                print(\"Optimization Finished!\")\n",
        "                                print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "                                # Restore best model\n",
        "                                print('Loading {}th epoch'.format(best_epoch))\n",
        "                                model.load_state_dict(torch.load(sv_path +'{}.pkl'.format(best_epoch)))\n",
        "\n",
        "                                # Testing\n",
        "                                test_loss, test_acc, auc, far, f1, y_pred, pr_auc= compute_test(model)\n",
        "                                hyper_para = {}\n",
        "                                hyper_para[\"lr\"] = lr\n",
        "                                hyper_para[\"weight_decay\"] = weight_decay\n",
        "                                hyper_para[\"dropout\"] = dropout\n",
        "                                hyper_para[\"hidden_dim1\"] = hid1\n",
        "                                hyper_para[\"hidden_dim2\"] = hid2\n",
        "                                hyper_para[\"lambda\"] = lambda_l1\n",
        "                                hyper_para[\"fusion1_dim\"] = fusion1_dim\n",
        "                                hyper_para[\"nb_heads\"] = nb_head\n",
        "                                hyper_para[\"alpha\"] = alpha\n",
        "                                hyper_para[\"loss\"] = test_loss\n",
        "                                hyper_para[\"accuracy\"] = test_acc\n",
        "                                hyper_para[\"auc\"] = auc\n",
        "                                hyper_para[\"pr_auc\"] = pr_auc\n",
        "                                hyper_para[\"false_alarm_rate\"] = far\n",
        "                                hyper_para[\"f1_score\"] = f1\n",
        "                                with open(sv_path +\"hyperpara.json\", \"a+\") as fp:\n",
        "                                    fp.write('\\n')\n",
        "                                    json.dump(hyper_para, fp)"
      ],
      "metadata": {
        "id": "mmct_H8lgNms",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4c22841-a4ab-43da-afff-a6a125f087bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001 loss_train: 0.661 acc_train: 0.650 loss_test: 0.717 acc_test: 0.526 AUC = 0.49 time: 2.325s\n",
            "Epoch: 002 loss_train: 0.642 acc_train: 0.673 loss_test: 0.719 acc_test: 0.492 AUC = 0.46 time: 0.494s\n",
            "Epoch: 003 loss_train: 0.648 acc_train: 0.676 loss_test: 0.707 acc_test: 0.518 AUC = 0.49 time: 0.494s\n",
            "Epoch: 004 loss_train: 0.640 acc_train: 0.689 loss_test: 0.712 acc_test: 0.539 AUC = 0.50 time: 0.494s\n",
            "Epoch: 005 loss_train: 0.643 acc_train: 0.684 loss_test: 0.719 acc_test: 0.527 AUC = 0.48 time: 0.494s\n",
            "Epoch: 006 loss_train: 0.641 acc_train: 0.683 loss_test: 0.717 acc_test: 0.518 AUC = 0.52 time: 0.494s\n",
            "Epoch: 007 loss_train: 0.631 acc_train: 0.691 loss_test: 0.724 acc_test: 0.536 AUC = 0.52 time: 0.494s\n",
            "Epoch: 008 loss_train: 0.639 acc_train: 0.684 loss_test: 0.718 acc_test: 0.530 AUC = 0.51 time: 0.494s\n",
            "Epoch: 009 loss_train: 0.628 acc_train: 0.692 loss_test: 0.717 acc_test: 0.545 AUC = 0.50 time: 0.495s\n",
            "Epoch: 010 loss_train: 0.624 acc_train: 0.692 loss_test: 0.716 acc_test: 0.538 AUC = 0.51 time: 0.495s\n",
            "Epoch: 011 loss_train: 0.624 acc_train: 0.693 loss_test: 0.714 acc_test: 0.541 AUC = 0.55 time: 0.495s\n",
            "Epoch: 012 loss_train: 0.619 acc_train: 0.699 loss_test: 0.711 acc_test: 0.538 AUC = 0.54 time: 0.495s\n",
            "Epoch: 013 loss_train: 0.624 acc_train: 0.690 loss_test: 0.716 acc_test: 0.549 AUC = 0.55 time: 0.494s\n",
            "Epoch: 014 loss_train: 0.627 acc_train: 0.686 loss_test: 0.710 acc_test: 0.546 AUC = 0.56 time: 0.494s\n",
            "Epoch: 015 loss_train: 0.609 acc_train: 0.701 loss_test: 0.721 acc_test: 0.538 AUC = 0.53 time: 0.495s\n",
            "Epoch: 016 loss_train: 0.613 acc_train: 0.685 loss_test: 0.724 acc_test: 0.552 AUC = 0.54 time: 0.494s\n",
            "Epoch: 017 loss_train: 0.616 acc_train: 0.695 loss_test: 0.712 acc_test: 0.545 AUC = 0.58 time: 0.494s\n",
            "Epoch: 018 loss_train: 0.606 acc_train: 0.700 loss_test: 0.735 acc_test: 0.545 AUC = 0.53 time: 0.495s\n",
            "Epoch: 019 loss_train: 0.604 acc_train: 0.691 loss_test: 0.729 acc_test: 0.541 AUC = 0.55 time: 0.495s\n",
            "Epoch: 020 loss_train: 0.605 acc_train: 0.700 loss_test: 0.715 acc_test: 0.564 AUC = 0.58 time: 0.497s\n",
            "Epoch: 021 loss_train: 0.604 acc_train: 0.701 loss_test: 0.706 acc_test: 0.546 AUC = 0.61 time: 0.495s\n",
            "Epoch: 022 loss_train: 0.606 acc_train: 0.703 loss_test: 0.721 acc_test: 0.545 AUC = 0.57 time: 0.494s\n",
            "Epoch: 023 loss_train: 0.602 acc_train: 0.693 loss_test: 0.722 acc_test: 0.554 AUC = 0.59 time: 0.495s\n",
            "Epoch: 024 loss_train: 0.596 acc_train: 0.701 loss_test: 0.721 acc_test: 0.552 AUC = 0.59 time: 0.495s\n",
            "Epoch: 025 loss_train: 0.593 acc_train: 0.703 loss_test: 0.717 acc_test: 0.548 AUC = 0.60 time: 0.495s\n",
            "Epoch: 026 loss_train: 0.591 acc_train: 0.698 loss_test: 0.703 acc_test: 0.574 AUC = 0.62 time: 0.495s\n",
            "Epoch: 027 loss_train: 0.583 acc_train: 0.719 loss_test: 0.722 acc_test: 0.580 AUC = 0.61 time: 0.495s\n",
            "Epoch: 028 loss_train: 0.588 acc_train: 0.705 loss_test: 0.704 acc_test: 0.577 AUC = 0.62 time: 0.495s\n",
            "Epoch: 029 loss_train: 0.585 acc_train: 0.717 loss_test: 0.713 acc_test: 0.579 AUC = 0.60 time: 0.495s\n",
            "Epoch: 030 loss_train: 0.586 acc_train: 0.717 loss_test: 0.701 acc_test: 0.585 AUC = 0.63 time: 0.495s\n",
            "Epoch: 031 loss_train: 0.580 acc_train: 0.717 loss_test: 0.698 acc_test: 0.586 AUC = 0.63 time: 0.532s\n",
            "Epoch: 032 loss_train: 0.575 acc_train: 0.721 loss_test: 0.717 acc_test: 0.588 AUC = 0.60 time: 0.495s\n",
            "Epoch: 033 loss_train: 0.576 acc_train: 0.732 loss_test: 0.701 acc_test: 0.580 AUC = 0.63 time: 0.495s\n",
            "Epoch: 034 loss_train: 0.569 acc_train: 0.732 loss_test: 0.703 acc_test: 0.591 AUC = 0.63 time: 0.495s\n",
            "Epoch: 035 loss_train: 0.564 acc_train: 0.726 loss_test: 0.698 acc_test: 0.596 AUC = 0.63 time: 0.495s\n",
            "Epoch: 036 loss_train: 0.577 acc_train: 0.730 loss_test: 0.698 acc_test: 0.598 AUC = 0.66 time: 0.495s\n",
            "Epoch: 037 loss_train: 0.573 acc_train: 0.728 loss_test: 0.706 acc_test: 0.599 AUC = 0.63 time: 0.495s\n",
            "Epoch: 038 loss_train: 0.579 acc_train: 0.727 loss_test: 0.689 acc_test: 0.610 AUC = 0.67 time: 0.496s\n",
            "Epoch: 039 loss_train: 0.561 acc_train: 0.741 loss_test: 0.712 acc_test: 0.610 AUC = 0.63 time: 0.496s\n",
            "Epoch: 040 loss_train: 0.570 acc_train: 0.736 loss_test: 0.702 acc_test: 0.616 AUC = 0.64 time: 0.496s\n",
            "Epoch: 041 loss_train: 0.582 acc_train: 0.725 loss_test: 0.696 acc_test: 0.633 AUC = 0.63 time: 0.496s\n",
            "Epoch: 042 loss_train: 0.561 acc_train: 0.751 loss_test: 0.692 acc_test: 0.611 AUC = 0.65 time: 0.495s\n",
            "Epoch: 043 loss_train: 0.553 acc_train: 0.751 loss_test: 0.714 acc_test: 0.627 AUC = 0.65 time: 0.495s\n",
            "Epoch: 044 loss_train: 0.555 acc_train: 0.742 loss_test: 0.700 acc_test: 0.622 AUC = 0.65 time: 0.496s\n",
            "Epoch: 045 loss_train: 0.566 acc_train: 0.749 loss_test: 0.685 acc_test: 0.632 AUC = 0.66 time: 0.496s\n",
            "Epoch: 046 loss_train: 0.553 acc_train: 0.753 loss_test: 0.689 acc_test: 0.641 AUC = 0.65 time: 0.495s\n",
            "Epoch: 047 loss_train: 0.547 acc_train: 0.746 loss_test: 0.704 acc_test: 0.616 AUC = 0.66 time: 0.496s\n",
            "Epoch: 048 loss_train: 0.553 acc_train: 0.748 loss_test: 0.674 acc_test: 0.623 AUC = 0.67 time: 0.495s\n",
            "Epoch: 049 loss_train: 0.544 acc_train: 0.746 loss_test: 0.674 acc_test: 0.632 AUC = 0.68 time: 0.496s\n",
            "Epoch: 050 loss_train: 0.553 acc_train: 0.750 loss_test: 0.716 acc_test: 0.622 AUC = 0.65 time: 0.496s\n",
            "Epoch: 051 loss_train: 0.548 acc_train: 0.763 loss_test: 0.662 acc_test: 0.642 AUC = 0.69 time: 0.535s\n",
            "Epoch: 052 loss_train: 0.541 acc_train: 0.746 loss_test: 0.684 acc_test: 0.647 AUC = 0.67 time: 0.498s\n",
            "Epoch: 053 loss_train: 0.546 acc_train: 0.748 loss_test: 0.680 acc_test: 0.638 AUC = 0.67 time: 0.496s\n",
            "Epoch: 054 loss_train: 0.556 acc_train: 0.748 loss_test: 0.683 acc_test: 0.648 AUC = 0.68 time: 0.496s\n",
            "Epoch: 055 loss_train: 0.538 acc_train: 0.763 loss_test: 0.654 acc_test: 0.673 AUC = 0.70 time: 0.496s\n",
            "Epoch: 056 loss_train: 0.538 acc_train: 0.757 loss_test: 0.686 acc_test: 0.632 AUC = 0.68 time: 0.496s\n",
            "Epoch: 057 loss_train: 0.533 acc_train: 0.757 loss_test: 0.656 acc_test: 0.655 AUC = 0.69 time: 0.497s\n",
            "Epoch: 058 loss_train: 0.550 acc_train: 0.739 loss_test: 0.667 acc_test: 0.673 AUC = 0.68 time: 0.497s\n",
            "Epoch: 059 loss_train: 0.539 acc_train: 0.756 loss_test: 0.671 acc_test: 0.657 AUC = 0.69 time: 0.497s\n",
            "Epoch: 060 loss_train: 0.543 acc_train: 0.746 loss_test: 0.666 acc_test: 0.638 AUC = 0.70 time: 0.496s\n",
            "Epoch: 061 loss_train: 0.539 acc_train: 0.760 loss_test: 0.663 acc_test: 0.649 AUC = 0.70 time: 0.496s\n",
            "Epoch: 062 loss_train: 0.527 acc_train: 0.765 loss_test: 0.655 acc_test: 0.647 AUC = 0.71 time: 0.496s\n",
            "Epoch: 063 loss_train: 0.533 acc_train: 0.762 loss_test: 0.642 acc_test: 0.669 AUC = 0.72 time: 0.496s\n",
            "Epoch: 064 loss_train: 0.529 acc_train: 0.758 loss_test: 0.640 acc_test: 0.664 AUC = 0.72 time: 0.496s\n",
            "Epoch: 065 loss_train: 0.534 acc_train: 0.762 loss_test: 0.650 acc_test: 0.658 AUC = 0.70 time: 0.497s\n",
            "Epoch: 066 loss_train: 0.526 acc_train: 0.758 loss_test: 0.663 acc_test: 0.654 AUC = 0.70 time: 0.496s\n",
            "Epoch: 067 loss_train: 0.529 acc_train: 0.760 loss_test: 0.679 acc_test: 0.642 AUC = 0.67 time: 0.498s\n",
            "Epoch: 068 loss_train: 0.524 acc_train: 0.763 loss_test: 0.647 acc_test: 0.645 AUC = 0.71 time: 0.497s\n",
            "Epoch: 069 loss_train: 0.521 acc_train: 0.768 loss_test: 0.677 acc_test: 0.657 AUC = 0.67 time: 0.497s\n",
            "Epoch: 070 loss_train: 0.536 acc_train: 0.746 loss_test: 0.664 acc_test: 0.644 AUC = 0.69 time: 0.497s\n",
            "Epoch: 071 loss_train: 0.527 acc_train: 0.763 loss_test: 0.656 acc_test: 0.655 AUC = 0.70 time: 0.497s\n",
            "Epoch: 072 loss_train: 0.533 acc_train: 0.753 loss_test: 0.672 acc_test: 0.660 AUC = 0.68 time: 0.535s\n",
            "Epoch: 073 loss_train: 0.527 acc_train: 0.773 loss_test: 0.681 acc_test: 0.645 AUC = 0.70 time: 0.497s\n",
            "Epoch: 074 loss_train: 0.531 acc_train: 0.755 loss_test: 0.665 acc_test: 0.638 AUC = 0.69 time: 0.497s\n",
            "Epoch: 075 loss_train: 0.522 acc_train: 0.763 loss_test: 0.694 acc_test: 0.663 AUC = 0.68 time: 0.497s\n",
            "Epoch: 076 loss_train: 0.529 acc_train: 0.756 loss_test: 0.632 acc_test: 0.669 AUC = 0.72 time: 0.497s\n",
            "Epoch: 077 loss_train: 0.536 acc_train: 0.750 loss_test: 0.643 acc_test: 0.649 AUC = 0.72 time: 0.497s\n",
            "Epoch: 078 loss_train: 0.520 acc_train: 0.765 loss_test: 0.626 acc_test: 0.669 AUC = 0.73 time: 0.497s\n",
            "Epoch: 079 loss_train: 0.529 acc_train: 0.765 loss_test: 0.643 acc_test: 0.660 AUC = 0.73 time: 0.497s\n",
            "Epoch: 080 loss_train: 0.512 acc_train: 0.768 loss_test: 0.640 acc_test: 0.652 AUC = 0.73 time: 0.497s\n",
            "Epoch: 081 loss_train: 0.518 acc_train: 0.770 loss_test: 0.642 acc_test: 0.670 AUC = 0.71 time: 0.497s\n",
            "Epoch: 082 loss_train: 0.513 acc_train: 0.772 loss_test: 0.627 acc_test: 0.682 AUC = 0.74 time: 0.497s\n",
            "Epoch: 083 loss_train: 0.519 acc_train: 0.767 loss_test: 0.685 acc_test: 0.677 AUC = 0.71 time: 0.500s\n",
            "Epoch: 084 loss_train: 0.503 acc_train: 0.775 loss_test: 0.652 acc_test: 0.661 AUC = 0.70 time: 0.497s\n",
            "Epoch: 085 loss_train: 0.511 acc_train: 0.765 loss_test: 0.646 acc_test: 0.669 AUC = 0.71 time: 0.497s\n",
            "Epoch: 086 loss_train: 0.509 acc_train: 0.765 loss_test: 0.612 acc_test: 0.694 AUC = 0.74 time: 0.497s\n",
            "Epoch: 087 loss_train: 0.518 acc_train: 0.769 loss_test: 0.654 acc_test: 0.673 AUC = 0.71 time: 0.497s\n",
            "Epoch: 088 loss_train: 0.508 acc_train: 0.763 loss_test: 0.660 acc_test: 0.647 AUC = 0.70 time: 0.497s\n",
            "Epoch: 089 loss_train: 0.506 acc_train: 0.775 loss_test: 0.628 acc_test: 0.676 AUC = 0.73 time: 0.497s\n",
            "Epoch: 090 loss_train: 0.516 acc_train: 0.760 loss_test: 0.648 acc_test: 0.658 AUC = 0.72 time: 0.497s\n",
            "Epoch: 091 loss_train: 0.510 acc_train: 0.770 loss_test: 0.629 acc_test: 0.680 AUC = 0.73 time: 0.498s\n",
            "Epoch: 092 loss_train: 0.515 acc_train: 0.768 loss_test: 0.622 acc_test: 0.667 AUC = 0.75 time: 0.500s\n",
            "Epoch: 093 loss_train: 0.514 acc_train: 0.760 loss_test: 0.633 acc_test: 0.670 AUC = 0.73 time: 0.498s\n",
            "Epoch: 094 loss_train: 0.511 acc_train: 0.774 loss_test: 0.630 acc_test: 0.688 AUC = 0.73 time: 0.498s\n",
            "Epoch: 095 loss_train: 0.512 acc_train: 0.766 loss_test: 0.613 acc_test: 0.677 AUC = 0.75 time: 0.498s\n",
            "Epoch: 096 loss_train: 0.506 acc_train: 0.763 loss_test: 0.623 acc_test: 0.672 AUC = 0.73 time: 0.498s\n",
            "Epoch: 097 loss_train: 0.510 acc_train: 0.774 loss_test: 0.647 acc_test: 0.648 AUC = 0.72 time: 0.498s\n",
            "Epoch: 098 loss_train: 0.512 acc_train: 0.758 loss_test: 0.624 acc_test: 0.683 AUC = 0.74 time: 0.499s\n",
            "Epoch: 099 loss_train: 0.495 acc_train: 0.774 loss_test: 0.643 acc_test: 0.672 AUC = 0.73 time: 0.498s\n",
            "Epoch: 100 loss_train: 0.499 acc_train: 0.770 loss_test: 0.648 acc_test: 0.649 AUC = 0.73 time: 0.498s\n",
            "Epoch: 101 loss_train: 0.509 acc_train: 0.770 loss_test: 0.646 acc_test: 0.685 AUC = 0.74 time: 0.498s\n",
            "Epoch: 102 loss_train: 0.506 acc_train: 0.771 loss_test: 0.635 acc_test: 0.682 AUC = 0.74 time: 0.498s\n",
            "Epoch: 103 loss_train: 0.492 acc_train: 0.780 loss_test: 0.601 acc_test: 0.686 AUC = 0.77 time: 0.498s\n",
            "Epoch: 104 loss_train: 0.501 acc_train: 0.771 loss_test: 0.610 acc_test: 0.682 AUC = 0.76 time: 0.498s\n",
            "Epoch: 105 loss_train: 0.507 acc_train: 0.770 loss_test: 0.622 acc_test: 0.677 AUC = 0.75 time: 0.498s\n",
            "Epoch: 106 loss_train: 0.506 acc_train: 0.766 loss_test: 0.603 acc_test: 0.711 AUC = 0.76 time: 0.498s\n",
            "Epoch: 107 loss_train: 0.502 acc_train: 0.781 loss_test: 0.607 acc_test: 0.672 AUC = 0.76 time: 0.498s\n",
            "Epoch: 108 loss_train: 0.512 acc_train: 0.768 loss_test: 0.601 acc_test: 0.686 AUC = 0.76 time: 0.498s\n",
            "Epoch: 109 loss_train: 0.502 acc_train: 0.778 loss_test: 0.609 acc_test: 0.667 AUC = 0.76 time: 0.498s\n",
            "Epoch: 110 loss_train: 0.509 acc_train: 0.770 loss_test: 0.597 acc_test: 0.689 AUC = 0.76 time: 0.498s\n",
            "Epoch: 111 loss_train: 0.513 acc_train: 0.776 loss_test: 0.618 acc_test: 0.680 AUC = 0.75 time: 0.497s\n",
            "Epoch: 112 loss_train: 0.510 acc_train: 0.770 loss_test: 0.618 acc_test: 0.689 AUC = 0.75 time: 0.498s\n",
            "Epoch: 113 loss_train: 0.502 acc_train: 0.766 loss_test: 0.615 acc_test: 0.689 AUC = 0.74 time: 0.497s\n",
            "Epoch: 114 loss_train: 0.494 acc_train: 0.781 loss_test: 0.616 acc_test: 0.689 AUC = 0.75 time: 0.498s\n",
            "Epoch: 115 loss_train: 0.516 acc_train: 0.770 loss_test: 0.632 acc_test: 0.663 AUC = 0.74 time: 0.497s\n",
            "Epoch: 116 loss_train: 0.507 acc_train: 0.767 loss_test: 0.620 acc_test: 0.691 AUC = 0.75 time: 0.497s\n",
            "Epoch: 117 loss_train: 0.493 acc_train: 0.777 loss_test: 0.617 acc_test: 0.691 AUC = 0.74 time: 0.497s\n",
            "Epoch: 118 loss_train: 0.503 acc_train: 0.779 loss_test: 0.654 acc_test: 0.675 AUC = 0.72 time: 0.497s\n",
            "Epoch: 119 loss_train: 0.497 acc_train: 0.779 loss_test: 0.602 acc_test: 0.689 AUC = 0.78 time: 0.497s\n",
            "Epoch: 120 loss_train: 0.490 acc_train: 0.779 loss_test: 0.616 acc_test: 0.672 AUC = 0.75 time: 0.497s\n",
            "Epoch: 121 loss_train: 0.496 acc_train: 0.783 loss_test: 0.594 acc_test: 0.672 AUC = 0.76 time: 0.497s\n",
            "Epoch: 122 loss_train: 0.497 acc_train: 0.768 loss_test: 0.599 acc_test: 0.682 AUC = 0.76 time: 0.497s\n",
            "Epoch: 123 loss_train: 0.500 acc_train: 0.768 loss_test: 0.596 acc_test: 0.677 AUC = 0.76 time: 0.497s\n",
            "Epoch: 124 loss_train: 0.494 acc_train: 0.777 loss_test: 0.628 acc_test: 0.669 AUC = 0.74 time: 0.533s\n",
            "Epoch: 125 loss_train: 0.497 acc_train: 0.772 loss_test: 0.592 acc_test: 0.675 AUC = 0.76 time: 0.497s\n",
            "Epoch: 126 loss_train: 0.498 acc_train: 0.780 loss_test: 0.608 acc_test: 0.673 AUC = 0.76 time: 0.497s\n",
            "Epoch: 127 loss_train: 0.491 acc_train: 0.781 loss_test: 0.602 acc_test: 0.695 AUC = 0.76 time: 0.497s\n",
            "Epoch: 128 loss_train: 0.495 acc_train: 0.765 loss_test: 0.594 acc_test: 0.677 AUC = 0.76 time: 0.497s\n",
            "Epoch: 129 loss_train: 0.491 acc_train: 0.787 loss_test: 0.624 acc_test: 0.664 AUC = 0.75 time: 0.497s\n",
            "Epoch: 130 loss_train: 0.498 acc_train: 0.772 loss_test: 0.562 acc_test: 0.711 AUC = 0.81 time: 0.497s\n",
            "Epoch: 131 loss_train: 0.481 acc_train: 0.782 loss_test: 0.602 acc_test: 0.680 AUC = 0.77 time: 0.496s\n",
            "Epoch: 132 loss_train: 0.493 acc_train: 0.775 loss_test: 0.609 acc_test: 0.672 AUC = 0.76 time: 0.497s\n",
            "Epoch: 133 loss_train: 0.494 acc_train: 0.782 loss_test: 0.591 acc_test: 0.677 AUC = 0.78 time: 0.497s\n",
            "Epoch: 134 loss_train: 0.500 acc_train: 0.772 loss_test: 0.586 acc_test: 0.697 AUC = 0.77 time: 0.497s\n",
            "Epoch: 135 loss_train: 0.495 acc_train: 0.777 loss_test: 0.615 acc_test: 0.698 AUC = 0.75 time: 0.497s\n",
            "Epoch: 136 loss_train: 0.492 acc_train: 0.777 loss_test: 0.594 acc_test: 0.691 AUC = 0.77 time: 0.499s\n",
            "Epoch: 137 loss_train: 0.491 acc_train: 0.775 loss_test: 0.596 acc_test: 0.683 AUC = 0.76 time: 0.498s\n",
            "Epoch: 138 loss_train: 0.489 acc_train: 0.790 loss_test: 0.597 acc_test: 0.692 AUC = 0.75 time: 0.497s\n",
            "Epoch: 139 loss_train: 0.475 acc_train: 0.782 loss_test: 0.603 acc_test: 0.689 AUC = 0.76 time: 0.496s\n",
            "Epoch: 140 loss_train: 0.498 acc_train: 0.779 loss_test: 0.610 acc_test: 0.682 AUC = 0.76 time: 0.496s\n",
            "Epoch: 141 loss_train: 0.480 acc_train: 0.788 loss_test: 0.590 acc_test: 0.689 AUC = 0.78 time: 0.497s\n",
            "Epoch: 142 loss_train: 0.488 acc_train: 0.783 loss_test: 0.589 acc_test: 0.695 AUC = 0.77 time: 0.497s\n",
            "Epoch: 143 loss_train: 0.501 acc_train: 0.784 loss_test: 0.582 acc_test: 0.705 AUC = 0.78 time: 0.496s\n",
            "Epoch: 144 loss_train: 0.495 acc_train: 0.785 loss_test: 0.575 acc_test: 0.713 AUC = 0.79 time: 0.497s\n",
            "Epoch: 145 loss_train: 0.494 acc_train: 0.777 loss_test: 0.614 acc_test: 0.698 AUC = 0.76 time: 0.496s\n",
            "Epoch: 146 loss_train: 0.489 acc_train: 0.779 loss_test: 0.599 acc_test: 0.679 AUC = 0.78 time: 0.496s\n",
            "Epoch: 147 loss_train: 0.488 acc_train: 0.784 loss_test: 0.626 acc_test: 0.689 AUC = 0.76 time: 0.496s\n",
            "Epoch: 148 loss_train: 0.492 acc_train: 0.779 loss_test: 0.605 acc_test: 0.689 AUC = 0.76 time: 0.497s\n",
            "Epoch: 149 loss_train: 0.496 acc_train: 0.782 loss_test: 0.676 acc_test: 0.682 AUC = 0.74 time: 0.496s\n",
            "Epoch: 150 loss_train: 0.475 acc_train: 0.787 loss_test: 0.576 acc_test: 0.691 AUC = 0.78 time: 0.496s\n",
            "Epoch: 151 loss_train: 0.479 acc_train: 0.786 loss_test: 0.617 acc_test: 0.694 AUC = 0.76 time: 0.496s\n",
            "Epoch: 152 loss_train: 0.495 acc_train: 0.768 loss_test: 0.580 acc_test: 0.710 AUC = 0.78 time: 0.496s\n",
            "Epoch: 153 loss_train: 0.490 acc_train: 0.787 loss_test: 0.625 acc_test: 0.669 AUC = 0.75 time: 0.496s\n",
            "Epoch: 154 loss_train: 0.501 acc_train: 0.775 loss_test: 0.596 acc_test: 0.688 AUC = 0.76 time: 0.497s\n",
            "Epoch: 155 loss_train: 0.481 acc_train: 0.798 loss_test: 0.612 acc_test: 0.677 AUC = 0.75 time: 0.496s\n",
            "Epoch: 156 loss_train: 0.490 acc_train: 0.787 loss_test: 0.588 acc_test: 0.703 AUC = 0.78 time: 0.496s\n",
            "Epoch: 157 loss_train: 0.473 acc_train: 0.790 loss_test: 0.570 acc_test: 0.707 AUC = 0.78 time: 0.497s\n",
            "Epoch: 158 loss_train: 0.488 acc_train: 0.783 loss_test: 0.586 acc_test: 0.692 AUC = 0.77 time: 0.496s\n",
            "Epoch: 159 loss_train: 0.471 acc_train: 0.784 loss_test: 0.580 acc_test: 0.697 AUC = 0.78 time: 0.496s\n",
            "Epoch: 160 loss_train: 0.492 acc_train: 0.775 loss_test: 0.565 acc_test: 0.716 AUC = 0.79 time: 0.496s\n",
            "Epoch: 161 loss_train: 0.490 acc_train: 0.782 loss_test: 0.585 acc_test: 0.704 AUC = 0.78 time: 0.496s\n",
            "Epoch: 162 loss_train: 0.473 acc_train: 0.790 loss_test: 0.610 acc_test: 0.682 AUC = 0.75 time: 0.496s\n",
            "Epoch: 163 loss_train: 0.477 acc_train: 0.788 loss_test: 0.578 acc_test: 0.703 AUC = 0.79 time: 0.496s\n",
            "Epoch: 164 loss_train: 0.475 acc_train: 0.788 loss_test: 0.579 acc_test: 0.708 AUC = 0.79 time: 0.496s\n",
            "Epoch: 165 loss_train: 0.480 acc_train: 0.793 loss_test: 0.584 acc_test: 0.705 AUC = 0.77 time: 0.496s\n",
            "Epoch: 166 loss_train: 0.484 acc_train: 0.781 loss_test: 0.590 acc_test: 0.694 AUC = 0.77 time: 0.496s\n",
            "Epoch: 167 loss_train: 0.485 acc_train: 0.781 loss_test: 0.608 acc_test: 0.682 AUC = 0.77 time: 0.496s\n",
            "Epoch: 168 loss_train: 0.482 acc_train: 0.784 loss_test: 0.601 acc_test: 0.704 AUC = 0.78 time: 0.496s\n",
            "Epoch: 169 loss_train: 0.492 acc_train: 0.779 loss_test: 0.573 acc_test: 0.694 AUC = 0.79 time: 0.496s\n",
            "Epoch: 170 loss_train: 0.491 acc_train: 0.775 loss_test: 0.601 acc_test: 0.695 AUC = 0.77 time: 0.497s\n",
            "Epoch: 171 loss_train: 0.493 acc_train: 0.780 loss_test: 0.603 acc_test: 0.697 AUC = 0.77 time: 0.496s\n",
            "Epoch: 172 loss_train: 0.478 acc_train: 0.781 loss_test: 0.564 acc_test: 0.701 AUC = 0.80 time: 0.497s\n",
            "Epoch: 173 loss_train: 0.474 acc_train: 0.791 loss_test: 0.569 acc_test: 0.689 AUC = 0.79 time: 0.496s\n",
            "Epoch: 174 loss_train: 0.498 acc_train: 0.775 loss_test: 0.590 acc_test: 0.697 AUC = 0.77 time: 0.496s\n",
            "Epoch: 175 loss_train: 0.480 acc_train: 0.779 loss_test: 0.547 acc_test: 0.717 AUC = 0.81 time: 0.497s\n",
            "Epoch: 176 loss_train: 0.494 acc_train: 0.777 loss_test: 0.601 acc_test: 0.692 AUC = 0.77 time: 0.497s\n",
            "Epoch: 177 loss_train: 0.487 acc_train: 0.787 loss_test: 0.607 acc_test: 0.700 AUC = 0.76 time: 0.496s\n",
            "Epoch: 178 loss_train: 0.472 acc_train: 0.790 loss_test: 0.569 acc_test: 0.710 AUC = 0.79 time: 0.496s\n",
            "Epoch: 179 loss_train: 0.473 acc_train: 0.789 loss_test: 0.593 acc_test: 0.683 AUC = 0.77 time: 0.497s\n",
            "Epoch: 180 loss_train: 0.486 acc_train: 0.792 loss_test: 0.579 acc_test: 0.708 AUC = 0.78 time: 0.496s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 114.8091s\n",
            "Loading 129th epoch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-c368eb4b4196>:179: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(sv_path +'{}.pkl'.format(best_epoch)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set results: loss= 0.5500 accuracy= 0.7099 AUC = 0.81 False Alarm Rate = 0.3019 F1 Score = 0.6204\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-c368eb4b4196>:144: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  idx_train = torch.tensor(idx_train).cuda()\n",
            "<ipython-input-5-c368eb4b4196>:145: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  idx_test = torch.tensor(idx_test).cuda()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001 loss_train: 0.729 acc_train: 0.435 loss_test: 0.715 acc_test: 0.508 AUC = 0.51 time: 0.497s\n",
            "Epoch: 002 loss_train: 0.704 acc_train: 0.483 loss_test: 0.701 acc_test: 0.501 AUC = 0.52 time: 0.496s\n",
            "Epoch: 003 loss_train: 0.721 acc_train: 0.473 loss_test: 0.720 acc_test: 0.515 AUC = 0.52 time: 0.496s\n",
            "Epoch: 004 loss_train: 0.703 acc_train: 0.509 loss_test: 0.705 acc_test: 0.465 AUC = 0.49 time: 0.496s\n",
            "Epoch: 005 loss_train: 0.701 acc_train: 0.524 loss_test: 0.702 acc_test: 0.518 AUC = 0.53 time: 0.496s\n",
            "Epoch: 006 loss_train: 0.686 acc_train: 0.561 loss_test: 0.693 acc_test: 0.533 AUC = 0.53 time: 0.496s\n",
            "Epoch: 007 loss_train: 0.689 acc_train: 0.563 loss_test: 0.701 acc_test: 0.515 AUC = 0.53 time: 0.496s\n",
            "Epoch: 008 loss_train: 0.686 acc_train: 0.589 loss_test: 0.700 acc_test: 0.530 AUC = 0.54 time: 0.498s\n",
            "Epoch: 009 loss_train: 0.686 acc_train: 0.596 loss_test: 0.699 acc_test: 0.505 AUC = 0.51 time: 0.496s\n",
            "Epoch: 010 loss_train: 0.669 acc_train: 0.626 loss_test: 0.697 acc_test: 0.530 AUC = 0.49 time: 0.496s\n",
            "Epoch: 011 loss_train: 0.670 acc_train: 0.633 loss_test: 0.689 acc_test: 0.535 AUC = 0.53 time: 0.496s\n",
            "Epoch: 012 loss_train: 0.671 acc_train: 0.638 loss_test: 0.698 acc_test: 0.539 AUC = 0.49 time: 0.497s\n",
            "Epoch: 013 loss_train: 0.661 acc_train: 0.661 loss_test: 0.689 acc_test: 0.543 AUC = 0.53 time: 0.497s\n",
            "Epoch: 014 loss_train: 0.653 acc_train: 0.682 loss_test: 0.691 acc_test: 0.536 AUC = 0.53 time: 0.497s\n",
            "Epoch: 015 loss_train: 0.653 acc_train: 0.674 loss_test: 0.692 acc_test: 0.536 AUC = 0.54 time: 0.498s\n",
            "Epoch: 016 loss_train: 0.655 acc_train: 0.669 loss_test: 0.692 acc_test: 0.555 AUC = 0.54 time: 0.497s\n",
            "Epoch: 017 loss_train: 0.650 acc_train: 0.674 loss_test: 0.700 acc_test: 0.530 AUC = 0.51 time: 0.496s\n",
            "Epoch: 018 loss_train: 0.645 acc_train: 0.687 loss_test: 0.689 acc_test: 0.560 AUC = 0.57 time: 0.496s\n",
            "Epoch: 019 loss_train: 0.636 acc_train: 0.688 loss_test: 0.699 acc_test: 0.536 AUC = 0.54 time: 0.496s\n",
            "Epoch: 020 loss_train: 0.639 acc_train: 0.691 loss_test: 0.696 acc_test: 0.548 AUC = 0.54 time: 0.497s\n",
            "Epoch: 021 loss_train: 0.635 acc_train: 0.690 loss_test: 0.700 acc_test: 0.555 AUC = 0.52 time: 0.497s\n",
            "Epoch: 022 loss_train: 0.631 acc_train: 0.695 loss_test: 0.696 acc_test: 0.543 AUC = 0.57 time: 0.496s\n",
            "Epoch: 023 loss_train: 0.625 acc_train: 0.695 loss_test: 0.703 acc_test: 0.551 AUC = 0.54 time: 0.497s\n",
            "Epoch: 024 loss_train: 0.627 acc_train: 0.699 loss_test: 0.716 acc_test: 0.551 AUC = 0.53 time: 0.497s\n",
            "Epoch: 025 loss_train: 0.622 acc_train: 0.700 loss_test: 0.699 acc_test: 0.546 AUC = 0.57 time: 0.498s\n",
            "Epoch: 026 loss_train: 0.620 acc_train: 0.701 loss_test: 0.698 acc_test: 0.561 AUC = 0.57 time: 0.497s\n",
            "Epoch: 027 loss_train: 0.613 acc_train: 0.693 loss_test: 0.716 acc_test: 0.551 AUC = 0.56 time: 0.498s\n",
            "Epoch: 028 loss_train: 0.620 acc_train: 0.694 loss_test: 0.712 acc_test: 0.541 AUC = 0.57 time: 0.498s\n",
            "Epoch: 029 loss_train: 0.614 acc_train: 0.696 loss_test: 0.720 acc_test: 0.554 AUC = 0.57 time: 0.499s\n",
            "Epoch: 030 loss_train: 0.608 acc_train: 0.696 loss_test: 0.709 acc_test: 0.552 AUC = 0.61 time: 0.497s\n",
            "Epoch: 031 loss_train: 0.610 acc_train: 0.701 loss_test: 0.710 acc_test: 0.548 AUC = 0.59 time: 0.499s\n",
            "Epoch: 032 loss_train: 0.599 acc_train: 0.701 loss_test: 0.711 acc_test: 0.557 AUC = 0.59 time: 0.499s\n",
            "Epoch: 033 loss_train: 0.607 acc_train: 0.699 loss_test: 0.710 acc_test: 0.558 AUC = 0.60 time: 0.497s\n",
            "Epoch: 034 loss_train: 0.600 acc_train: 0.711 loss_test: 0.722 acc_test: 0.552 AUC = 0.60 time: 0.497s\n",
            "Epoch: 035 loss_train: 0.606 acc_train: 0.697 loss_test: 0.725 acc_test: 0.566 AUC = 0.56 time: 0.497s\n",
            "Epoch: 036 loss_train: 0.598 acc_train: 0.707 loss_test: 0.709 acc_test: 0.566 AUC = 0.61 time: 0.497s\n",
            "Epoch: 037 loss_train: 0.598 acc_train: 0.705 loss_test: 0.713 acc_test: 0.560 AUC = 0.62 time: 0.497s\n",
            "Epoch: 038 loss_train: 0.600 acc_train: 0.707 loss_test: 0.713 acc_test: 0.570 AUC = 0.62 time: 0.497s\n",
            "Epoch: 039 loss_train: 0.587 acc_train: 0.705 loss_test: 0.716 acc_test: 0.570 AUC = 0.60 time: 0.497s\n",
            "Epoch: 040 loss_train: 0.592 acc_train: 0.714 loss_test: 0.703 acc_test: 0.579 AUC = 0.62 time: 0.497s\n",
            "Epoch: 041 loss_train: 0.582 acc_train: 0.715 loss_test: 0.699 acc_test: 0.579 AUC = 0.64 time: 0.497s\n",
            "Epoch: 042 loss_train: 0.590 acc_train: 0.714 loss_test: 0.704 acc_test: 0.574 AUC = 0.63 time: 0.497s\n",
            "Epoch: 043 loss_train: 0.574 acc_train: 0.726 loss_test: 0.675 acc_test: 0.596 AUC = 0.67 time: 0.497s\n",
            "Epoch: 044 loss_train: 0.582 acc_train: 0.721 loss_test: 0.694 acc_test: 0.577 AUC = 0.63 time: 0.497s\n",
            "Epoch: 045 loss_train: 0.590 acc_train: 0.712 loss_test: 0.691 acc_test: 0.605 AUC = 0.65 time: 0.496s\n",
            "Epoch: 046 loss_train: 0.576 acc_train: 0.721 loss_test: 0.698 acc_test: 0.598 AUC = 0.64 time: 0.497s\n",
            "Epoch: 047 loss_train: 0.585 acc_train: 0.715 loss_test: 0.676 acc_test: 0.611 AUC = 0.65 time: 0.497s\n",
            "Epoch: 048 loss_train: 0.584 acc_train: 0.726 loss_test: 0.692 acc_test: 0.594 AUC = 0.65 time: 0.497s\n",
            "Epoch: 049 loss_train: 0.572 acc_train: 0.719 loss_test: 0.667 acc_test: 0.616 AUC = 0.66 time: 0.497s\n",
            "Epoch: 050 loss_train: 0.571 acc_train: 0.731 loss_test: 0.686 acc_test: 0.601 AUC = 0.63 time: 0.497s\n",
            "Epoch: 051 loss_train: 0.576 acc_train: 0.719 loss_test: 0.668 acc_test: 0.627 AUC = 0.66 time: 0.497s\n",
            "Epoch: 052 loss_train: 0.570 acc_train: 0.739 loss_test: 0.699 acc_test: 0.608 AUC = 0.63 time: 0.497s\n",
            "Epoch: 053 loss_train: 0.573 acc_train: 0.719 loss_test: 0.675 acc_test: 0.623 AUC = 0.66 time: 0.554s\n",
            "Epoch: 054 loss_train: 0.566 acc_train: 0.731 loss_test: 0.692 acc_test: 0.616 AUC = 0.64 time: 0.497s\n",
            "Epoch: 055 loss_train: 0.570 acc_train: 0.726 loss_test: 0.678 acc_test: 0.630 AUC = 0.65 time: 0.497s\n",
            "Epoch: 056 loss_train: 0.578 acc_train: 0.737 loss_test: 0.692 acc_test: 0.616 AUC = 0.64 time: 0.497s\n",
            "Epoch: 057 loss_train: 0.552 acc_train: 0.743 loss_test: 0.690 acc_test: 0.622 AUC = 0.65 time: 0.497s\n",
            "Epoch: 058 loss_train: 0.570 acc_train: 0.733 loss_test: 0.696 acc_test: 0.617 AUC = 0.65 time: 0.497s\n",
            "Epoch: 059 loss_train: 0.565 acc_train: 0.738 loss_test: 0.682 acc_test: 0.610 AUC = 0.65 time: 0.497s\n",
            "Epoch: 060 loss_train: 0.553 acc_train: 0.747 loss_test: 0.708 acc_test: 0.611 AUC = 0.64 time: 0.497s\n",
            "Epoch: 061 loss_train: 0.560 acc_train: 0.745 loss_test: 0.706 acc_test: 0.624 AUC = 0.65 time: 0.497s\n",
            "Epoch: 062 loss_train: 0.569 acc_train: 0.738 loss_test: 0.685 acc_test: 0.629 AUC = 0.65 time: 0.497s\n",
            "Epoch: 063 loss_train: 0.567 acc_train: 0.732 loss_test: 0.695 acc_test: 0.617 AUC = 0.67 time: 0.497s\n",
            "Epoch: 064 loss_train: 0.567 acc_train: 0.743 loss_test: 0.680 acc_test: 0.642 AUC = 0.67 time: 0.497s\n",
            "Epoch: 065 loss_train: 0.557 acc_train: 0.741 loss_test: 0.663 acc_test: 0.630 AUC = 0.69 time: 0.497s\n",
            "Epoch: 066 loss_train: 0.558 acc_train: 0.739 loss_test: 0.676 acc_test: 0.626 AUC = 0.66 time: 0.497s\n",
            "Epoch: 067 loss_train: 0.552 acc_train: 0.746 loss_test: 0.701 acc_test: 0.633 AUC = 0.66 time: 0.497s\n",
            "Epoch: 068 loss_train: 0.573 acc_train: 0.729 loss_test: 0.692 acc_test: 0.613 AUC = 0.65 time: 0.497s\n",
            "Epoch: 069 loss_train: 0.552 acc_train: 0.743 loss_test: 0.663 acc_test: 0.639 AUC = 0.68 time: 0.497s\n",
            "Epoch: 070 loss_train: 0.553 acc_train: 0.743 loss_test: 0.701 acc_test: 0.627 AUC = 0.66 time: 0.497s\n",
            "Epoch: 071 loss_train: 0.547 acc_train: 0.740 loss_test: 0.654 acc_test: 0.635 AUC = 0.70 time: 0.497s\n",
            "Epoch: 072 loss_train: 0.565 acc_train: 0.736 loss_test: 0.690 acc_test: 0.616 AUC = 0.66 time: 0.497s\n",
            "Epoch: 073 loss_train: 0.548 acc_train: 0.747 loss_test: 0.669 acc_test: 0.651 AUC = 0.68 time: 0.497s\n",
            "Epoch: 074 loss_train: 0.537 acc_train: 0.753 loss_test: 0.669 acc_test: 0.654 AUC = 0.67 time: 0.499s\n",
            "Epoch: 075 loss_train: 0.546 acc_train: 0.741 loss_test: 0.660 acc_test: 0.644 AUC = 0.67 time: 0.497s\n",
            "Epoch: 076 loss_train: 0.555 acc_train: 0.744 loss_test: 0.678 acc_test: 0.645 AUC = 0.68 time: 0.497s\n",
            "Epoch: 077 loss_train: 0.557 acc_train: 0.750 loss_test: 0.668 acc_test: 0.639 AUC = 0.69 time: 0.497s\n",
            "Epoch: 078 loss_train: 0.558 acc_train: 0.746 loss_test: 0.685 acc_test: 0.645 AUC = 0.66 time: 0.497s\n",
            "Epoch: 079 loss_train: 0.551 acc_train: 0.744 loss_test: 0.665 acc_test: 0.642 AUC = 0.69 time: 0.496s\n",
            "Epoch: 080 loss_train: 0.557 acc_train: 0.743 loss_test: 0.644 acc_test: 0.651 AUC = 0.70 time: 0.497s\n",
            "Epoch: 081 loss_train: 0.558 acc_train: 0.744 loss_test: 0.676 acc_test: 0.645 AUC = 0.69 time: 0.497s\n",
            "Epoch: 082 loss_train: 0.546 acc_train: 0.738 loss_test: 0.697 acc_test: 0.624 AUC = 0.67 time: 0.497s\n",
            "Epoch: 083 loss_train: 0.546 acc_train: 0.750 loss_test: 0.668 acc_test: 0.647 AUC = 0.68 time: 0.497s\n",
            "Epoch: 084 loss_train: 0.541 acc_train: 0.744 loss_test: 0.671 acc_test: 0.645 AUC = 0.68 time: 0.497s\n",
            "Epoch: 085 loss_train: 0.548 acc_train: 0.750 loss_test: 0.666 acc_test: 0.638 AUC = 0.68 time: 0.496s\n",
            "Epoch: 086 loss_train: 0.549 acc_train: 0.738 loss_test: 0.670 acc_test: 0.660 AUC = 0.68 time: 0.497s\n",
            "Epoch: 087 loss_train: 0.547 acc_train: 0.750 loss_test: 0.678 acc_test: 0.623 AUC = 0.67 time: 0.497s\n",
            "Epoch: 088 loss_train: 0.544 acc_train: 0.751 loss_test: 0.681 acc_test: 0.616 AUC = 0.67 time: 0.497s\n",
            "Epoch: 089 loss_train: 0.537 acc_train: 0.760 loss_test: 0.691 acc_test: 0.644 AUC = 0.67 time: 0.497s\n",
            "Epoch: 090 loss_train: 0.539 acc_train: 0.754 loss_test: 0.680 acc_test: 0.626 AUC = 0.68 time: 0.497s\n",
            "Epoch: 091 loss_train: 0.537 acc_train: 0.746 loss_test: 0.655 acc_test: 0.648 AUC = 0.70 time: 0.497s\n",
            "Epoch: 092 loss_train: 0.536 acc_train: 0.755 loss_test: 0.655 acc_test: 0.655 AUC = 0.71 time: 0.497s\n",
            "Epoch: 093 loss_train: 0.531 acc_train: 0.756 loss_test: 0.638 acc_test: 0.669 AUC = 0.71 time: 0.498s\n",
            "Epoch: 094 loss_train: 0.537 acc_train: 0.743 loss_test: 0.643 acc_test: 0.658 AUC = 0.72 time: 0.497s\n",
            "Epoch: 095 loss_train: 0.548 acc_train: 0.755 loss_test: 0.625 acc_test: 0.675 AUC = 0.73 time: 0.497s\n",
            "Epoch: 096 loss_train: 0.537 acc_train: 0.758 loss_test: 0.646 acc_test: 0.669 AUC = 0.71 time: 0.496s\n",
            "Epoch: 097 loss_train: 0.540 acc_train: 0.755 loss_test: 0.654 acc_test: 0.654 AUC = 0.70 time: 0.497s\n",
            "Epoch: 098 loss_train: 0.533 acc_train: 0.757 loss_test: 0.672 acc_test: 0.642 AUC = 0.69 time: 0.497s\n",
            "Epoch: 099 loss_train: 0.545 acc_train: 0.751 loss_test: 0.648 acc_test: 0.658 AUC = 0.71 time: 0.497s\n",
            "Epoch: 100 loss_train: 0.540 acc_train: 0.748 loss_test: 0.685 acc_test: 0.630 AUC = 0.67 time: 0.497s\n",
            "Epoch: 101 loss_train: 0.545 acc_train: 0.751 loss_test: 0.673 acc_test: 0.655 AUC = 0.69 time: 0.496s\n",
            "Epoch: 102 loss_train: 0.535 acc_train: 0.751 loss_test: 0.672 acc_test: 0.657 AUC = 0.68 time: 0.497s\n",
            "Epoch: 103 loss_train: 0.535 acc_train: 0.749 loss_test: 0.669 acc_test: 0.663 AUC = 0.69 time: 0.497s\n",
            "Epoch: 104 loss_train: 0.528 acc_train: 0.756 loss_test: 0.657 acc_test: 0.649 AUC = 0.70 time: 0.533s\n",
            "Epoch: 105 loss_train: 0.538 acc_train: 0.756 loss_test: 0.640 acc_test: 0.667 AUC = 0.71 time: 0.497s\n",
            "Epoch: 106 loss_train: 0.540 acc_train: 0.755 loss_test: 0.645 acc_test: 0.667 AUC = 0.71 time: 0.497s\n",
            "Epoch: 107 loss_train: 0.531 acc_train: 0.762 loss_test: 0.671 acc_test: 0.648 AUC = 0.69 time: 0.497s\n",
            "Epoch: 108 loss_train: 0.526 acc_train: 0.755 loss_test: 0.633 acc_test: 0.670 AUC = 0.72 time: 0.497s\n",
            "Epoch: 109 loss_train: 0.519 acc_train: 0.765 loss_test: 0.638 acc_test: 0.664 AUC = 0.71 time: 0.497s\n",
            "Epoch: 110 loss_train: 0.537 acc_train: 0.755 loss_test: 0.651 acc_test: 0.670 AUC = 0.70 time: 0.497s\n",
            "Epoch: 111 loss_train: 0.519 acc_train: 0.762 loss_test: 0.642 acc_test: 0.647 AUC = 0.72 time: 0.499s\n",
            "Epoch: 112 loss_train: 0.526 acc_train: 0.763 loss_test: 0.627 acc_test: 0.647 AUC = 0.73 time: 0.499s\n",
            "Epoch: 113 loss_train: 0.519 acc_train: 0.757 loss_test: 0.630 acc_test: 0.661 AUC = 0.73 time: 0.497s\n",
            "Epoch: 114 loss_train: 0.538 acc_train: 0.742 loss_test: 0.646 acc_test: 0.648 AUC = 0.72 time: 0.497s\n",
            "Epoch: 115 loss_train: 0.533 acc_train: 0.753 loss_test: 0.620 acc_test: 0.666 AUC = 0.74 time: 0.496s\n",
            "Epoch: 116 loss_train: 0.538 acc_train: 0.751 loss_test: 0.650 acc_test: 0.661 AUC = 0.71 time: 0.497s\n",
            "Epoch: 117 loss_train: 0.527 acc_train: 0.762 loss_test: 0.624 acc_test: 0.669 AUC = 0.72 time: 0.497s\n",
            "Epoch: 118 loss_train: 0.532 acc_train: 0.748 loss_test: 0.650 acc_test: 0.655 AUC = 0.71 time: 0.496s\n",
            "Epoch: 119 loss_train: 0.530 acc_train: 0.749 loss_test: 0.644 acc_test: 0.664 AUC = 0.73 time: 0.497s\n",
            "Epoch: 120 loss_train: 0.518 acc_train: 0.758 loss_test: 0.633 acc_test: 0.680 AUC = 0.72 time: 0.496s\n",
            "Epoch: 121 loss_train: 0.527 acc_train: 0.769 loss_test: 0.651 acc_test: 0.672 AUC = 0.70 time: 0.497s\n",
            "Epoch: 122 loss_train: 0.527 acc_train: 0.751 loss_test: 0.638 acc_test: 0.652 AUC = 0.72 time: 0.497s\n",
            "Epoch: 123 loss_train: 0.520 acc_train: 0.767 loss_test: 0.679 acc_test: 0.652 AUC = 0.70 time: 0.497s\n",
            "Epoch: 124 loss_train: 0.536 acc_train: 0.758 loss_test: 0.639 acc_test: 0.658 AUC = 0.72 time: 0.497s\n",
            "Epoch: 125 loss_train: 0.518 acc_train: 0.751 loss_test: 0.641 acc_test: 0.660 AUC = 0.74 time: 0.497s\n",
            "Epoch: 126 loss_train: 0.522 acc_train: 0.756 loss_test: 0.655 acc_test: 0.658 AUC = 0.70 time: 0.497s\n",
            "Epoch: 127 loss_train: 0.522 acc_train: 0.762 loss_test: 0.610 acc_test: 0.673 AUC = 0.75 time: 0.497s\n",
            "Epoch: 128 loss_train: 0.523 acc_train: 0.760 loss_test: 0.649 acc_test: 0.676 AUC = 0.72 time: 0.496s\n",
            "Epoch: 129 loss_train: 0.506 acc_train: 0.773 loss_test: 0.613 acc_test: 0.675 AUC = 0.75 time: 0.497s\n",
            "Epoch: 130 loss_train: 0.511 acc_train: 0.768 loss_test: 0.632 acc_test: 0.672 AUC = 0.73 time: 0.497s\n",
            "Epoch: 131 loss_train: 0.525 acc_train: 0.760 loss_test: 0.629 acc_test: 0.680 AUC = 0.73 time: 0.497s\n",
            "Epoch: 132 loss_train: 0.524 acc_train: 0.764 loss_test: 0.622 acc_test: 0.688 AUC = 0.74 time: 0.497s\n",
            "Epoch: 133 loss_train: 0.511 acc_train: 0.776 loss_test: 0.621 acc_test: 0.654 AUC = 0.74 time: 0.496s\n",
            "Epoch: 134 loss_train: 0.508 acc_train: 0.770 loss_test: 0.620 acc_test: 0.677 AUC = 0.73 time: 0.496s\n",
            "Epoch: 135 loss_train: 0.517 acc_train: 0.763 loss_test: 0.620 acc_test: 0.677 AUC = 0.74 time: 0.497s\n",
            "Epoch: 136 loss_train: 0.516 acc_train: 0.764 loss_test: 0.617 acc_test: 0.685 AUC = 0.74 time: 0.496s\n",
            "Epoch: 137 loss_train: 0.519 acc_train: 0.767 loss_test: 0.611 acc_test: 0.667 AUC = 0.75 time: 0.497s\n",
            "Epoch: 138 loss_train: 0.511 acc_train: 0.763 loss_test: 0.648 acc_test: 0.663 AUC = 0.71 time: 0.497s\n",
            "Epoch: 139 loss_train: 0.517 acc_train: 0.765 loss_test: 0.653 acc_test: 0.661 AUC = 0.72 time: 0.497s\n",
            "Epoch: 140 loss_train: 0.512 acc_train: 0.763 loss_test: 0.623 acc_test: 0.666 AUC = 0.74 time: 0.497s\n",
            "Epoch: 141 loss_train: 0.525 acc_train: 0.753 loss_test: 0.645 acc_test: 0.660 AUC = 0.71 time: 0.497s\n",
            "Epoch: 142 loss_train: 0.508 acc_train: 0.774 loss_test: 0.619 acc_test: 0.677 AUC = 0.74 time: 0.496s\n",
            "Epoch: 143 loss_train: 0.506 acc_train: 0.768 loss_test: 0.623 acc_test: 0.680 AUC = 0.73 time: 0.496s\n",
            "Epoch: 144 loss_train: 0.520 acc_train: 0.765 loss_test: 0.623 acc_test: 0.685 AUC = 0.73 time: 0.497s\n",
            "Epoch: 145 loss_train: 0.520 acc_train: 0.768 loss_test: 0.615 acc_test: 0.683 AUC = 0.74 time: 0.496s\n",
            "Epoch: 146 loss_train: 0.516 acc_train: 0.768 loss_test: 0.644 acc_test: 0.672 AUC = 0.72 time: 0.497s\n",
            "Epoch: 147 loss_train: 0.530 acc_train: 0.760 loss_test: 0.636 acc_test: 0.661 AUC = 0.72 time: 0.496s\n",
            "Epoch: 148 loss_train: 0.510 acc_train: 0.773 loss_test: 0.628 acc_test: 0.655 AUC = 0.73 time: 0.497s\n",
            "Epoch: 149 loss_train: 0.498 acc_train: 0.778 loss_test: 0.639 acc_test: 0.682 AUC = 0.72 time: 0.496s\n",
            "Epoch: 150 loss_train: 0.521 acc_train: 0.768 loss_test: 0.623 acc_test: 0.682 AUC = 0.73 time: 0.497s\n",
            "Epoch: 151 loss_train: 0.508 acc_train: 0.767 loss_test: 0.639 acc_test: 0.667 AUC = 0.73 time: 0.497s\n",
            "Epoch: 152 loss_train: 0.525 acc_train: 0.770 loss_test: 0.643 acc_test: 0.657 AUC = 0.73 time: 0.497s\n",
            "Epoch: 153 loss_train: 0.518 acc_train: 0.750 loss_test: 0.608 acc_test: 0.676 AUC = 0.75 time: 0.497s\n",
            "Epoch: 154 loss_train: 0.507 acc_train: 0.776 loss_test: 0.636 acc_test: 0.672 AUC = 0.73 time: 0.496s\n",
            "Epoch: 155 loss_train: 0.506 acc_train: 0.772 loss_test: 0.654 acc_test: 0.667 AUC = 0.71 time: 0.497s\n",
            "Epoch: 156 loss_train: 0.511 acc_train: 0.767 loss_test: 0.634 acc_test: 0.670 AUC = 0.73 time: 0.497s\n",
            "Epoch: 157 loss_train: 0.506 acc_train: 0.765 loss_test: 0.621 acc_test: 0.682 AUC = 0.75 time: 0.496s\n",
            "Epoch: 158 loss_train: 0.499 acc_train: 0.760 loss_test: 0.595 acc_test: 0.669 AUC = 0.76 time: 0.497s\n",
            "Epoch: 159 loss_train: 0.500 acc_train: 0.772 loss_test: 0.599 acc_test: 0.680 AUC = 0.76 time: 0.497s\n",
            "Epoch: 160 loss_train: 0.519 acc_train: 0.767 loss_test: 0.631 acc_test: 0.657 AUC = 0.72 time: 0.497s\n",
            "Epoch: 161 loss_train: 0.515 acc_train: 0.769 loss_test: 0.616 acc_test: 0.682 AUC = 0.74 time: 0.496s\n",
            "Epoch: 162 loss_train: 0.501 acc_train: 0.773 loss_test: 0.607 acc_test: 0.661 AUC = 0.76 time: 0.496s\n",
            "Epoch: 163 loss_train: 0.512 acc_train: 0.763 loss_test: 0.613 acc_test: 0.680 AUC = 0.74 time: 0.496s\n",
            "Epoch: 164 loss_train: 0.501 acc_train: 0.776 loss_test: 0.584 acc_test: 0.688 AUC = 0.77 time: 0.496s\n",
            "Epoch: 165 loss_train: 0.493 acc_train: 0.772 loss_test: 0.591 acc_test: 0.688 AUC = 0.77 time: 0.496s\n",
            "Epoch: 166 loss_train: 0.509 acc_train: 0.770 loss_test: 0.575 acc_test: 0.697 AUC = 0.79 time: 0.497s\n",
            "Epoch: 167 loss_train: 0.498 acc_train: 0.782 loss_test: 0.611 acc_test: 0.686 AUC = 0.74 time: 0.497s\n",
            "Epoch: 168 loss_train: 0.495 acc_train: 0.767 loss_test: 0.601 acc_test: 0.701 AUC = 0.76 time: 0.496s\n",
            "Epoch: 169 loss_train: 0.500 acc_train: 0.779 loss_test: 0.603 acc_test: 0.679 AUC = 0.77 time: 0.497s\n",
            "Epoch: 170 loss_train: 0.507 acc_train: 0.772 loss_test: 0.621 acc_test: 0.666 AUC = 0.74 time: 0.497s\n",
            "Epoch: 171 loss_train: 0.508 acc_train: 0.761 loss_test: 0.595 acc_test: 0.694 AUC = 0.77 time: 0.497s\n",
            "Epoch: 172 loss_train: 0.508 acc_train: 0.773 loss_test: 0.610 acc_test: 0.675 AUC = 0.74 time: 0.497s\n",
            "Epoch: 173 loss_train: 0.496 acc_train: 0.777 loss_test: 0.567 acc_test: 0.695 AUC = 0.79 time: 0.496s\n",
            "Epoch: 174 loss_train: 0.497 acc_train: 0.780 loss_test: 0.602 acc_test: 0.686 AUC = 0.77 time: 0.496s\n",
            "Epoch: 175 loss_train: 0.500 acc_train: 0.766 loss_test: 0.623 acc_test: 0.663 AUC = 0.75 time: 0.497s\n",
            "Epoch: 176 loss_train: 0.491 acc_train: 0.775 loss_test: 0.617 acc_test: 0.661 AUC = 0.74 time: 0.497s\n",
            "Epoch: 177 loss_train: 0.501 acc_train: 0.777 loss_test: 0.604 acc_test: 0.670 AUC = 0.75 time: 0.496s\n",
            "Epoch: 178 loss_train: 0.505 acc_train: 0.776 loss_test: 0.562 acc_test: 0.695 AUC = 0.80 time: 0.496s\n",
            "Epoch: 179 loss_train: 0.496 acc_train: 0.777 loss_test: 0.618 acc_test: 0.708 AUC = 0.74 time: 0.497s\n",
            "Epoch: 180 loss_train: 0.495 acc_train: 0.777 loss_test: 0.628 acc_test: 0.688 AUC = 0.73 time: 0.497s\n",
            "Epoch: 181 loss_train: 0.502 acc_train: 0.776 loss_test: 0.576 acc_test: 0.689 AUC = 0.78 time: 0.497s\n",
            "Epoch: 182 loss_train: 0.504 acc_train: 0.784 loss_test: 0.600 acc_test: 0.700 AUC = 0.76 time: 0.497s\n",
            "Epoch: 183 loss_train: 0.513 acc_train: 0.768 loss_test: 0.603 acc_test: 0.698 AUC = 0.76 time: 0.497s\n",
            "Epoch: 184 loss_train: 0.494 acc_train: 0.783 loss_test: 0.578 acc_test: 0.683 AUC = 0.77 time: 0.497s\n",
            "Epoch: 185 loss_train: 0.488 acc_train: 0.774 loss_test: 0.612 acc_test: 0.676 AUC = 0.74 time: 0.497s\n",
            "Epoch: 186 loss_train: 0.496 acc_train: 0.773 loss_test: 0.612 acc_test: 0.683 AUC = 0.74 time: 0.497s\n",
            "Epoch: 187 loss_train: 0.498 acc_train: 0.777 loss_test: 0.596 acc_test: 0.677 AUC = 0.76 time: 0.497s\n",
            "Epoch: 188 loss_train: 0.480 acc_train: 0.781 loss_test: 0.605 acc_test: 0.685 AUC = 0.75 time: 0.497s\n",
            "Epoch: 189 loss_train: 0.488 acc_train: 0.784 loss_test: 0.590 acc_test: 0.705 AUC = 0.76 time: 0.497s\n",
            "Epoch: 190 loss_train: 0.502 acc_train: 0.770 loss_test: 0.600 acc_test: 0.698 AUC = 0.77 time: 0.497s\n",
            "Epoch: 191 loss_train: 0.496 acc_train: 0.777 loss_test: 0.598 acc_test: 0.697 AUC = 0.76 time: 0.496s\n",
            "Epoch: 192 loss_train: 0.492 acc_train: 0.772 loss_test: 0.594 acc_test: 0.694 AUC = 0.77 time: 0.496s\n",
            "Epoch: 193 loss_train: 0.477 acc_train: 0.777 loss_test: 0.595 acc_test: 0.698 AUC = 0.77 time: 0.497s\n",
            "Epoch: 194 loss_train: 0.485 acc_train: 0.784 loss_test: 0.629 acc_test: 0.651 AUC = 0.75 time: 0.497s\n",
            "Epoch: 195 loss_train: 0.502 acc_train: 0.774 loss_test: 0.643 acc_test: 0.685 AUC = 0.75 time: 0.497s\n",
            "Epoch: 196 loss_train: 0.481 acc_train: 0.785 loss_test: 0.593 acc_test: 0.700 AUC = 0.78 time: 0.497s\n",
            "Epoch: 197 loss_train: 0.496 acc_train: 0.772 loss_test: 0.614 acc_test: 0.691 AUC = 0.76 time: 0.497s\n",
            "Epoch: 198 loss_train: 0.480 acc_train: 0.777 loss_test: 0.589 acc_test: 0.705 AUC = 0.77 time: 0.497s\n",
            "Epoch: 199 loss_train: 0.483 acc_train: 0.780 loss_test: 0.620 acc_test: 0.680 AUC = 0.75 time: 0.497s\n",
            "Epoch: 200 loss_train: 0.487 acc_train: 0.772 loss_test: 0.599 acc_test: 0.698 AUC = 0.78 time: 0.497s\n",
            "Epoch: 201 loss_train: 0.495 acc_train: 0.768 loss_test: 0.573 acc_test: 0.716 AUC = 0.78 time: 0.497s\n",
            "Epoch: 202 loss_train: 0.490 acc_train: 0.773 loss_test: 0.577 acc_test: 0.685 AUC = 0.78 time: 0.497s\n",
            "Epoch: 203 loss_train: 0.492 acc_train: 0.775 loss_test: 0.586 acc_test: 0.700 AUC = 0.77 time: 0.497s\n",
            "Epoch: 204 loss_train: 0.477 acc_train: 0.797 loss_test: 0.595 acc_test: 0.701 AUC = 0.77 time: 0.498s\n",
            "Epoch: 205 loss_train: 0.487 acc_train: 0.786 loss_test: 0.603 acc_test: 0.689 AUC = 0.75 time: 0.497s\n",
            "Epoch: 206 loss_train: 0.488 acc_train: 0.777 loss_test: 0.593 acc_test: 0.686 AUC = 0.77 time: 0.496s\n",
            "Epoch: 207 loss_train: 0.492 acc_train: 0.777 loss_test: 0.594 acc_test: 0.688 AUC = 0.78 time: 0.497s\n",
            "Epoch: 208 loss_train: 0.478 acc_train: 0.786 loss_test: 0.585 acc_test: 0.711 AUC = 0.77 time: 0.497s\n",
            "Epoch: 209 loss_train: 0.476 acc_train: 0.786 loss_test: 0.606 acc_test: 0.695 AUC = 0.75 time: 0.497s\n",
            "Epoch: 210 loss_train: 0.487 acc_train: 0.779 loss_test: 0.593 acc_test: 0.700 AUC = 0.78 time: 0.497s\n",
            "Epoch: 211 loss_train: 0.479 acc_train: 0.785 loss_test: 0.594 acc_test: 0.683 AUC = 0.77 time: 0.497s\n",
            "Epoch: 212 loss_train: 0.480 acc_train: 0.780 loss_test: 0.570 acc_test: 0.710 AUC = 0.80 time: 0.497s\n",
            "Epoch: 213 loss_train: 0.492 acc_train: 0.772 loss_test: 0.581 acc_test: 0.700 AUC = 0.79 time: 0.497s\n",
            "Epoch: 214 loss_train: 0.495 acc_train: 0.777 loss_test: 0.569 acc_test: 0.708 AUC = 0.79 time: 0.497s\n",
            "Epoch: 215 loss_train: 0.489 acc_train: 0.784 loss_test: 0.581 acc_test: 0.692 AUC = 0.77 time: 0.497s\n",
            "Epoch: 216 loss_train: 0.468 acc_train: 0.789 loss_test: 0.597 acc_test: 0.703 AUC = 0.79 time: 0.497s\n",
            "Epoch: 217 loss_train: 0.478 acc_train: 0.783 loss_test: 0.605 acc_test: 0.692 AUC = 0.77 time: 0.498s\n",
            "Epoch: 218 loss_train: 0.484 acc_train: 0.791 loss_test: 0.623 acc_test: 0.667 AUC = 0.75 time: 0.497s\n",
            "Epoch: 219 loss_train: 0.484 acc_train: 0.786 loss_test: 0.593 acc_test: 0.683 AUC = 0.78 time: 0.497s\n",
            "Epoch: 220 loss_train: 0.476 acc_train: 0.779 loss_test: 0.602 acc_test: 0.697 AUC = 0.77 time: 0.497s\n",
            "Epoch: 221 loss_train: 0.473 acc_train: 0.787 loss_test: 0.574 acc_test: 0.697 AUC = 0.79 time: 0.497s\n",
            "Epoch: 222 loss_train: 0.475 acc_train: 0.791 loss_test: 0.590 acc_test: 0.705 AUC = 0.78 time: 0.497s\n",
            "Epoch: 223 loss_train: 0.490 acc_train: 0.772 loss_test: 0.554 acc_test: 0.722 AUC = 0.80 time: 0.497s\n",
            "Epoch: 224 loss_train: 0.490 acc_train: 0.775 loss_test: 0.599 acc_test: 0.688 AUC = 0.76 time: 0.497s\n",
            "Epoch: 225 loss_train: 0.474 acc_train: 0.787 loss_test: 0.560 acc_test: 0.708 AUC = 0.80 time: 0.497s\n",
            "Epoch: 226 loss_train: 0.493 acc_train: 0.780 loss_test: 0.611 acc_test: 0.683 AUC = 0.76 time: 0.500s\n",
            "Epoch: 227 loss_train: 0.476 acc_train: 0.785 loss_test: 0.574 acc_test: 0.705 AUC = 0.78 time: 0.497s\n",
            "Epoch: 228 loss_train: 0.483 acc_train: 0.782 loss_test: 0.606 acc_test: 0.686 AUC = 0.77 time: 0.498s\n",
            "Epoch: 229 loss_train: 0.466 acc_train: 0.791 loss_test: 0.598 acc_test: 0.694 AUC = 0.77 time: 0.497s\n",
            "Epoch: 230 loss_train: 0.485 acc_train: 0.781 loss_test: 0.585 acc_test: 0.705 AUC = 0.78 time: 0.497s\n",
            "Epoch: 231 loss_train: 0.466 acc_train: 0.796 loss_test: 0.592 acc_test: 0.714 AUC = 0.78 time: 0.497s\n",
            "Epoch: 232 loss_train: 0.477 acc_train: 0.785 loss_test: 0.594 acc_test: 0.698 AUC = 0.78 time: 0.497s\n",
            "Epoch: 233 loss_train: 0.466 acc_train: 0.785 loss_test: 0.593 acc_test: 0.698 AUC = 0.78 time: 0.497s\n",
            "Epoch: 234 loss_train: 0.479 acc_train: 0.789 loss_test: 0.577 acc_test: 0.700 AUC = 0.79 time: 0.497s\n",
            "Epoch: 235 loss_train: 0.481 acc_train: 0.782 loss_test: 0.565 acc_test: 0.710 AUC = 0.80 time: 0.497s\n",
            "Epoch: 236 loss_train: 0.473 acc_train: 0.782 loss_test: 0.594 acc_test: 0.686 AUC = 0.77 time: 0.497s\n",
            "Epoch: 237 loss_train: 0.480 acc_train: 0.783 loss_test: 0.571 acc_test: 0.695 AUC = 0.79 time: 0.497s\n",
            "Epoch: 238 loss_train: 0.485 acc_train: 0.782 loss_test: 0.578 acc_test: 0.708 AUC = 0.79 time: 0.497s\n",
            "Epoch: 239 loss_train: 0.477 acc_train: 0.790 loss_test: 0.555 acc_test: 0.697 AUC = 0.80 time: 0.497s\n",
            "Epoch: 240 loss_train: 0.481 acc_train: 0.785 loss_test: 0.550 acc_test: 0.714 AUC = 0.81 time: 0.497s\n",
            "Epoch: 241 loss_train: 0.477 acc_train: 0.785 loss_test: 0.567 acc_test: 0.723 AUC = 0.79 time: 0.497s\n",
            "Epoch: 242 loss_train: 0.479 acc_train: 0.787 loss_test: 0.557 acc_test: 0.716 AUC = 0.80 time: 0.497s\n",
            "Epoch: 243 loss_train: 0.456 acc_train: 0.800 loss_test: 0.566 acc_test: 0.711 AUC = 0.79 time: 0.497s\n",
            "Epoch: 244 loss_train: 0.477 acc_train: 0.794 loss_test: 0.596 acc_test: 0.694 AUC = 0.78 time: 0.497s\n",
            "Epoch: 245 loss_train: 0.472 acc_train: 0.788 loss_test: 0.577 acc_test: 0.705 AUC = 0.78 time: 0.497s\n",
            "Epoch: 246 loss_train: 0.482 acc_train: 0.798 loss_test: 0.596 acc_test: 0.697 AUC = 0.78 time: 0.497s\n",
            "Epoch: 247 loss_train: 0.476 acc_train: 0.784 loss_test: 0.562 acc_test: 0.726 AUC = 0.78 time: 0.497s\n",
            "Epoch: 248 loss_train: 0.477 acc_train: 0.791 loss_test: 0.583 acc_test: 0.704 AUC = 0.78 time: 0.497s\n",
            "Epoch: 249 loss_train: 0.469 acc_train: 0.792 loss_test: 0.579 acc_test: 0.695 AUC = 0.79 time: 0.496s\n",
            "Epoch: 250 loss_train: 0.468 acc_train: 0.785 loss_test: 0.561 acc_test: 0.728 AUC = 0.80 time: 0.497s\n",
            "Epoch: 251 loss_train: 0.464 acc_train: 0.788 loss_test: 0.564 acc_test: 0.698 AUC = 0.80 time: 0.497s\n",
            "Epoch: 252 loss_train: 0.456 acc_train: 0.800 loss_test: 0.601 acc_test: 0.685 AUC = 0.76 time: 0.497s\n",
            "Epoch: 253 loss_train: 0.462 acc_train: 0.786 loss_test: 0.572 acc_test: 0.692 AUC = 0.79 time: 0.496s\n",
            "Epoch: 254 loss_train: 0.472 acc_train: 0.792 loss_test: 0.564 acc_test: 0.714 AUC = 0.79 time: 0.497s\n",
            "Epoch: 255 loss_train: 0.485 acc_train: 0.789 loss_test: 0.592 acc_test: 0.700 AUC = 0.79 time: 0.497s\n",
            "Epoch: 256 loss_train: 0.464 acc_train: 0.798 loss_test: 0.570 acc_test: 0.704 AUC = 0.80 time: 0.496s\n",
            "Epoch: 257 loss_train: 0.472 acc_train: 0.788 loss_test: 0.556 acc_test: 0.705 AUC = 0.81 time: 0.497s\n",
            "Epoch: 258 loss_train: 0.474 acc_train: 0.782 loss_test: 0.591 acc_test: 0.692 AUC = 0.79 time: 0.497s\n",
            "Epoch: 259 loss_train: 0.468 acc_train: 0.799 loss_test: 0.599 acc_test: 0.682 AUC = 0.77 time: 0.497s\n",
            "Epoch: 260 loss_train: 0.474 acc_train: 0.789 loss_test: 0.596 acc_test: 0.677 AUC = 0.77 time: 0.497s\n",
            "Epoch: 261 loss_train: 0.475 acc_train: 0.786 loss_test: 0.577 acc_test: 0.705 AUC = 0.79 time: 0.497s\n",
            "Epoch: 262 loss_train: 0.460 acc_train: 0.792 loss_test: 0.553 acc_test: 0.730 AUC = 0.80 time: 0.497s\n",
            "Epoch: 263 loss_train: 0.460 acc_train: 0.797 loss_test: 0.598 acc_test: 0.686 AUC = 0.76 time: 0.497s\n",
            "Epoch: 264 loss_train: 0.471 acc_train: 0.784 loss_test: 0.572 acc_test: 0.714 AUC = 0.80 time: 0.497s\n",
            "Epoch: 265 loss_train: 0.468 acc_train: 0.789 loss_test: 0.574 acc_test: 0.707 AUC = 0.79 time: 0.497s\n",
            "Epoch: 266 loss_train: 0.478 acc_train: 0.787 loss_test: 0.592 acc_test: 0.700 AUC = 0.78 time: 0.497s\n",
            "Epoch: 267 loss_train: 0.480 acc_train: 0.785 loss_test: 0.577 acc_test: 0.704 AUC = 0.79 time: 0.497s\n",
            "Epoch: 268 loss_train: 0.464 acc_train: 0.789 loss_test: 0.581 acc_test: 0.705 AUC = 0.80 time: 0.497s\n",
            "Epoch: 269 loss_train: 0.479 acc_train: 0.787 loss_test: 0.567 acc_test: 0.705 AUC = 0.79 time: 0.496s\n",
            "Epoch: 270 loss_train: 0.449 acc_train: 0.804 loss_test: 0.556 acc_test: 0.717 AUC = 0.80 time: 0.497s\n",
            "Epoch: 271 loss_train: 0.457 acc_train: 0.798 loss_test: 0.545 acc_test: 0.723 AUC = 0.81 time: 0.497s\n",
            "Epoch: 272 loss_train: 0.468 acc_train: 0.785 loss_test: 0.559 acc_test: 0.717 AUC = 0.80 time: 0.496s\n",
            "Epoch: 273 loss_train: 0.477 acc_train: 0.792 loss_test: 0.599 acc_test: 0.688 AUC = 0.78 time: 0.497s\n",
            "Epoch: 274 loss_train: 0.468 acc_train: 0.791 loss_test: 0.560 acc_test: 0.720 AUC = 0.80 time: 0.497s\n",
            "Epoch: 275 loss_train: 0.468 acc_train: 0.792 loss_test: 0.588 acc_test: 0.707 AUC = 0.78 time: 0.497s\n",
            "Epoch: 276 loss_train: 0.479 acc_train: 0.780 loss_test: 0.582 acc_test: 0.694 AUC = 0.79 time: 0.496s\n",
            "Epoch: 277 loss_train: 0.452 acc_train: 0.799 loss_test: 0.564 acc_test: 0.704 AUC = 0.81 time: 0.497s\n",
            "Epoch: 278 loss_train: 0.449 acc_train: 0.802 loss_test: 0.572 acc_test: 0.695 AUC = 0.79 time: 0.497s\n",
            "Epoch: 279 loss_train: 0.460 acc_train: 0.792 loss_test: 0.569 acc_test: 0.703 AUC = 0.80 time: 0.496s\n",
            "Epoch: 280 loss_train: 0.477 acc_train: 0.781 loss_test: 0.567 acc_test: 0.710 AUC = 0.80 time: 0.497s\n",
            "Epoch: 281 loss_train: 0.485 acc_train: 0.792 loss_test: 0.569 acc_test: 0.700 AUC = 0.80 time: 0.497s\n",
            "Epoch: 282 loss_train: 0.480 acc_train: 0.792 loss_test: 0.563 acc_test: 0.717 AUC = 0.80 time: 0.497s\n",
            "Epoch: 283 loss_train: 0.462 acc_train: 0.797 loss_test: 0.581 acc_test: 0.704 AUC = 0.79 time: 0.497s\n",
            "Epoch: 284 loss_train: 0.458 acc_train: 0.796 loss_test: 0.562 acc_test: 0.716 AUC = 0.80 time: 0.497s\n",
            "Epoch: 285 loss_train: 0.456 acc_train: 0.803 loss_test: 0.565 acc_test: 0.719 AUC = 0.80 time: 0.497s\n",
            "Epoch: 286 loss_train: 0.474 acc_train: 0.787 loss_test: 0.580 acc_test: 0.697 AUC = 0.79 time: 0.497s\n",
            "Epoch: 287 loss_train: 0.453 acc_train: 0.800 loss_test: 0.551 acc_test: 0.705 AUC = 0.81 time: 0.497s\n",
            "Epoch: 288 loss_train: 0.473 acc_train: 0.791 loss_test: 0.575 acc_test: 0.704 AUC = 0.79 time: 0.497s\n",
            "Epoch: 289 loss_train: 0.460 acc_train: 0.800 loss_test: 0.566 acc_test: 0.722 AUC = 0.79 time: 0.496s\n",
            "Epoch: 290 loss_train: 0.451 acc_train: 0.797 loss_test: 0.590 acc_test: 0.703 AUC = 0.79 time: 0.497s\n",
            "Epoch: 291 loss_train: 0.450 acc_train: 0.803 loss_test: 0.576 acc_test: 0.719 AUC = 0.78 time: 0.497s\n",
            "Epoch: 292 loss_train: 0.464 acc_train: 0.794 loss_test: 0.558 acc_test: 0.719 AUC = 0.81 time: 0.496s\n",
            "Epoch: 293 loss_train: 0.447 acc_train: 0.797 loss_test: 0.558 acc_test: 0.704 AUC = 0.80 time: 0.497s\n",
            "Epoch: 294 loss_train: 0.460 acc_train: 0.795 loss_test: 0.544 acc_test: 0.722 AUC = 0.82 time: 0.497s\n",
            "Epoch: 295 loss_train: 0.457 acc_train: 0.798 loss_test: 0.560 acc_test: 0.703 AUC = 0.80 time: 0.497s\n",
            "Epoch: 296 loss_train: 0.451 acc_train: 0.794 loss_test: 0.580 acc_test: 0.710 AUC = 0.79 time: 0.497s\n",
            "Epoch: 297 loss_train: 0.458 acc_train: 0.801 loss_test: 0.571 acc_test: 0.705 AUC = 0.79 time: 0.497s\n",
            "Epoch: 298 loss_train: 0.459 acc_train: 0.792 loss_test: 0.570 acc_test: 0.708 AUC = 0.80 time: 0.497s\n",
            "Epoch: 299 loss_train: 0.454 acc_train: 0.796 loss_test: 0.570 acc_test: 0.722 AUC = 0.79 time: 0.497s\n",
            "Epoch: 300 loss_train: 0.442 acc_train: 0.802 loss_test: 0.559 acc_test: 0.705 AUC = 0.80 time: 0.496s\n",
            "Epoch: 301 loss_train: 0.457 acc_train: 0.801 loss_test: 0.551 acc_test: 0.725 AUC = 0.80 time: 0.497s\n",
            "Epoch: 302 loss_train: 0.441 acc_train: 0.804 loss_test: 0.565 acc_test: 0.714 AUC = 0.79 time: 0.497s\n",
            "Epoch: 303 loss_train: 0.454 acc_train: 0.802 loss_test: 0.576 acc_test: 0.719 AUC = 0.79 time: 0.497s\n",
            "Epoch: 304 loss_train: 0.466 acc_train: 0.792 loss_test: 0.556 acc_test: 0.703 AUC = 0.81 time: 0.497s\n",
            "Epoch: 305 loss_train: 0.454 acc_train: 0.805 loss_test: 0.547 acc_test: 0.725 AUC = 0.81 time: 0.497s\n",
            "Epoch: 306 loss_train: 0.463 acc_train: 0.793 loss_test: 0.574 acc_test: 0.710 AUC = 0.79 time: 0.497s\n",
            "Epoch: 307 loss_train: 0.450 acc_train: 0.801 loss_test: 0.539 acc_test: 0.720 AUC = 0.82 time: 0.497s\n",
            "Epoch: 308 loss_train: 0.465 acc_train: 0.792 loss_test: 0.570 acc_test: 0.719 AUC = 0.79 time: 0.497s\n",
            "Epoch: 309 loss_train: 0.467 acc_train: 0.787 loss_test: 0.582 acc_test: 0.695 AUC = 0.79 time: 0.497s\n",
            "Epoch: 310 loss_train: 0.458 acc_train: 0.791 loss_test: 0.573 acc_test: 0.700 AUC = 0.79 time: 0.497s\n",
            "Epoch: 311 loss_train: 0.467 acc_train: 0.785 loss_test: 0.551 acc_test: 0.730 AUC = 0.81 time: 0.497s\n",
            "Epoch: 312 loss_train: 0.463 acc_train: 0.786 loss_test: 0.562 acc_test: 0.723 AUC = 0.81 time: 0.497s\n",
            "Epoch: 313 loss_train: 0.456 acc_train: 0.796 loss_test: 0.578 acc_test: 0.719 AUC = 0.78 time: 0.497s\n",
            "Epoch: 314 loss_train: 0.464 acc_train: 0.794 loss_test: 0.582 acc_test: 0.704 AUC = 0.79 time: 0.497s\n",
            "Epoch: 315 loss_train: 0.455 acc_train: 0.796 loss_test: 0.517 acc_test: 0.739 AUC = 0.83 time: 0.497s\n",
            "Epoch: 316 loss_train: 0.443 acc_train: 0.801 loss_test: 0.548 acc_test: 0.716 AUC = 0.81 time: 0.497s\n",
            "Epoch: 317 loss_train: 0.451 acc_train: 0.797 loss_test: 0.552 acc_test: 0.708 AUC = 0.81 time: 0.497s\n",
            "Epoch: 318 loss_train: 0.459 acc_train: 0.794 loss_test: 0.560 acc_test: 0.717 AUC = 0.82 time: 0.496s\n",
            "Epoch: 319 loss_train: 0.462 acc_train: 0.789 loss_test: 0.543 acc_test: 0.723 AUC = 0.83 time: 0.497s\n",
            "Epoch: 320 loss_train: 0.464 acc_train: 0.795 loss_test: 0.569 acc_test: 0.720 AUC = 0.79 time: 0.497s\n",
            "Epoch: 321 loss_train: 0.456 acc_train: 0.803 loss_test: 0.558 acc_test: 0.733 AUC = 0.81 time: 0.497s\n",
            "Epoch: 322 loss_train: 0.436 acc_train: 0.813 loss_test: 0.554 acc_test: 0.716 AUC = 0.81 time: 0.497s\n",
            "Epoch: 323 loss_train: 0.446 acc_train: 0.792 loss_test: 0.530 acc_test: 0.716 AUC = 0.83 time: 0.497s\n",
            "Epoch: 324 loss_train: 0.429 acc_train: 0.813 loss_test: 0.554 acc_test: 0.723 AUC = 0.82 time: 0.497s\n",
            "Epoch: 325 loss_train: 0.461 acc_train: 0.791 loss_test: 0.557 acc_test: 0.732 AUC = 0.81 time: 0.497s\n",
            "Epoch: 326 loss_train: 0.471 acc_train: 0.787 loss_test: 0.567 acc_test: 0.711 AUC = 0.80 time: 0.497s\n",
            "Epoch: 327 loss_train: 0.465 acc_train: 0.796 loss_test: 0.561 acc_test: 0.738 AUC = 0.81 time: 0.497s\n",
            "Epoch: 328 loss_train: 0.458 acc_train: 0.792 loss_test: 0.557 acc_test: 0.730 AUC = 0.80 time: 0.497s\n",
            "Epoch: 329 loss_train: 0.455 acc_train: 0.805 loss_test: 0.560 acc_test: 0.711 AUC = 0.80 time: 0.496s\n",
            "Epoch: 330 loss_train: 0.446 acc_train: 0.811 loss_test: 0.549 acc_test: 0.739 AUC = 0.82 time: 0.497s\n",
            "Epoch: 331 loss_train: 0.456 acc_train: 0.795 loss_test: 0.539 acc_test: 0.729 AUC = 0.82 time: 0.497s\n",
            "Epoch: 332 loss_train: 0.451 acc_train: 0.787 loss_test: 0.573 acc_test: 0.708 AUC = 0.80 time: 0.497s\n",
            "Epoch: 333 loss_train: 0.444 acc_train: 0.801 loss_test: 0.551 acc_test: 0.722 AUC = 0.81 time: 0.497s\n",
            "Epoch: 334 loss_train: 0.445 acc_train: 0.799 loss_test: 0.554 acc_test: 0.726 AUC = 0.80 time: 0.497s\n",
            "Epoch: 335 loss_train: 0.452 acc_train: 0.809 loss_test: 0.583 acc_test: 0.695 AUC = 0.80 time: 0.497s\n",
            "Epoch: 336 loss_train: 0.467 acc_train: 0.799 loss_test: 0.561 acc_test: 0.701 AUC = 0.80 time: 0.498s\n",
            "Epoch: 337 loss_train: 0.451 acc_train: 0.792 loss_test: 0.551 acc_test: 0.716 AUC = 0.80 time: 0.497s\n",
            "Epoch: 338 loss_train: 0.465 acc_train: 0.796 loss_test: 0.571 acc_test: 0.708 AUC = 0.79 time: 0.497s\n",
            "Epoch: 339 loss_train: 0.454 acc_train: 0.803 loss_test: 0.548 acc_test: 0.728 AUC = 0.81 time: 0.497s\n",
            "Epoch: 340 loss_train: 0.451 acc_train: 0.796 loss_test: 0.559 acc_test: 0.704 AUC = 0.80 time: 0.497s\n",
            "Epoch: 341 loss_train: 0.451 acc_train: 0.791 loss_test: 0.527 acc_test: 0.725 AUC = 0.82 time: 0.497s\n",
            "Epoch: 342 loss_train: 0.453 acc_train: 0.806 loss_test: 0.547 acc_test: 0.728 AUC = 0.81 time: 0.497s\n",
            "Epoch: 343 loss_train: 0.463 acc_train: 0.792 loss_test: 0.590 acc_test: 0.707 AUC = 0.78 time: 0.497s\n",
            "Epoch: 344 loss_train: 0.447 acc_train: 0.801 loss_test: 0.548 acc_test: 0.726 AUC = 0.81 time: 0.498s\n",
            "Epoch: 345 loss_train: 0.456 acc_train: 0.799 loss_test: 0.549 acc_test: 0.708 AUC = 0.81 time: 0.497s\n",
            "Epoch: 346 loss_train: 0.457 acc_train: 0.801 loss_test: 0.553 acc_test: 0.726 AUC = 0.82 time: 0.497s\n",
            "Epoch: 347 loss_train: 0.444 acc_train: 0.801 loss_test: 0.583 acc_test: 0.704 AUC = 0.79 time: 0.497s\n",
            "Epoch: 348 loss_train: 0.451 acc_train: 0.807 loss_test: 0.535 acc_test: 0.723 AUC = 0.83 time: 0.497s\n",
            "Epoch: 349 loss_train: 0.457 acc_train: 0.804 loss_test: 0.556 acc_test: 0.705 AUC = 0.81 time: 0.497s\n",
            "Epoch: 350 loss_train: 0.444 acc_train: 0.803 loss_test: 0.526 acc_test: 0.741 AUC = 0.83 time: 0.497s\n",
            "Epoch: 351 loss_train: 0.449 acc_train: 0.807 loss_test: 0.555 acc_test: 0.726 AUC = 0.81 time: 0.497s\n",
            "Epoch: 352 loss_train: 0.455 acc_train: 0.797 loss_test: 0.532 acc_test: 0.733 AUC = 0.82 time: 0.497s\n",
            "Epoch: 353 loss_train: 0.444 acc_train: 0.806 loss_test: 0.532 acc_test: 0.729 AUC = 0.83 time: 0.497s\n",
            "Epoch: 354 loss_train: 0.438 acc_train: 0.807 loss_test: 0.551 acc_test: 0.716 AUC = 0.81 time: 0.497s\n",
            "Epoch: 355 loss_train: 0.448 acc_train: 0.803 loss_test: 0.542 acc_test: 0.719 AUC = 0.83 time: 0.497s\n",
            "Epoch: 356 loss_train: 0.447 acc_train: 0.812 loss_test: 0.577 acc_test: 0.735 AUC = 0.79 time: 0.497s\n",
            "Epoch: 357 loss_train: 0.454 acc_train: 0.798 loss_test: 0.567 acc_test: 0.703 AUC = 0.80 time: 0.497s\n",
            "Epoch: 358 loss_train: 0.468 acc_train: 0.787 loss_test: 0.562 acc_test: 0.728 AUC = 0.81 time: 0.497s\n",
            "Epoch: 359 loss_train: 0.452 acc_train: 0.803 loss_test: 0.567 acc_test: 0.714 AUC = 0.80 time: 0.497s\n",
            "Epoch: 360 loss_train: 0.449 acc_train: 0.799 loss_test: 0.522 acc_test: 0.748 AUC = 0.83 time: 0.497s\n",
            "Epoch: 361 loss_train: 0.434 acc_train: 0.809 loss_test: 0.540 acc_test: 0.722 AUC = 0.82 time: 0.497s\n",
            "Epoch: 362 loss_train: 0.447 acc_train: 0.793 loss_test: 0.549 acc_test: 0.720 AUC = 0.81 time: 0.497s\n",
            "Epoch: 363 loss_train: 0.462 acc_train: 0.799 loss_test: 0.541 acc_test: 0.723 AUC = 0.81 time: 0.497s\n",
            "Epoch: 364 loss_train: 0.453 acc_train: 0.801 loss_test: 0.541 acc_test: 0.733 AUC = 0.82 time: 0.497s\n",
            "Epoch: 365 loss_train: 0.441 acc_train: 0.808 loss_test: 0.572 acc_test: 0.717 AUC = 0.80 time: 0.497s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 227.6481s\n",
            "Loading 314th epoch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-c368eb4b4196>:179: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(sv_path +'{}.pkl'.format(best_epoch)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set results: loss= 0.4758 accuracy= 0.7570 AUC = 0.87 False Alarm Rate = 0.3078 F1 Score = 0.6845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-c368eb4b4196>:144: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  idx_train = torch.tensor(idx_train).cuda()\n",
            "<ipython-input-5-c368eb4b4196>:145: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  idx_test = torch.tensor(idx_test).cuda()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001 loss_train: 0.754 acc_train: 0.346 loss_test: 0.711 acc_test: 0.495 AUC = 0.57 time: 0.497s\n",
            "Epoch: 002 loss_train: 0.757 acc_train: 0.329 loss_test: 0.712 acc_test: 0.473 AUC = 0.53 time: 0.497s\n",
            "Epoch: 003 loss_train: 0.752 acc_train: 0.330 loss_test: 0.706 acc_test: 0.479 AUC = 0.55 time: 0.497s\n",
            "Epoch: 004 loss_train: 0.740 acc_train: 0.367 loss_test: 0.722 acc_test: 0.489 AUC = 0.56 time: 0.497s\n",
            "Epoch: 005 loss_train: 0.735 acc_train: 0.368 loss_test: 0.702 acc_test: 0.487 AUC = 0.56 time: 0.497s\n",
            "Epoch: 006 loss_train: 0.730 acc_train: 0.377 loss_test: 0.710 acc_test: 0.533 AUC = 0.55 time: 0.496s\n",
            "Epoch: 007 loss_train: 0.726 acc_train: 0.390 loss_test: 0.693 acc_test: 0.511 AUC = 0.57 time: 0.496s\n",
            "Epoch: 008 loss_train: 0.723 acc_train: 0.417 loss_test: 0.697 acc_test: 0.526 AUC = 0.54 time: 0.497s\n",
            "Epoch: 009 loss_train: 0.712 acc_train: 0.441 loss_test: 0.711 acc_test: 0.510 AUC = 0.54 time: 0.498s\n",
            "Epoch: 010 loss_train: 0.702 acc_train: 0.472 loss_test: 0.712 acc_test: 0.535 AUC = 0.55 time: 0.497s\n",
            "Epoch: 011 loss_train: 0.701 acc_train: 0.478 loss_test: 0.697 acc_test: 0.549 AUC = 0.57 time: 0.498s\n",
            "Epoch: 012 loss_train: 0.692 acc_train: 0.521 loss_test: 0.700 acc_test: 0.515 AUC = 0.54 time: 0.497s\n",
            "Epoch: 013 loss_train: 0.687 acc_train: 0.532 loss_test: 0.700 acc_test: 0.511 AUC = 0.51 time: 0.497s\n",
            "Epoch: 014 loss_train: 0.677 acc_train: 0.570 loss_test: 0.692 acc_test: 0.536 AUC = 0.54 time: 0.499s\n",
            "Epoch: 015 loss_train: 0.673 acc_train: 0.574 loss_test: 0.691 acc_test: 0.548 AUC = 0.55 time: 0.497s\n",
            "Epoch: 016 loss_train: 0.667 acc_train: 0.606 loss_test: 0.719 acc_test: 0.557 AUC = 0.56 time: 0.497s\n",
            "Epoch: 017 loss_train: 0.657 acc_train: 0.632 loss_test: 0.706 acc_test: 0.560 AUC = 0.56 time: 0.497s\n",
            "Epoch: 018 loss_train: 0.658 acc_train: 0.649 loss_test: 0.710 acc_test: 0.573 AUC = 0.61 time: 0.497s\n",
            "Epoch: 019 loss_train: 0.658 acc_train: 0.652 loss_test: 0.723 acc_test: 0.585 AUC = 0.59 time: 0.497s\n",
            "Epoch: 020 loss_train: 0.649 acc_train: 0.651 loss_test: 0.695 acc_test: 0.538 AUC = 0.58 time: 0.497s\n",
            "Epoch: 021 loss_train: 0.639 acc_train: 0.681 loss_test: 0.728 acc_test: 0.573 AUC = 0.59 time: 0.497s\n",
            "Epoch: 022 loss_train: 0.637 acc_train: 0.671 loss_test: 0.744 acc_test: 0.549 AUC = 0.57 time: 0.497s\n",
            "Epoch: 023 loss_train: 0.650 acc_train: 0.671 loss_test: 0.749 acc_test: 0.561 AUC = 0.56 time: 0.497s\n",
            "Epoch: 024 loss_train: 0.631 acc_train: 0.686 loss_test: 0.730 acc_test: 0.551 AUC = 0.57 time: 0.497s\n",
            "Epoch: 025 loss_train: 0.633 acc_train: 0.688 loss_test: 0.741 acc_test: 0.568 AUC = 0.59 time: 0.497s\n",
            "Epoch: 026 loss_train: 0.632 acc_train: 0.692 loss_test: 0.720 acc_test: 0.560 AUC = 0.60 time: 0.497s\n",
            "Epoch: 027 loss_train: 0.611 acc_train: 0.697 loss_test: 0.706 acc_test: 0.567 AUC = 0.61 time: 0.497s\n",
            "Epoch: 028 loss_train: 0.608 acc_train: 0.710 loss_test: 0.714 acc_test: 0.583 AUC = 0.63 time: 0.498s\n",
            "Epoch: 029 loss_train: 0.599 acc_train: 0.705 loss_test: 0.711 acc_test: 0.558 AUC = 0.61 time: 0.497s\n",
            "Epoch: 030 loss_train: 0.612 acc_train: 0.704 loss_test: 0.730 acc_test: 0.583 AUC = 0.62 time: 0.496s\n",
            "Epoch: 031 loss_train: 0.611 acc_train: 0.712 loss_test: 0.700 acc_test: 0.582 AUC = 0.64 time: 0.497s\n",
            "Epoch: 032 loss_train: 0.616 acc_train: 0.704 loss_test: 0.723 acc_test: 0.585 AUC = 0.63 time: 0.497s\n",
            "Epoch: 033 loss_train: 0.595 acc_train: 0.709 loss_test: 0.699 acc_test: 0.592 AUC = 0.64 time: 0.497s\n",
            "Epoch: 034 loss_train: 0.596 acc_train: 0.711 loss_test: 0.674 acc_test: 0.602 AUC = 0.65 time: 0.497s\n",
            "Epoch: 035 loss_train: 0.589 acc_train: 0.721 loss_test: 0.689 acc_test: 0.599 AUC = 0.66 time: 0.497s\n",
            "Epoch: 036 loss_train: 0.578 acc_train: 0.726 loss_test: 0.689 acc_test: 0.583 AUC = 0.63 time: 0.497s\n",
            "Epoch: 037 loss_train: 0.574 acc_train: 0.727 loss_test: 0.673 acc_test: 0.617 AUC = 0.66 time: 0.496s\n",
            "Epoch: 038 loss_train: 0.579 acc_train: 0.734 loss_test: 0.702 acc_test: 0.604 AUC = 0.64 time: 0.497s\n",
            "Epoch: 039 loss_train: 0.579 acc_train: 0.720 loss_test: 0.657 acc_test: 0.622 AUC = 0.68 time: 0.497s\n",
            "Epoch: 040 loss_train: 0.582 acc_train: 0.726 loss_test: 0.659 acc_test: 0.624 AUC = 0.67 time: 0.497s\n",
            "Epoch: 041 loss_train: 0.574 acc_train: 0.728 loss_test: 0.686 acc_test: 0.636 AUC = 0.65 time: 0.497s\n",
            "Epoch: 042 loss_train: 0.570 acc_train: 0.739 loss_test: 0.703 acc_test: 0.613 AUC = 0.66 time: 0.497s\n",
            "Epoch: 043 loss_train: 0.556 acc_train: 0.736 loss_test: 0.623 acc_test: 0.642 AUC = 0.74 time: 0.497s\n",
            "Epoch: 044 loss_train: 0.567 acc_train: 0.734 loss_test: 0.644 acc_test: 0.642 AUC = 0.69 time: 0.497s\n",
            "Epoch: 045 loss_train: 0.560 acc_train: 0.744 loss_test: 0.670 acc_test: 0.626 AUC = 0.67 time: 0.497s\n",
            "Epoch: 046 loss_train: 0.566 acc_train: 0.735 loss_test: 0.644 acc_test: 0.655 AUC = 0.69 time: 0.497s\n",
            "Epoch: 047 loss_train: 0.563 acc_train: 0.738 loss_test: 0.675 acc_test: 0.622 AUC = 0.67 time: 0.496s\n",
            "Epoch: 048 loss_train: 0.576 acc_train: 0.726 loss_test: 0.687 acc_test: 0.622 AUC = 0.66 time: 0.497s\n",
            "Epoch: 049 loss_train: 0.566 acc_train: 0.730 loss_test: 0.670 acc_test: 0.613 AUC = 0.68 time: 0.497s\n",
            "Epoch: 050 loss_train: 0.566 acc_train: 0.739 loss_test: 0.642 acc_test: 0.679 AUC = 0.70 time: 0.497s\n",
            "Epoch: 051 loss_train: 0.561 acc_train: 0.744 loss_test: 0.680 acc_test: 0.624 AUC = 0.66 time: 0.497s\n",
            "Epoch: 052 loss_train: 0.553 acc_train: 0.752 loss_test: 0.687 acc_test: 0.644 AUC = 0.68 time: 0.497s\n",
            "Epoch: 053 loss_train: 0.553 acc_train: 0.756 loss_test: 0.709 acc_test: 0.630 AUC = 0.67 time: 0.497s\n",
            "Epoch: 054 loss_train: 0.545 acc_train: 0.758 loss_test: 0.678 acc_test: 0.632 AUC = 0.69 time: 0.499s\n",
            "Epoch: 055 loss_train: 0.549 acc_train: 0.748 loss_test: 0.686 acc_test: 0.649 AUC = 0.67 time: 0.497s\n",
            "Epoch: 056 loss_train: 0.548 acc_train: 0.751 loss_test: 0.690 acc_test: 0.629 AUC = 0.70 time: 0.499s\n",
            "Epoch: 057 loss_train: 0.559 acc_train: 0.744 loss_test: 0.694 acc_test: 0.635 AUC = 0.67 time: 0.496s\n",
            "Epoch: 058 loss_train: 0.557 acc_train: 0.744 loss_test: 0.663 acc_test: 0.644 AUC = 0.69 time: 0.496s\n",
            "Epoch: 059 loss_train: 0.560 acc_train: 0.749 loss_test: 0.709 acc_test: 0.649 AUC = 0.68 time: 0.497s\n",
            "Epoch: 060 loss_train: 0.539 acc_train: 0.758 loss_test: 0.659 acc_test: 0.661 AUC = 0.71 time: 0.496s\n",
            "Epoch: 061 loss_train: 0.562 acc_train: 0.746 loss_test: 0.684 acc_test: 0.644 AUC = 0.69 time: 0.497s\n",
            "Epoch: 062 loss_train: 0.543 acc_train: 0.756 loss_test: 0.664 acc_test: 0.639 AUC = 0.70 time: 0.497s\n",
            "Epoch: 063 loss_train: 0.529 acc_train: 0.760 loss_test: 0.668 acc_test: 0.639 AUC = 0.71 time: 0.497s\n",
            "Epoch: 064 loss_train: 0.562 acc_train: 0.744 loss_test: 0.696 acc_test: 0.660 AUC = 0.68 time: 0.497s\n",
            "Epoch: 065 loss_train: 0.543 acc_train: 0.745 loss_test: 0.668 acc_test: 0.658 AUC = 0.69 time: 0.497s\n",
            "Epoch: 066 loss_train: 0.542 acc_train: 0.757 loss_test: 0.673 acc_test: 0.645 AUC = 0.68 time: 0.497s\n",
            "Epoch: 067 loss_train: 0.537 acc_train: 0.738 loss_test: 0.692 acc_test: 0.629 AUC = 0.67 time: 0.497s\n",
            "Epoch: 068 loss_train: 0.544 acc_train: 0.748 loss_test: 0.644 acc_test: 0.658 AUC = 0.71 time: 0.497s\n",
            "Epoch: 069 loss_train: 0.538 acc_train: 0.764 loss_test: 0.679 acc_test: 0.629 AUC = 0.69 time: 0.497s\n",
            "Epoch: 070 loss_train: 0.543 acc_train: 0.755 loss_test: 0.625 acc_test: 0.672 AUC = 0.73 time: 0.497s\n",
            "Epoch: 071 loss_train: 0.531 acc_train: 0.754 loss_test: 0.682 acc_test: 0.629 AUC = 0.68 time: 0.497s\n",
            "Epoch: 072 loss_train: 0.544 acc_train: 0.753 loss_test: 0.650 acc_test: 0.636 AUC = 0.70 time: 0.497s\n",
            "Epoch: 073 loss_train: 0.542 acc_train: 0.755 loss_test: 0.651 acc_test: 0.648 AUC = 0.72 time: 0.497s\n",
            "Epoch: 074 loss_train: 0.531 acc_train: 0.755 loss_test: 0.607 acc_test: 0.667 AUC = 0.76 time: 0.497s\n",
            "Epoch: 075 loss_train: 0.536 acc_train: 0.759 loss_test: 0.678 acc_test: 0.657 AUC = 0.72 time: 0.497s\n",
            "Epoch: 076 loss_train: 0.525 acc_train: 0.756 loss_test: 0.641 acc_test: 0.652 AUC = 0.72 time: 0.497s\n",
            "Epoch: 077 loss_train: 0.541 acc_train: 0.750 loss_test: 0.630 acc_test: 0.664 AUC = 0.73 time: 0.497s\n",
            "Epoch: 078 loss_train: 0.555 acc_train: 0.750 loss_test: 0.664 acc_test: 0.669 AUC = 0.72 time: 0.496s\n",
            "Epoch: 079 loss_train: 0.522 acc_train: 0.768 loss_test: 0.634 acc_test: 0.664 AUC = 0.73 time: 0.497s\n",
            "Epoch: 080 loss_train: 0.522 acc_train: 0.760 loss_test: 0.643 acc_test: 0.657 AUC = 0.72 time: 0.497s\n",
            "Epoch: 081 loss_train: 0.539 acc_train: 0.754 loss_test: 0.647 acc_test: 0.645 AUC = 0.71 time: 0.497s\n",
            "Epoch: 082 loss_train: 0.515 acc_train: 0.768 loss_test: 0.645 acc_test: 0.645 AUC = 0.73 time: 0.497s\n",
            "Epoch: 083 loss_train: 0.531 acc_train: 0.760 loss_test: 0.617 acc_test: 0.666 AUC = 0.74 time: 0.496s\n",
            "Epoch: 084 loss_train: 0.519 acc_train: 0.762 loss_test: 0.641 acc_test: 0.657 AUC = 0.72 time: 0.497s\n",
            "Epoch: 085 loss_train: 0.529 acc_train: 0.756 loss_test: 0.648 acc_test: 0.654 AUC = 0.73 time: 0.497s\n",
            "Epoch: 086 loss_train: 0.517 acc_train: 0.766 loss_test: 0.635 acc_test: 0.661 AUC = 0.73 time: 0.497s\n",
            "Epoch: 087 loss_train: 0.539 acc_train: 0.744 loss_test: 0.632 acc_test: 0.675 AUC = 0.73 time: 0.497s\n",
            "Epoch: 088 loss_train: 0.518 acc_train: 0.755 loss_test: 0.631 acc_test: 0.666 AUC = 0.74 time: 0.497s\n",
            "Epoch: 089 loss_train: 0.518 acc_train: 0.764 loss_test: 0.633 acc_test: 0.667 AUC = 0.73 time: 0.497s\n",
            "Epoch: 090 loss_train: 0.540 acc_train: 0.753 loss_test: 0.638 acc_test: 0.664 AUC = 0.73 time: 0.497s\n",
            "Epoch: 091 loss_train: 0.527 acc_train: 0.759 loss_test: 0.644 acc_test: 0.667 AUC = 0.72 time: 0.497s\n",
            "Epoch: 092 loss_train: 0.521 acc_train: 0.756 loss_test: 0.614 acc_test: 0.688 AUC = 0.75 time: 0.497s\n",
            "Epoch: 093 loss_train: 0.526 acc_train: 0.768 loss_test: 0.615 acc_test: 0.669 AUC = 0.75 time: 0.497s\n",
            "Epoch: 094 loss_train: 0.520 acc_train: 0.762 loss_test: 0.609 acc_test: 0.685 AUC = 0.76 time: 0.497s\n",
            "Epoch: 095 loss_train: 0.538 acc_train: 0.756 loss_test: 0.628 acc_test: 0.663 AUC = 0.75 time: 0.498s\n",
            "Epoch: 096 loss_train: 0.499 acc_train: 0.778 loss_test: 0.638 acc_test: 0.669 AUC = 0.74 time: 0.497s\n",
            "Epoch: 097 loss_train: 0.538 acc_train: 0.753 loss_test: 0.660 acc_test: 0.667 AUC = 0.74 time: 0.497s\n",
            "Epoch: 098 loss_train: 0.522 acc_train: 0.774 loss_test: 0.653 acc_test: 0.675 AUC = 0.73 time: 0.496s\n",
            "Epoch: 099 loss_train: 0.517 acc_train: 0.765 loss_test: 0.629 acc_test: 0.672 AUC = 0.73 time: 0.497s\n",
            "Epoch: 100 loss_train: 0.514 acc_train: 0.773 loss_test: 0.611 acc_test: 0.666 AUC = 0.76 time: 0.497s\n",
            "Epoch: 101 loss_train: 0.524 acc_train: 0.758 loss_test: 0.613 acc_test: 0.679 AUC = 0.76 time: 0.496s\n",
            "Epoch: 102 loss_train: 0.517 acc_train: 0.769 loss_test: 0.628 acc_test: 0.680 AUC = 0.75 time: 0.496s\n",
            "Epoch: 103 loss_train: 0.534 acc_train: 0.761 loss_test: 0.618 acc_test: 0.660 AUC = 0.74 time: 0.497s\n",
            "Epoch: 104 loss_train: 0.523 acc_train: 0.763 loss_test: 0.647 acc_test: 0.649 AUC = 0.73 time: 0.497s\n",
            "Epoch: 105 loss_train: 0.512 acc_train: 0.767 loss_test: 0.624 acc_test: 0.651 AUC = 0.75 time: 0.497s\n",
            "Epoch: 106 loss_train: 0.514 acc_train: 0.774 loss_test: 0.618 acc_test: 0.657 AUC = 0.74 time: 0.497s\n",
            "Epoch: 107 loss_train: 0.528 acc_train: 0.762 loss_test: 0.635 acc_test: 0.676 AUC = 0.74 time: 0.497s\n",
            "Epoch: 108 loss_train: 0.510 acc_train: 0.768 loss_test: 0.629 acc_test: 0.670 AUC = 0.74 time: 0.497s\n",
            "Epoch: 109 loss_train: 0.506 acc_train: 0.778 loss_test: 0.628 acc_test: 0.688 AUC = 0.74 time: 0.497s\n",
            "Epoch: 110 loss_train: 0.516 acc_train: 0.769 loss_test: 0.608 acc_test: 0.679 AUC = 0.76 time: 0.497s\n",
            "Epoch: 111 loss_train: 0.514 acc_train: 0.767 loss_test: 0.601 acc_test: 0.697 AUC = 0.76 time: 0.497s\n",
            "Epoch: 112 loss_train: 0.519 acc_train: 0.764 loss_test: 0.627 acc_test: 0.670 AUC = 0.74 time: 0.497s\n",
            "Epoch: 113 loss_train: 0.506 acc_train: 0.772 loss_test: 0.637 acc_test: 0.667 AUC = 0.73 time: 0.497s\n",
            "Epoch: 114 loss_train: 0.503 acc_train: 0.782 loss_test: 0.637 acc_test: 0.670 AUC = 0.73 time: 0.497s\n",
            "Epoch: 115 loss_train: 0.503 acc_train: 0.774 loss_test: 0.612 acc_test: 0.676 AUC = 0.75 time: 0.497s\n",
            "Epoch: 116 loss_train: 0.524 acc_train: 0.767 loss_test: 0.689 acc_test: 0.667 AUC = 0.71 time: 0.497s\n",
            "Epoch: 117 loss_train: 0.515 acc_train: 0.763 loss_test: 0.634 acc_test: 0.669 AUC = 0.74 time: 0.497s\n",
            "Epoch: 118 loss_train: 0.489 acc_train: 0.775 loss_test: 0.607 acc_test: 0.679 AUC = 0.77 time: 0.497s\n",
            "Epoch: 119 loss_train: 0.513 acc_train: 0.775 loss_test: 0.620 acc_test: 0.667 AUC = 0.75 time: 0.497s\n",
            "Epoch: 120 loss_train: 0.523 acc_train: 0.770 loss_test: 0.667 acc_test: 0.680 AUC = 0.74 time: 0.498s\n",
            "Epoch: 121 loss_train: 0.503 acc_train: 0.767 loss_test: 0.623 acc_test: 0.673 AUC = 0.75 time: 0.497s\n",
            "Epoch: 122 loss_train: 0.504 acc_train: 0.772 loss_test: 0.623 acc_test: 0.657 AUC = 0.74 time: 0.497s\n",
            "Epoch: 123 loss_train: 0.510 acc_train: 0.772 loss_test: 0.608 acc_test: 0.686 AUC = 0.75 time: 0.497s\n",
            "Epoch: 124 loss_train: 0.517 acc_train: 0.767 loss_test: 0.614 acc_test: 0.667 AUC = 0.75 time: 0.497s\n",
            "Epoch: 125 loss_train: 0.509 acc_train: 0.767 loss_test: 0.632 acc_test: 0.679 AUC = 0.74 time: 0.497s\n",
            "Epoch: 126 loss_train: 0.516 acc_train: 0.772 loss_test: 0.609 acc_test: 0.677 AUC = 0.76 time: 0.497s\n",
            "Epoch: 127 loss_train: 0.505 acc_train: 0.770 loss_test: 0.654 acc_test: 0.675 AUC = 0.73 time: 0.498s\n",
            "Epoch: 128 loss_train: 0.510 acc_train: 0.767 loss_test: 0.622 acc_test: 0.651 AUC = 0.75 time: 0.497s\n",
            "Epoch: 129 loss_train: 0.502 acc_train: 0.775 loss_test: 0.645 acc_test: 0.670 AUC = 0.74 time: 0.497s\n",
            "Epoch: 130 loss_train: 0.508 acc_train: 0.776 loss_test: 0.595 acc_test: 0.680 AUC = 0.77 time: 0.497s\n",
            "Epoch: 131 loss_train: 0.500 acc_train: 0.779 loss_test: 0.608 acc_test: 0.676 AUC = 0.75 time: 0.497s\n",
            "Epoch: 132 loss_train: 0.501 acc_train: 0.772 loss_test: 0.631 acc_test: 0.679 AUC = 0.75 time: 0.497s\n",
            "Epoch: 133 loss_train: 0.510 acc_train: 0.772 loss_test: 0.594 acc_test: 0.688 AUC = 0.77 time: 0.497s\n",
            "Epoch: 134 loss_train: 0.503 acc_train: 0.766 loss_test: 0.630 acc_test: 0.673 AUC = 0.74 time: 0.496s\n",
            "Epoch: 135 loss_train: 0.504 acc_train: 0.777 loss_test: 0.608 acc_test: 0.680 AUC = 0.75 time: 0.497s\n",
            "Epoch: 136 loss_train: 0.509 acc_train: 0.770 loss_test: 0.607 acc_test: 0.694 AUC = 0.75 time: 0.497s\n",
            "Epoch: 137 loss_train: 0.501 acc_train: 0.770 loss_test: 0.585 acc_test: 0.694 AUC = 0.78 time: 0.497s\n",
            "Epoch: 138 loss_train: 0.512 acc_train: 0.772 loss_test: 0.647 acc_test: 0.680 AUC = 0.74 time: 0.497s\n",
            "Epoch: 139 loss_train: 0.507 acc_train: 0.779 loss_test: 0.642 acc_test: 0.672 AUC = 0.74 time: 0.497s\n",
            "Epoch: 140 loss_train: 0.505 acc_train: 0.770 loss_test: 0.677 acc_test: 0.677 AUC = 0.72 time: 0.497s\n",
            "Epoch: 141 loss_train: 0.508 acc_train: 0.770 loss_test: 0.611 acc_test: 0.670 AUC = 0.76 time: 0.497s\n",
            "Epoch: 142 loss_train: 0.494 acc_train: 0.780 loss_test: 0.622 acc_test: 0.669 AUC = 0.74 time: 0.497s\n",
            "Epoch: 143 loss_train: 0.508 acc_train: 0.771 loss_test: 0.628 acc_test: 0.667 AUC = 0.75 time: 0.497s\n",
            "Epoch: 144 loss_train: 0.505 acc_train: 0.770 loss_test: 0.600 acc_test: 0.677 AUC = 0.76 time: 0.497s\n",
            "Epoch: 145 loss_train: 0.503 acc_train: 0.772 loss_test: 0.666 acc_test: 0.669 AUC = 0.75 time: 0.497s\n",
            "Epoch: 146 loss_train: 0.510 acc_train: 0.772 loss_test: 0.650 acc_test: 0.661 AUC = 0.75 time: 0.497s\n",
            "Epoch: 147 loss_train: 0.498 acc_train: 0.768 loss_test: 0.618 acc_test: 0.694 AUC = 0.77 time: 0.497s\n",
            "Epoch: 148 loss_train: 0.502 acc_train: 0.766 loss_test: 0.616 acc_test: 0.677 AUC = 0.76 time: 0.497s\n",
            "Epoch: 149 loss_train: 0.516 acc_train: 0.769 loss_test: 0.642 acc_test: 0.683 AUC = 0.75 time: 0.497s\n",
            "Epoch: 150 loss_train: 0.501 acc_train: 0.774 loss_test: 0.625 acc_test: 0.679 AUC = 0.76 time: 0.497s\n",
            "Epoch: 151 loss_train: 0.511 acc_train: 0.768 loss_test: 0.578 acc_test: 0.701 AUC = 0.78 time: 0.497s\n",
            "Epoch: 152 loss_train: 0.503 acc_train: 0.775 loss_test: 0.624 acc_test: 0.683 AUC = 0.74 time: 0.497s\n",
            "Epoch: 153 loss_train: 0.500 acc_train: 0.782 loss_test: 0.610 acc_test: 0.688 AUC = 0.75 time: 0.497s\n",
            "Epoch: 154 loss_train: 0.507 acc_train: 0.775 loss_test: 0.605 acc_test: 0.672 AUC = 0.76 time: 0.497s\n",
            "Epoch: 155 loss_train: 0.512 acc_train: 0.772 loss_test: 0.584 acc_test: 0.694 AUC = 0.78 time: 0.497s\n",
            "Epoch: 156 loss_train: 0.502 acc_train: 0.782 loss_test: 0.595 acc_test: 0.685 AUC = 0.76 time: 0.498s\n",
            "Epoch: 157 loss_train: 0.509 acc_train: 0.768 loss_test: 0.617 acc_test: 0.664 AUC = 0.74 time: 0.497s\n",
            "Epoch: 158 loss_train: 0.500 acc_train: 0.776 loss_test: 0.622 acc_test: 0.677 AUC = 0.76 time: 0.497s\n",
            "Epoch: 159 loss_train: 0.495 acc_train: 0.781 loss_test: 0.590 acc_test: 0.703 AUC = 0.77 time: 0.496s\n",
            "Epoch: 160 loss_train: 0.500 acc_train: 0.778 loss_test: 0.579 acc_test: 0.716 AUC = 0.78 time: 0.496s\n",
            "Epoch: 161 loss_train: 0.499 acc_train: 0.772 loss_test: 0.667 acc_test: 0.669 AUC = 0.75 time: 0.497s\n",
            "Epoch: 162 loss_train: 0.493 acc_train: 0.777 loss_test: 0.581 acc_test: 0.685 AUC = 0.77 time: 0.497s\n",
            "Epoch: 163 loss_train: 0.487 acc_train: 0.778 loss_test: 0.584 acc_test: 0.677 AUC = 0.78 time: 0.497s\n",
            "Epoch: 164 loss_train: 0.496 acc_train: 0.774 loss_test: 0.617 acc_test: 0.672 AUC = 0.75 time: 0.497s\n",
            "Epoch: 165 loss_train: 0.497 acc_train: 0.775 loss_test: 0.570 acc_test: 0.704 AUC = 0.79 time: 0.497s\n",
            "Epoch: 166 loss_train: 0.504 acc_train: 0.770 loss_test: 0.608 acc_test: 0.694 AUC = 0.76 time: 0.496s\n",
            "Epoch: 167 loss_train: 0.498 acc_train: 0.773 loss_test: 0.590 acc_test: 0.703 AUC = 0.77 time: 0.497s\n",
            "Epoch: 168 loss_train: 0.492 acc_train: 0.778 loss_test: 0.598 acc_test: 0.692 AUC = 0.77 time: 0.497s\n",
            "Epoch: 169 loss_train: 0.493 acc_train: 0.785 loss_test: 0.615 acc_test: 0.679 AUC = 0.75 time: 0.497s\n",
            "Epoch: 170 loss_train: 0.496 acc_train: 0.780 loss_test: 0.610 acc_test: 0.677 AUC = 0.76 time: 0.497s\n",
            "Epoch: 171 loss_train: 0.487 acc_train: 0.782 loss_test: 0.584 acc_test: 0.722 AUC = 0.78 time: 0.497s\n",
            "Epoch: 172 loss_train: 0.494 acc_train: 0.774 loss_test: 0.630 acc_test: 0.670 AUC = 0.74 time: 0.497s\n",
            "Epoch: 173 loss_train: 0.488 acc_train: 0.782 loss_test: 0.594 acc_test: 0.675 AUC = 0.78 time: 0.497s\n",
            "Epoch: 174 loss_train: 0.493 acc_train: 0.770 loss_test: 0.592 acc_test: 0.707 AUC = 0.77 time: 0.497s\n",
            "Epoch: 175 loss_train: 0.494 acc_train: 0.775 loss_test: 0.604 acc_test: 0.708 AUC = 0.77 time: 0.497s\n",
            "Epoch: 176 loss_train: 0.500 acc_train: 0.768 loss_test: 0.588 acc_test: 0.688 AUC = 0.78 time: 0.496s\n",
            "Epoch: 177 loss_train: 0.480 acc_train: 0.789 loss_test: 0.589 acc_test: 0.688 AUC = 0.78 time: 0.496s\n",
            "Epoch: 178 loss_train: 0.494 acc_train: 0.770 loss_test: 0.569 acc_test: 0.695 AUC = 0.80 time: 0.497s\n",
            "Epoch: 179 loss_train: 0.501 acc_train: 0.770 loss_test: 0.647 acc_test: 0.691 AUC = 0.77 time: 0.497s\n",
            "Epoch: 180 loss_train: 0.501 acc_train: 0.762 loss_test: 0.639 acc_test: 0.695 AUC = 0.76 time: 0.497s\n",
            "Epoch: 181 loss_train: 0.492 acc_train: 0.780 loss_test: 0.610 acc_test: 0.666 AUC = 0.76 time: 0.497s\n",
            "Epoch: 182 loss_train: 0.491 acc_train: 0.775 loss_test: 0.578 acc_test: 0.719 AUC = 0.79 time: 0.497s\n",
            "Epoch: 183 loss_train: 0.493 acc_train: 0.774 loss_test: 0.598 acc_test: 0.691 AUC = 0.77 time: 0.497s\n",
            "Epoch: 184 loss_train: 0.504 acc_train: 0.779 loss_test: 0.569 acc_test: 0.713 AUC = 0.79 time: 0.497s\n",
            "Epoch: 185 loss_train: 0.486 acc_train: 0.785 loss_test: 0.608 acc_test: 0.683 AUC = 0.77 time: 0.497s\n",
            "Epoch: 186 loss_train: 0.503 acc_train: 0.774 loss_test: 0.602 acc_test: 0.685 AUC = 0.78 time: 0.497s\n",
            "Epoch: 187 loss_train: 0.495 acc_train: 0.777 loss_test: 0.578 acc_test: 0.708 AUC = 0.78 time: 0.497s\n",
            "Epoch: 188 loss_train: 0.504 acc_train: 0.775 loss_test: 0.597 acc_test: 0.703 AUC = 0.77 time: 0.497s\n",
            "Epoch: 189 loss_train: 0.483 acc_train: 0.784 loss_test: 0.595 acc_test: 0.700 AUC = 0.76 time: 0.497s\n",
            "Epoch: 190 loss_train: 0.480 acc_train: 0.782 loss_test: 0.596 acc_test: 0.695 AUC = 0.76 time: 0.497s\n",
            "Epoch: 191 loss_train: 0.482 acc_train: 0.783 loss_test: 0.605 acc_test: 0.688 AUC = 0.76 time: 0.497s\n",
            "Epoch: 192 loss_train: 0.494 acc_train: 0.781 loss_test: 0.634 acc_test: 0.691 AUC = 0.74 time: 0.497s\n",
            "Epoch: 193 loss_train: 0.483 acc_train: 0.787 loss_test: 0.622 acc_test: 0.673 AUC = 0.74 time: 0.496s\n",
            "Epoch: 194 loss_train: 0.484 acc_train: 0.786 loss_test: 0.573 acc_test: 0.726 AUC = 0.79 time: 0.497s\n",
            "Epoch: 195 loss_train: 0.494 acc_train: 0.777 loss_test: 0.577 acc_test: 0.705 AUC = 0.79 time: 0.497s\n",
            "Epoch: 196 loss_train: 0.503 acc_train: 0.775 loss_test: 0.596 acc_test: 0.689 AUC = 0.78 time: 0.496s\n",
            "Epoch: 197 loss_train: 0.493 acc_train: 0.774 loss_test: 0.625 acc_test: 0.697 AUC = 0.77 time: 0.496s\n",
            "Epoch: 198 loss_train: 0.477 acc_train: 0.790 loss_test: 0.574 acc_test: 0.707 AUC = 0.79 time: 0.497s\n",
            "Epoch: 199 loss_train: 0.471 acc_train: 0.785 loss_test: 0.563 acc_test: 0.704 AUC = 0.80 time: 0.497s\n",
            "Epoch: 200 loss_train: 0.496 acc_train: 0.784 loss_test: 0.566 acc_test: 0.711 AUC = 0.79 time: 0.496s\n",
            "Epoch: 201 loss_train: 0.500 acc_train: 0.774 loss_test: 0.590 acc_test: 0.717 AUC = 0.79 time: 0.497s\n",
            "Epoch: 202 loss_train: 0.505 acc_train: 0.763 loss_test: 0.617 acc_test: 0.683 AUC = 0.77 time: 0.498s\n",
            "Epoch: 203 loss_train: 0.474 acc_train: 0.787 loss_test: 0.607 acc_test: 0.692 AUC = 0.77 time: 0.497s\n",
            "Epoch: 204 loss_train: 0.490 acc_train: 0.786 loss_test: 0.583 acc_test: 0.680 AUC = 0.78 time: 0.498s\n",
            "Epoch: 205 loss_train: 0.492 acc_train: 0.775 loss_test: 0.587 acc_test: 0.691 AUC = 0.78 time: 0.497s\n",
            "Epoch: 206 loss_train: 0.499 acc_train: 0.777 loss_test: 0.648 acc_test: 0.683 AUC = 0.75 time: 0.497s\n",
            "Epoch: 207 loss_train: 0.478 acc_train: 0.784 loss_test: 0.588 acc_test: 0.692 AUC = 0.77 time: 0.497s\n",
            "Epoch: 208 loss_train: 0.491 acc_train: 0.787 loss_test: 0.581 acc_test: 0.689 AUC = 0.78 time: 0.497s\n",
            "Epoch: 209 loss_train: 0.487 acc_train: 0.779 loss_test: 0.581 acc_test: 0.711 AUC = 0.78 time: 0.497s\n",
            "Epoch: 210 loss_train: 0.494 acc_train: 0.785 loss_test: 0.580 acc_test: 0.692 AUC = 0.79 time: 0.497s\n",
            "Epoch: 211 loss_train: 0.488 acc_train: 0.770 loss_test: 0.618 acc_test: 0.692 AUC = 0.76 time: 0.497s\n",
            "Epoch: 212 loss_train: 0.471 acc_train: 0.791 loss_test: 0.597 acc_test: 0.682 AUC = 0.77 time: 0.496s\n",
            "Epoch: 213 loss_train: 0.483 acc_train: 0.787 loss_test: 0.569 acc_test: 0.704 AUC = 0.79 time: 0.497s\n",
            "Epoch: 214 loss_train: 0.487 acc_train: 0.787 loss_test: 0.570 acc_test: 0.695 AUC = 0.79 time: 0.497s\n",
            "Epoch: 215 loss_train: 0.480 acc_train: 0.784 loss_test: 0.603 acc_test: 0.707 AUC = 0.78 time: 0.497s\n",
            "Epoch: 216 loss_train: 0.485 acc_train: 0.783 loss_test: 0.581 acc_test: 0.711 AUC = 0.79 time: 0.497s\n",
            "Epoch: 217 loss_train: 0.477 acc_train: 0.784 loss_test: 0.567 acc_test: 0.698 AUC = 0.79 time: 0.497s\n",
            "Epoch: 218 loss_train: 0.483 acc_train: 0.794 loss_test: 0.600 acc_test: 0.703 AUC = 0.78 time: 0.497s\n",
            "Epoch: 219 loss_train: 0.496 acc_train: 0.775 loss_test: 0.601 acc_test: 0.694 AUC = 0.77 time: 0.497s\n",
            "Epoch: 220 loss_train: 0.470 acc_train: 0.784 loss_test: 0.594 acc_test: 0.685 AUC = 0.78 time: 0.497s\n",
            "Epoch: 221 loss_train: 0.467 acc_train: 0.787 loss_test: 0.579 acc_test: 0.695 AUC = 0.79 time: 0.498s\n",
            "Epoch: 222 loss_train: 0.474 acc_train: 0.782 loss_test: 0.610 acc_test: 0.682 AUC = 0.76 time: 0.497s\n",
            "Epoch: 223 loss_train: 0.482 acc_train: 0.780 loss_test: 0.595 acc_test: 0.691 AUC = 0.78 time: 0.498s\n",
            "Epoch: 224 loss_train: 0.489 acc_train: 0.780 loss_test: 0.572 acc_test: 0.717 AUC = 0.79 time: 0.497s\n",
            "Epoch: 225 loss_train: 0.472 acc_train: 0.793 loss_test: 0.585 acc_test: 0.704 AUC = 0.78 time: 0.497s\n",
            "Epoch: 226 loss_train: 0.484 acc_train: 0.770 loss_test: 0.604 acc_test: 0.698 AUC = 0.77 time: 0.498s\n",
            "Epoch: 227 loss_train: 0.491 acc_train: 0.784 loss_test: 0.610 acc_test: 0.688 AUC = 0.77 time: 0.497s\n",
            "Epoch: 228 loss_train: 0.478 acc_train: 0.781 loss_test: 0.548 acc_test: 0.723 AUC = 0.81 time: 0.497s\n",
            "Epoch: 229 loss_train: 0.505 acc_train: 0.774 loss_test: 0.594 acc_test: 0.703 AUC = 0.77 time: 0.497s\n",
            "Epoch: 230 loss_train: 0.489 acc_train: 0.777 loss_test: 0.591 acc_test: 0.692 AUC = 0.77 time: 0.497s\n",
            "Epoch: 231 loss_train: 0.467 acc_train: 0.791 loss_test: 0.595 acc_test: 0.697 AUC = 0.77 time: 0.497s\n",
            "Epoch: 232 loss_train: 0.497 acc_train: 0.779 loss_test: 0.585 acc_test: 0.698 AUC = 0.78 time: 0.497s\n",
            "Epoch: 233 loss_train: 0.484 acc_train: 0.786 loss_test: 0.589 acc_test: 0.713 AUC = 0.79 time: 0.497s\n",
            "Epoch: 234 loss_train: 0.488 acc_train: 0.779 loss_test: 0.569 acc_test: 0.711 AUC = 0.79 time: 0.497s\n",
            "Epoch: 235 loss_train: 0.469 acc_train: 0.789 loss_test: 0.589 acc_test: 0.697 AUC = 0.78 time: 0.499s\n",
            "Epoch: 236 loss_train: 0.476 acc_train: 0.781 loss_test: 0.572 acc_test: 0.720 AUC = 0.79 time: 0.497s\n",
            "Epoch: 237 loss_train: 0.482 acc_train: 0.771 loss_test: 0.579 acc_test: 0.720 AUC = 0.79 time: 0.497s\n",
            "Epoch: 238 loss_train: 0.497 acc_train: 0.779 loss_test: 0.628 acc_test: 0.669 AUC = 0.75 time: 0.497s\n",
            "Epoch: 239 loss_train: 0.488 acc_train: 0.782 loss_test: 0.557 acc_test: 0.698 AUC = 0.81 time: 0.497s\n",
            "Epoch: 240 loss_train: 0.478 acc_train: 0.792 loss_test: 0.644 acc_test: 0.688 AUC = 0.75 time: 0.498s\n",
            "Epoch: 241 loss_train: 0.461 acc_train: 0.793 loss_test: 0.558 acc_test: 0.723 AUC = 0.81 time: 0.496s\n",
            "Epoch: 242 loss_train: 0.486 acc_train: 0.777 loss_test: 0.586 acc_test: 0.710 AUC = 0.77 time: 0.497s\n",
            "Epoch: 243 loss_train: 0.474 acc_train: 0.799 loss_test: 0.567 acc_test: 0.713 AUC = 0.80 time: 0.497s\n",
            "Epoch: 244 loss_train: 0.472 acc_train: 0.781 loss_test: 0.570 acc_test: 0.710 AUC = 0.80 time: 0.497s\n",
            "Epoch: 245 loss_train: 0.481 acc_train: 0.782 loss_test: 0.585 acc_test: 0.701 AUC = 0.78 time: 0.497s\n",
            "Epoch: 246 loss_train: 0.477 acc_train: 0.782 loss_test: 0.580 acc_test: 0.701 AUC = 0.79 time: 0.497s\n",
            "Epoch: 247 loss_train: 0.464 acc_train: 0.796 loss_test: 0.591 acc_test: 0.705 AUC = 0.79 time: 0.497s\n",
            "Epoch: 248 loss_train: 0.463 acc_train: 0.792 loss_test: 0.556 acc_test: 0.719 AUC = 0.80 time: 0.497s\n",
            "Epoch: 249 loss_train: 0.464 acc_train: 0.796 loss_test: 0.604 acc_test: 0.683 AUC = 0.78 time: 0.497s\n",
            "Epoch: 250 loss_train: 0.476 acc_train: 0.786 loss_test: 0.558 acc_test: 0.708 AUC = 0.81 time: 0.497s\n",
            "Epoch: 251 loss_train: 0.479 acc_train: 0.790 loss_test: 0.587 acc_test: 0.704 AUC = 0.79 time: 0.497s\n",
            "Epoch: 252 loss_train: 0.485 acc_train: 0.786 loss_test: 0.575 acc_test: 0.692 AUC = 0.79 time: 0.497s\n",
            "Epoch: 253 loss_train: 0.466 acc_train: 0.791 loss_test: 0.583 acc_test: 0.704 AUC = 0.79 time: 0.496s\n",
            "Epoch: 254 loss_train: 0.468 acc_train: 0.786 loss_test: 0.580 acc_test: 0.704 AUC = 0.78 time: 0.497s\n",
            "Epoch: 255 loss_train: 0.463 acc_train: 0.794 loss_test: 0.586 acc_test: 0.714 AUC = 0.79 time: 0.497s\n",
            "Epoch: 256 loss_train: 0.479 acc_train: 0.779 loss_test: 0.595 acc_test: 0.704 AUC = 0.79 time: 0.496s\n",
            "Epoch: 257 loss_train: 0.463 acc_train: 0.807 loss_test: 0.572 acc_test: 0.707 AUC = 0.79 time: 0.497s\n",
            "Epoch: 258 loss_train: 0.480 acc_train: 0.781 loss_test: 0.552 acc_test: 0.713 AUC = 0.80 time: 0.496s\n",
            "Epoch: 259 loss_train: 0.460 acc_train: 0.794 loss_test: 0.552 acc_test: 0.707 AUC = 0.81 time: 0.497s\n",
            "Epoch: 260 loss_train: 0.467 acc_train: 0.795 loss_test: 0.553 acc_test: 0.719 AUC = 0.80 time: 0.497s\n",
            "Epoch: 261 loss_train: 0.473 acc_train: 0.775 loss_test: 0.565 acc_test: 0.694 AUC = 0.80 time: 0.496s\n",
            "Epoch: 262 loss_train: 0.467 acc_train: 0.791 loss_test: 0.601 acc_test: 0.698 AUC = 0.79 time: 0.497s\n",
            "Epoch: 263 loss_train: 0.473 acc_train: 0.784 loss_test: 0.571 acc_test: 0.714 AUC = 0.79 time: 0.496s\n",
            "Epoch: 264 loss_train: 0.467 acc_train: 0.794 loss_test: 0.587 acc_test: 0.710 AUC = 0.78 time: 0.497s\n",
            "Epoch: 265 loss_train: 0.461 acc_train: 0.794 loss_test: 0.550 acc_test: 0.725 AUC = 0.80 time: 0.497s\n",
            "Epoch: 266 loss_train: 0.471 acc_train: 0.788 loss_test: 0.576 acc_test: 0.700 AUC = 0.80 time: 0.497s\n",
            "Epoch: 267 loss_train: 0.473 acc_train: 0.792 loss_test: 0.574 acc_test: 0.711 AUC = 0.79 time: 0.496s\n",
            "Epoch: 268 loss_train: 0.461 acc_train: 0.794 loss_test: 0.562 acc_test: 0.713 AUC = 0.80 time: 0.497s\n",
            "Epoch: 269 loss_train: 0.467 acc_train: 0.790 loss_test: 0.590 acc_test: 0.691 AUC = 0.78 time: 0.498s\n",
            "Epoch: 270 loss_train: 0.459 acc_train: 0.796 loss_test: 0.574 acc_test: 0.697 AUC = 0.80 time: 0.497s\n",
            "Epoch: 271 loss_train: 0.464 acc_train: 0.794 loss_test: 0.559 acc_test: 0.708 AUC = 0.81 time: 0.497s\n",
            "Epoch: 272 loss_train: 0.477 acc_train: 0.787 loss_test: 0.591 acc_test: 0.701 AUC = 0.79 time: 0.497s\n",
            "Epoch: 273 loss_train: 0.469 acc_train: 0.799 loss_test: 0.570 acc_test: 0.698 AUC = 0.80 time: 0.497s\n",
            "Epoch: 274 loss_train: 0.462 acc_train: 0.787 loss_test: 0.592 acc_test: 0.695 AUC = 0.78 time: 0.497s\n",
            "Epoch: 275 loss_train: 0.471 acc_train: 0.789 loss_test: 0.582 acc_test: 0.708 AUC = 0.79 time: 0.496s\n",
            "Epoch: 276 loss_train: 0.466 acc_train: 0.785 loss_test: 0.581 acc_test: 0.689 AUC = 0.78 time: 0.497s\n",
            "Epoch: 277 loss_train: 0.471 acc_train: 0.797 loss_test: 0.549 acc_test: 0.723 AUC = 0.81 time: 0.496s\n",
            "Epoch: 278 loss_train: 0.471 acc_train: 0.790 loss_test: 0.591 acc_test: 0.692 AUC = 0.78 time: 0.497s\n",
            "Epoch: 279 loss_train: 0.476 acc_train: 0.784 loss_test: 0.564 acc_test: 0.714 AUC = 0.80 time: 0.497s\n",
            "Epoch: 280 loss_train: 0.467 acc_train: 0.786 loss_test: 0.581 acc_test: 0.694 AUC = 0.79 time: 0.497s\n",
            "Epoch: 281 loss_train: 0.463 acc_train: 0.794 loss_test: 0.554 acc_test: 0.717 AUC = 0.81 time: 0.497s\n",
            "Epoch: 282 loss_train: 0.472 acc_train: 0.793 loss_test: 0.550 acc_test: 0.736 AUC = 0.81 time: 0.496s\n",
            "Epoch: 283 loss_train: 0.468 acc_train: 0.794 loss_test: 0.550 acc_test: 0.723 AUC = 0.81 time: 0.496s\n",
            "Epoch: 284 loss_train: 0.471 acc_train: 0.797 loss_test: 0.587 acc_test: 0.697 AUC = 0.79 time: 0.497s\n",
            "Epoch: 285 loss_train: 0.483 acc_train: 0.791 loss_test: 0.606 acc_test: 0.703 AUC = 0.77 time: 0.497s\n",
            "Epoch: 286 loss_train: 0.455 acc_train: 0.799 loss_test: 0.569 acc_test: 0.714 AUC = 0.80 time: 0.497s\n",
            "Epoch: 287 loss_train: 0.462 acc_train: 0.793 loss_test: 0.566 acc_test: 0.714 AUC = 0.81 time: 0.497s\n",
            "Epoch: 288 loss_train: 0.455 acc_train: 0.801 loss_test: 0.574 acc_test: 0.697 AUC = 0.80 time: 0.498s\n",
            "Epoch: 289 loss_train: 0.462 acc_train: 0.796 loss_test: 0.632 acc_test: 0.686 AUC = 0.76 time: 0.497s\n",
            "Epoch: 290 loss_train: 0.464 acc_train: 0.786 loss_test: 0.552 acc_test: 0.717 AUC = 0.81 time: 0.534s\n",
            "Epoch: 291 loss_train: 0.476 acc_train: 0.792 loss_test: 0.586 acc_test: 0.726 AUC = 0.79 time: 0.497s\n",
            "Epoch: 292 loss_train: 0.473 acc_train: 0.791 loss_test: 0.586 acc_test: 0.701 AUC = 0.78 time: 0.497s\n",
            "Epoch: 293 loss_train: 0.471 acc_train: 0.791 loss_test: 0.585 acc_test: 0.719 AUC = 0.79 time: 0.497s\n",
            "Epoch: 294 loss_train: 0.467 acc_train: 0.800 loss_test: 0.572 acc_test: 0.707 AUC = 0.80 time: 0.497s\n",
            "Epoch: 295 loss_train: 0.456 acc_train: 0.790 loss_test: 0.570 acc_test: 0.711 AUC = 0.79 time: 0.497s\n",
            "Epoch: 296 loss_train: 0.473 acc_train: 0.796 loss_test: 0.557 acc_test: 0.704 AUC = 0.81 time: 0.496s\n",
            "Epoch: 297 loss_train: 0.481 acc_train: 0.782 loss_test: 0.553 acc_test: 0.694 AUC = 0.81 time: 0.497s\n",
            "Epoch: 298 loss_train: 0.442 acc_train: 0.808 loss_test: 0.588 acc_test: 0.710 AUC = 0.79 time: 0.498s\n",
            "Epoch: 299 loss_train: 0.471 acc_train: 0.790 loss_test: 0.583 acc_test: 0.703 AUC = 0.78 time: 0.498s\n",
            "Epoch: 300 loss_train: 0.461 acc_train: 0.795 loss_test: 0.583 acc_test: 0.686 AUC = 0.78 time: 0.497s\n",
            "Epoch: 301 loss_train: 0.475 acc_train: 0.783 loss_test: 0.570 acc_test: 0.716 AUC = 0.80 time: 0.497s\n",
            "Epoch: 302 loss_train: 0.466 acc_train: 0.791 loss_test: 0.577 acc_test: 0.700 AUC = 0.79 time: 0.497s\n",
            "Epoch: 303 loss_train: 0.460 acc_train: 0.796 loss_test: 0.567 acc_test: 0.705 AUC = 0.81 time: 0.497s\n",
            "Epoch: 304 loss_train: 0.460 acc_train: 0.799 loss_test: 0.559 acc_test: 0.705 AUC = 0.80 time: 0.497s\n",
            "Epoch: 305 loss_train: 0.458 acc_train: 0.792 loss_test: 0.549 acc_test: 0.728 AUC = 0.81 time: 0.497s\n",
            "Epoch: 306 loss_train: 0.458 acc_train: 0.800 loss_test: 0.552 acc_test: 0.714 AUC = 0.80 time: 0.498s\n",
            "Epoch: 307 loss_train: 0.449 acc_train: 0.796 loss_test: 0.562 acc_test: 0.711 AUC = 0.80 time: 0.497s\n",
            "Epoch: 308 loss_train: 0.465 acc_train: 0.796 loss_test: 0.591 acc_test: 0.692 AUC = 0.78 time: 0.497s\n",
            "Epoch: 309 loss_train: 0.457 acc_train: 0.794 loss_test: 0.558 acc_test: 0.711 AUC = 0.81 time: 0.497s\n",
            "Epoch: 310 loss_train: 0.462 acc_train: 0.798 loss_test: 0.571 acc_test: 0.720 AUC = 0.81 time: 0.497s\n",
            "Epoch: 311 loss_train: 0.450 acc_train: 0.791 loss_test: 0.565 acc_test: 0.713 AUC = 0.80 time: 0.497s\n",
            "Epoch: 312 loss_train: 0.460 acc_train: 0.790 loss_test: 0.543 acc_test: 0.730 AUC = 0.82 time: 0.497s\n",
            "Epoch: 313 loss_train: 0.475 acc_train: 0.797 loss_test: 0.574 acc_test: 0.711 AUC = 0.80 time: 0.496s\n",
            "Epoch: 314 loss_train: 0.463 acc_train: 0.791 loss_test: 0.596 acc_test: 0.683 AUC = 0.77 time: 0.497s\n",
            "Epoch: 315 loss_train: 0.468 acc_train: 0.791 loss_test: 0.592 acc_test: 0.691 AUC = 0.79 time: 0.497s\n",
            "Epoch: 316 loss_train: 0.463 acc_train: 0.788 loss_test: 0.581 acc_test: 0.701 AUC = 0.79 time: 0.496s\n",
            "Epoch: 317 loss_train: 0.456 acc_train: 0.796 loss_test: 0.563 acc_test: 0.714 AUC = 0.81 time: 0.496s\n",
            "Epoch: 318 loss_train: 0.470 acc_train: 0.783 loss_test: 0.575 acc_test: 0.705 AUC = 0.79 time: 0.497s\n",
            "Epoch: 319 loss_train: 0.453 acc_train: 0.803 loss_test: 0.595 acc_test: 0.694 AUC = 0.79 time: 0.497s\n",
            "Epoch: 320 loss_train: 0.463 acc_train: 0.796 loss_test: 0.558 acc_test: 0.723 AUC = 0.81 time: 0.497s\n",
            "Epoch: 321 loss_train: 0.454 acc_train: 0.800 loss_test: 0.576 acc_test: 0.722 AUC = 0.79 time: 0.497s\n",
            "Epoch: 322 loss_train: 0.462 acc_train: 0.796 loss_test: 0.541 acc_test: 0.717 AUC = 0.82 time: 0.496s\n",
            "Epoch: 323 loss_train: 0.458 acc_train: 0.795 loss_test: 0.576 acc_test: 0.713 AUC = 0.80 time: 0.497s\n",
            "Epoch: 324 loss_train: 0.469 acc_train: 0.789 loss_test: 0.557 acc_test: 0.713 AUC = 0.80 time: 0.497s\n",
            "Epoch: 325 loss_train: 0.455 acc_train: 0.796 loss_test: 0.539 acc_test: 0.723 AUC = 0.82 time: 0.497s\n",
            "Epoch: 326 loss_train: 0.442 acc_train: 0.806 loss_test: 0.555 acc_test: 0.711 AUC = 0.80 time: 0.497s\n",
            "Epoch: 327 loss_train: 0.452 acc_train: 0.797 loss_test: 0.555 acc_test: 0.723 AUC = 0.81 time: 0.497s\n",
            "Epoch: 328 loss_train: 0.465 acc_train: 0.799 loss_test: 0.576 acc_test: 0.720 AUC = 0.80 time: 0.497s\n",
            "Epoch: 329 loss_train: 0.453 acc_train: 0.801 loss_test: 0.564 acc_test: 0.710 AUC = 0.80 time: 0.498s\n",
            "Epoch: 330 loss_train: 0.457 acc_train: 0.796 loss_test: 0.535 acc_test: 0.726 AUC = 0.82 time: 0.497s\n",
            "Epoch: 331 loss_train: 0.455 acc_train: 0.803 loss_test: 0.546 acc_test: 0.719 AUC = 0.82 time: 0.497s\n",
            "Epoch: 332 loss_train: 0.468 acc_train: 0.791 loss_test: 0.611 acc_test: 0.708 AUC = 0.77 time: 0.497s\n",
            "Epoch: 333 loss_train: 0.461 acc_train: 0.787 loss_test: 0.564 acc_test: 0.719 AUC = 0.81 time: 0.497s\n",
            "Epoch: 334 loss_train: 0.453 acc_train: 0.799 loss_test: 0.552 acc_test: 0.714 AUC = 0.82 time: 0.497s\n",
            "Epoch: 335 loss_train: 0.457 acc_train: 0.801 loss_test: 0.554 acc_test: 0.723 AUC = 0.81 time: 0.497s\n",
            "Epoch: 336 loss_train: 0.456 acc_train: 0.800 loss_test: 0.545 acc_test: 0.711 AUC = 0.81 time: 0.497s\n",
            "Epoch: 337 loss_train: 0.455 acc_train: 0.803 loss_test: 0.553 acc_test: 0.720 AUC = 0.81 time: 0.497s\n",
            "Epoch: 338 loss_train: 0.463 acc_train: 0.793 loss_test: 0.524 acc_test: 0.739 AUC = 0.83 time: 0.497s\n",
            "Epoch: 339 loss_train: 0.456 acc_train: 0.801 loss_test: 0.549 acc_test: 0.707 AUC = 0.81 time: 0.497s\n",
            "Epoch: 340 loss_train: 0.441 acc_train: 0.811 loss_test: 0.545 acc_test: 0.713 AUC = 0.82 time: 0.497s\n",
            "Epoch: 341 loss_train: 0.454 acc_train: 0.794 loss_test: 0.579 acc_test: 0.688 AUC = 0.79 time: 0.497s\n",
            "Epoch: 342 loss_train: 0.462 acc_train: 0.797 loss_test: 0.581 acc_test: 0.710 AUC = 0.80 time: 0.497s\n",
            "Epoch: 343 loss_train: 0.470 acc_train: 0.787 loss_test: 0.538 acc_test: 0.716 AUC = 0.83 time: 0.497s\n",
            "Epoch: 344 loss_train: 0.465 acc_train: 0.786 loss_test: 0.559 acc_test: 0.726 AUC = 0.80 time: 0.497s\n",
            "Epoch: 345 loss_train: 0.452 acc_train: 0.791 loss_test: 0.594 acc_test: 0.720 AUC = 0.80 time: 0.497s\n",
            "Epoch: 346 loss_train: 0.451 acc_train: 0.802 loss_test: 0.585 acc_test: 0.707 AUC = 0.79 time: 0.497s\n",
            "Epoch: 347 loss_train: 0.452 acc_train: 0.794 loss_test: 0.564 acc_test: 0.697 AUC = 0.80 time: 0.497s\n",
            "Epoch: 348 loss_train: 0.455 acc_train: 0.794 loss_test: 0.569 acc_test: 0.711 AUC = 0.80 time: 0.496s\n",
            "Epoch: 349 loss_train: 0.440 acc_train: 0.804 loss_test: 0.551 acc_test: 0.717 AUC = 0.81 time: 0.497s\n",
            "Epoch: 350 loss_train: 0.461 acc_train: 0.794 loss_test: 0.553 acc_test: 0.728 AUC = 0.81 time: 0.497s\n",
            "Epoch: 351 loss_train: 0.437 acc_train: 0.816 loss_test: 0.538 acc_test: 0.708 AUC = 0.82 time: 0.497s\n",
            "Epoch: 352 loss_train: 0.456 acc_train: 0.791 loss_test: 0.568 acc_test: 0.701 AUC = 0.79 time: 0.497s\n",
            "Epoch: 353 loss_train: 0.447 acc_train: 0.799 loss_test: 0.564 acc_test: 0.714 AUC = 0.80 time: 0.497s\n",
            "Epoch: 354 loss_train: 0.457 acc_train: 0.796 loss_test: 0.557 acc_test: 0.720 AUC = 0.81 time: 0.497s\n",
            "Epoch: 355 loss_train: 0.450 acc_train: 0.803 loss_test: 0.557 acc_test: 0.708 AUC = 0.80 time: 0.497s\n",
            "Epoch: 356 loss_train: 0.458 acc_train: 0.794 loss_test: 0.549 acc_test: 0.728 AUC = 0.81 time: 0.497s\n",
            "Epoch: 357 loss_train: 0.446 acc_train: 0.801 loss_test: 0.545 acc_test: 0.736 AUC = 0.83 time: 0.497s\n",
            "Epoch: 358 loss_train: 0.472 acc_train: 0.787 loss_test: 0.555 acc_test: 0.703 AUC = 0.81 time: 0.497s\n",
            "Epoch: 359 loss_train: 0.449 acc_train: 0.803 loss_test: 0.561 acc_test: 0.723 AUC = 0.81 time: 0.497s\n",
            "Epoch: 360 loss_train: 0.452 acc_train: 0.802 loss_test: 0.562 acc_test: 0.701 AUC = 0.81 time: 0.497s\n",
            "Epoch: 361 loss_train: 0.439 acc_train: 0.807 loss_test: 0.562 acc_test: 0.723 AUC = 0.81 time: 0.497s\n",
            "Epoch: 362 loss_train: 0.454 acc_train: 0.800 loss_test: 0.554 acc_test: 0.714 AUC = 0.81 time: 0.497s\n",
            "Epoch: 363 loss_train: 0.449 acc_train: 0.791 loss_test: 0.570 acc_test: 0.711 AUC = 0.81 time: 0.498s\n",
            "Epoch: 364 loss_train: 0.453 acc_train: 0.791 loss_test: 0.581 acc_test: 0.701 AUC = 0.80 time: 0.497s\n",
            "Epoch: 365 loss_train: 0.461 acc_train: 0.790 loss_test: 0.552 acc_test: 0.732 AUC = 0.81 time: 0.498s\n",
            "Epoch: 366 loss_train: 0.461 acc_train: 0.792 loss_test: 0.522 acc_test: 0.736 AUC = 0.83 time: 0.497s\n",
            "Epoch: 367 loss_train: 0.445 acc_train: 0.813 loss_test: 0.588 acc_test: 0.705 AUC = 0.80 time: 0.497s\n",
            "Epoch: 368 loss_train: 0.446 acc_train: 0.815 loss_test: 0.556 acc_test: 0.711 AUC = 0.80 time: 0.497s\n",
            "Epoch: 369 loss_train: 0.462 acc_train: 0.794 loss_test: 0.552 acc_test: 0.698 AUC = 0.81 time: 0.497s\n",
            "Epoch: 370 loss_train: 0.454 acc_train: 0.791 loss_test: 0.546 acc_test: 0.713 AUC = 0.81 time: 0.497s\n",
            "Epoch: 371 loss_train: 0.454 acc_train: 0.796 loss_test: 0.555 acc_test: 0.714 AUC = 0.81 time: 0.497s\n",
            "Epoch: 372 loss_train: 0.457 acc_train: 0.799 loss_test: 0.540 acc_test: 0.711 AUC = 0.82 time: 0.497s\n",
            "Epoch: 373 loss_train: 0.448 acc_train: 0.803 loss_test: 0.548 acc_test: 0.717 AUC = 0.82 time: 0.497s\n",
            "Epoch: 374 loss_train: 0.447 acc_train: 0.803 loss_test: 0.573 acc_test: 0.708 AUC = 0.79 time: 0.497s\n",
            "Epoch: 375 loss_train: 0.445 acc_train: 0.808 loss_test: 0.537 acc_test: 0.732 AUC = 0.82 time: 0.497s\n",
            "Epoch: 376 loss_train: 0.465 acc_train: 0.800 loss_test: 0.551 acc_test: 0.720 AUC = 0.81 time: 0.497s\n",
            "Epoch: 377 loss_train: 0.457 acc_train: 0.796 loss_test: 0.574 acc_test: 0.704 AUC = 0.80 time: 0.496s\n",
            "Epoch: 378 loss_train: 0.435 acc_train: 0.799 loss_test: 0.590 acc_test: 0.695 AUC = 0.78 time: 0.496s\n",
            "Epoch: 379 loss_train: 0.444 acc_train: 0.796 loss_test: 0.544 acc_test: 0.719 AUC = 0.81 time: 0.497s\n",
            "Epoch: 380 loss_train: 0.448 acc_train: 0.800 loss_test: 0.566 acc_test: 0.714 AUC = 0.80 time: 0.497s\n",
            "Epoch: 381 loss_train: 0.451 acc_train: 0.800 loss_test: 0.555 acc_test: 0.717 AUC = 0.81 time: 0.497s\n",
            "Epoch: 382 loss_train: 0.445 acc_train: 0.800 loss_test: 0.533 acc_test: 0.708 AUC = 0.82 time: 0.497s\n",
            "Epoch: 383 loss_train: 0.447 acc_train: 0.800 loss_test: 0.574 acc_test: 0.716 AUC = 0.80 time: 0.497s\n",
            "Epoch: 384 loss_train: 0.451 acc_train: 0.787 loss_test: 0.530 acc_test: 0.723 AUC = 0.83 time: 0.499s\n",
            "Epoch: 385 loss_train: 0.441 acc_train: 0.799 loss_test: 0.531 acc_test: 0.722 AUC = 0.82 time: 0.497s\n",
            "Epoch: 386 loss_train: 0.449 acc_train: 0.800 loss_test: 0.546 acc_test: 0.723 AUC = 0.82 time: 0.496s\n",
            "Epoch: 387 loss_train: 0.448 acc_train: 0.801 loss_test: 0.583 acc_test: 0.694 AUC = 0.79 time: 0.497s\n",
            "Epoch: 388 loss_train: 0.461 acc_train: 0.795 loss_test: 0.578 acc_test: 0.705 AUC = 0.80 time: 0.498s\n",
            "Epoch: 389 loss_train: 0.438 acc_train: 0.801 loss_test: 0.530 acc_test: 0.732 AUC = 0.83 time: 0.497s\n",
            "Epoch: 390 loss_train: 0.456 acc_train: 0.801 loss_test: 0.553 acc_test: 0.733 AUC = 0.82 time: 0.497s\n",
            "Epoch: 391 loss_train: 0.445 acc_train: 0.801 loss_test: 0.561 acc_test: 0.716 AUC = 0.80 time: 0.497s\n",
            "Epoch: 392 loss_train: 0.455 acc_train: 0.799 loss_test: 0.550 acc_test: 0.700 AUC = 0.81 time: 0.497s\n",
            "Epoch: 393 loss_train: 0.444 acc_train: 0.789 loss_test: 0.557 acc_test: 0.707 AUC = 0.81 time: 0.497s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 245.3649s\n",
            "Loading 342th epoch\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-c368eb4b4196>:179: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(sv_path +'{}.pkl'.format(best_epoch)))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set results: loss= 0.4879 accuracy= 0.7482 AUC = 0.87 False Alarm Rate = 0.3019 F1 Score = 0.6705\n"
          ]
        }
      ]
    }
  ]
}